{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of QEV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansonmiu0214/C490CW/blob/master/MinibatchQEV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEKvQGwiJhlX",
        "colab_type": "text"
      },
      "source": [
        "# Coursework: Quality Estimation Vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSN72uyXX8zr",
        "colab_type": "code",
        "outputId": "fcd9f069-de9a-4530-dfc4-a40e4ab30536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'gpu')\n",
        "print(f'DEVICE={device}')\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "# PyTorch version\n",
        "print(torch.__version__)\n",
        "\n",
        "# Disable warnings :)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE=cuda\n",
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW0tBzjD6zEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Google Drive authorisation\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# def in_gdrive(path):\n",
        "#     return f'/content/gdrive/My Drive/Colab Notebooks/{path}'\n",
        "\n",
        "# # !ls /content/gdrive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JV59lAFJ2Tv",
        "colab_type": "text"
      },
      "source": [
        "## Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rjr6T2qJlxm",
        "colab_type": "code",
        "outputId": "285363dd-9fad-4512-88dc-9aab088ac220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip\n",
        "\n",
        "TRAIN_EN = 'train.enzh.src'\n",
        "TRAIN_ZH = 'train.enzh.mt'\n",
        "TRAIN_SCORES = 'train.enzh.scores'\n",
        "VAL_EN = 'dev.enzh.src'\n",
        "VAL_ZH = 'dev.enzh.mt'\n",
        "VAL_SCORES = 'dev.enzh.scores'\n",
        "TEST_EN = 'test.enzh.src'\n",
        "TEST_ZH = 'test.enzh.mt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 14:14:47--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=cfcf912b83cd9acc16105af5382275abf6711ee3d2801294e2a1d6cf8f9e2143&X-Amz-Date=20200217T141447Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200217%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-17 14:14:47--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=cfcf912b83cd9acc16105af5382275abf6711ee3d2801294e2a1d6cf8f9e2143&X-Amz-Date=20200217T141447Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200217%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "\renzh_data.zip         0%[                    ]       0  --.-KB/s               \renzh_data.zip       100%[===================>] 850.48K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-02-17 14:14:47 (10.2 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQJ06kXdK-qc",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZU6c-b6Lgp1",
        "colab_type": "text"
      },
      "source": [
        "### English\n",
        "\n",
        "1. Tokenise with spaCy language model\n",
        "2. Remove stop words and punctuation\n",
        "3. Normalise - lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYS01soSJ8DP",
        "colab_type": "code",
        "outputId": "56e9fcef-edb4-4af2-f206-0a3646282cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# Downloading spacy models for English\n",
        "\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300 --force"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 2.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=48afd473c19a5d85b306666b5bab0491f7beafd62a343245808958898ac5aa7c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-peipkewp/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysk8sSgwOMNJ",
        "colab_type": "code",
        "outputId": "0737d94f-7931-4fa3-92a7-2277591d455f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Downloading stop words for English\n",
        "\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "download('stopwords')\n",
        "stop_words_en = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFh-oANJOv9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get tokenizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh5Bst_uRXzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_en(sentence=None, *, keep_stopwords=False):\n",
        "    def wrapper(sentence):\n",
        "        text = sentence.lower()\n",
        "        processed = [token.lemma_ for token in nlp_en.tokenizer(text)]\n",
        "        processed = [token for token in processed if token.isalpha()]\n",
        "        if not keep_stopwords:\n",
        "            processed = [token for token in processed if token not in stop_words_en]\n",
        "        return processed\n",
        "\n",
        "    return wrapper if sentence is None else wrapper(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UsvjK3mMGEW",
        "colab_type": "text"
      },
      "source": [
        "### Chinese\n",
        "\n",
        "1. Tokenise with jieba\n",
        "2. Remove stop words and punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaaRTU3XMPmA",
        "colab_type": "code",
        "outputId": "417e80af-a391-43f0-ccb6-ae9166c1fc21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Download stop words\n",
        "FILE_STOP_WORDS_ZH = './chinese_stop_words.txt'\n",
        "\n",
        "if not os.path.exists(FILE_STOP_WORDS_ZH):\n",
        "    !wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "with open(FILE_STOP_WORDS_ZH, 'r', encoding='utf-8') as f:\n",
        "    stop_words_zh = [line.rstrip() for line in f]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 14:15:39--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "\rchinese_stop_words.     [<=>                 ]       0  --.-KB/s               \rchinese_stop_words.     [ <=>                ] 419.14K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-02-17 14:15:40 (21.0 MB/s) - ‘chinese_stop_words.txt’ saved [429200]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmS0Q6fOUO2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jieba\n",
        "\n",
        "def preprocess_zh(sentence=None, *, keep_stopwords=False):\n",
        "    def wrapper(sentence):\n",
        "        tokens = jieba.cut(sentence, cut_all=True)\n",
        "        processed = [token for token in tokens if token.isalnum()]\n",
        "        if not keep_stopwords:\n",
        "            processed = [token for token in processed if token not in stop_words_zh]\n",
        "        return processed\n",
        "\n",
        "    return wrapper if sentence is None else wrapper(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn3Ar3-6MZWV",
        "colab_type": "text"
      },
      "source": [
        "## Language Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgPvP2XYMr5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Language(object):\n",
        "\n",
        "    PAD_TOKEN = '<PAD>'\n",
        "    SOS_TOKEN = '<SOS>'\n",
        "    EOS_TOKEN = '<EOS>'\n",
        "    UNK_TOKEN = '<UNK>'\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2idx = {}\n",
        "        self.word2count = {}\n",
        "        self.idx2word = {0: self.PAD_TOKEN,\n",
        "                         1: self.SOS_TOKEN,\n",
        "                         2: self.EOS_TOKEN,\n",
        "                         3: self.UNK_TOKEN}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for token in sentence:\n",
        "            self.add_word(token)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            idx = len(self)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "        \n",
        "        count = self.word2count.get(word, 0)\n",
        "        self.word2count[word] = count + 1\n",
        "\n",
        "    def sent_to_idxs(self, sent):\n",
        "        return [self.word2idx.get(word, 2) for word in sent]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f'Language(name={self.name}) with {len(self)} words'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DZWJs-CQMIb",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMBmQxeIQOPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read from file\n",
        "\n",
        "with open(TRAIN_EN) as f:\n",
        "    train_en = f.readlines()\n",
        "with open(TRAIN_ZH) as f:\n",
        "    train_zh = f.readlines()\n",
        "with open(TRAIN_SCORES) as f:\n",
        "    train_scores = f.readlines()\n",
        "with open(VAL_EN) as f:\n",
        "    val_en = f.readlines()\n",
        "with open(VAL_ZH) as f:\n",
        "    val_zh = f.readlines()\n",
        "with open(VAL_SCORES) as f:\n",
        "    val_scores = f.readlines()\n",
        "with open(TEST_EN) as f:\n",
        "    test_en = f.readlines()\n",
        "with open(TEST_ZH) as f:\n",
        "    test_zh = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2mfBOXbeu4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentences(vocab, sent):\n",
        "    sent_lengths = [len(sentence) for sentence in sent]\n",
        "\n",
        "    # create an empty matrix with padding tokens\n",
        "    pad_token = 0\n",
        "    longest_sent = max(sent_lengths)\n",
        "    batch_size = len(sent)\n",
        "    padded_sent = np.ones((batch_size, longest_sent)) * pad_token\n",
        "\n",
        "    # copy over the actual sequences\n",
        "    for i, sent_len in enumerate(sent_lengths):\n",
        "      sequence = sent[i]\n",
        "      padded_sent[i, 0:sent_len] = sequence[:sent_len]\n",
        "    return padded_sent, sent_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aaR6q37RMsO",
        "colab_type": "code",
        "outputId": "193ff769-2c5e-455e-ba60-9a71c6a106b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# English data\n",
        "\n",
        "preprocess_english = preprocess_en(keep_stopwords=True)\n",
        "train_en_sents = [preprocess_english(sent) for sent in train_en]\n",
        "val_en_sents = [preprocess_english(sent) for sent in val_en]\n",
        "test_en_sents = [preprocess_english(sent) for sent in test_en]\n",
        "\n",
        "EN = Language('EN')\n",
        "for sent in train_en_sents:\n",
        "    EN.add_sentence(sent)\n",
        "\n",
        "print(EN)\n",
        "\n",
        "print()\n",
        "print('Sample sentence')\n",
        "sample_sent_en = train_en_sents[42]\n",
        "print(sample_sent_en)\n",
        "print(EN.sent_to_idxs(sample_sent_en))\n",
        "\n",
        "train_en_idxs = [EN.sent_to_idxs(sent) for sent in train_en_sents]\n",
        "train_en_idxs, en_sent_lengths = pad_sentences(EN, train_en_idxs)\n",
        "val_en_idxs = [EN.sent_to_idxs(sent) for sent in val_en_sents]\n",
        "val_en_idxs, en_sent_lengths = pad_sentences(EN, val_en_idxs)\n",
        "test_en_idxs = [EN.sent_to_idxs(sent) for sent in test_en_sents]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language(name=EN) with 19249 words\n",
            "\n",
            "Sample sentence\n",
            "['all', 'of', 'the', 'artilleryman', 'record', 'a', 'wound', 'die']\n",
            "[341, 32, 4, 342, 343, 60, 344, 345]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DEZK9El_U9v3",
        "outputId": "cab37698-e072-4d29-ec90-512a2afb5572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\n",
        "# Chinese data\n",
        "\n",
        "preprocess_chinese = preprocess_zh(keep_stopwords=False)\n",
        "\n",
        "train_zh_sents = [preprocess_chinese(sent) for sent in train_zh]\n",
        "val_zh_sents = [preprocess_chinese(sent) for sent in val_zh]\n",
        "test_zh_sents = [preprocess_chinese(sent) for sent in test_zh]\n",
        "\n",
        "ZH = Language('ZH')\n",
        "for sent in train_zh_sents:\n",
        "    ZH.add_sentence(sent)\n",
        "\n",
        "print(ZH)\n",
        "\n",
        "print()\n",
        "print('Sample sentence')\n",
        "sample_sent_zh = train_zh_sents[42]\n",
        "print(sample_sent_zh)\n",
        "print(ZH.sent_to_idxs(sample_sent_zh))\n",
        "\n",
        "train_zh_idxs = [ZH.sent_to_idxs(sent) for sent in train_zh_sents]\n",
        "train_zh_idxs, zh_sent_lengths = pad_sentences(ZH, train_zh_idxs)\n",
        "val_zh_idxs = [ZH.sent_to_idxs(sent) for sent in val_zh_sents]\n",
        "val_zh_idxs, zh_sent_lengths = pad_sentences(ZH, val_zh_idxs)\n",
        "test_zh_idxs = [ZH.sent_to_idxs(sent) for sent in test_zh_sents]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.790 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Language(name=ZH) with 23852 words\n",
            "\n",
            "Sample sentence\n",
            "['据', '记录', '所有', '6', '名', '炮兵', '都', '受伤', '了']\n",
            "[484, 485, 486, 268, 487, 488, 489, 490, 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCR41KviW6Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process scores\n",
        "\n",
        "def prepare_score(score):\n",
        "    return float(score)\n",
        "\n",
        "train_scores = [prepare_score(score) for score in train_scores]\n",
        "val_scores = [prepare_score(score) for score in val_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eJnzunswRuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom Dataset Wrapper\n",
        "class SentenceData(Dataset):\n",
        "    def __init__(self, en_tensor, zh_tensor, score_tensor):\n",
        "        self.en = en_tensor\n",
        "        self.zh = zh_tensor\n",
        "        self.score = score_tensor\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return (self.en[index], self.zh[index], self.score[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qYQlzaEWViN",
        "colab_type": "code",
        "outputId": "d146d415-8c7e-4803-d786-9643a3451dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Datasets\n",
        "\n",
        "train_en_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in train_en_idxs]\n",
        "train_zh_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in train_zh_idxs]\n",
        "\n",
        "# train_pairs = list(zip(train_en_tensors, train_zh_tensors))\n",
        "# train_set = list(zip(train_pairs, train_scores))\n",
        "train_set = SentenceData(train_en_tensors, train_zh_tensors, train_scores)\n",
        "\n",
        "# train_set = [dat for dat in train_set if dat[2] < 1 and dat[2] > -1]\n",
        "\n",
        "print(train_set[0])\n",
        "\n",
        "val_en_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in val_en_idxs]\n",
        "val_zh_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in val_zh_idxs]\n",
        "\n",
        "# val_pairs = list(zip(val_en_tensors, val_zh_tensors))\n",
        "# val_set = list(zip(val_pairs, val_scores))\n",
        "val_set = SentenceData(val_en_tensors, val_zh_tensors, val_scores)\n",
        "\n",
        "# val_set = [dat for dat in val_set if dat[2] < 1 and dat[2] > -1]\n",
        "\n",
        "print(val_set[0])\n",
        "# val_pairs = list(zip(val_en_idxs, val_zh_idxs))\n",
        "# test_pairs = list(zip(test_en_idxs, test_zh_idxs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([ 4,  5,  6,  7,  8,  9, 10,  5, 11, 12, 13, 14,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), -1.5284005772625449)\n",
            "(tensor([11805, 10544,    11,  9460,   284,     4,     2,  2611,    32,  2085,\n",
            "          358,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), tensor([16174, 16175, 16176, 12583,    35,  2987,   510,   511,     5,     2,\n",
            "         3757,   397, 13875,    10,     5,  9582,  2524,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0]), -1.0228233810174194)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBU5dUInLx-R",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtsfA_hdPmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities\n",
        "\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "def unzip(args):\n",
        "    return zip(*args)\n",
        "\n",
        "def RMSELoss(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred.to('cpu') - target.to('cpu')) ** 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmQwblc_WAOT",
        "colab_type": "text"
      },
      "source": [
        "### Bi-Direction NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ7rerILiNZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_context_vec(hidden_outputs, zh_emb):\n",
        "    def activation(zh_emb, hidden):\n",
        "        # print(\"hidden shape:\", hidden.shape)\n",
        "        # print(\"zh shape:\", zh_emb.shape)\n",
        "\n",
        "        zh = zh_emb.detach().cpu().numpy().reshape(1, -1)\n",
        "        hid = hidden.detach().cpu().numpy().reshape(1, -1)\n",
        "        res = zh.dot(hid.transpose())\n",
        "        # print(res.shape)\n",
        "        return res\n",
        "\n",
        "    e_s = torch.Tensor([\n",
        "        activation(zh_emb, hid)\n",
        "        for hid in hidden_outputs\n",
        "    ])\n",
        "\n",
        "    a_s = F.softmax(e_s)\n",
        "\n",
        "    ctx = torch.zeros(zh_emb.shape)\n",
        "    for a, h in zip(a_s, hidden_outputs):\n",
        "        ctx += a * h.to('cpu')\n",
        "    return ctx.to(device)\n",
        "    # tan = torch.nn.Tanh\n",
        "    # return tan(ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mobMXC_V_0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, *, vocab_size, emb_dim=100, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
        "\n",
        "        self.forward_gru = torch.nn.GRU(\n",
        "            input_size=self.emb_dim, hidden_size=self.emb_dim, num_layers=self.num_layers,batch_first=False, bidirectional=True)\n",
        "        self.backward_gru = torch.nn.GRU(\n",
        "            input_size=self.emb_dim, hidden_size=self.emb_dim, num_layers=self.num_layers, batch_first=False, bidirectional=False)\n",
        "        \n",
        "        self.backward_gru.weight_ih_l0 = self.forward_gru.weight_ih_l0_reverse\n",
        "        self.backward_gru.weight_hh_l0 = self.forward_gru.weight_hh_l0_reverse\n",
        "        self.backward_gru.bias_ih_l0 = self.forward_gru.bias_ih_l0_reverse\n",
        "        self.backward_gru.bias_hh_l0 = self.forward_gru.bias_hh_l0_reverse\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        \"\"\"Return hidden states of backward RNN.\"\"\"\n",
        "        num_sent = tensor.shape[0]\n",
        "        sent_len = tensor.shape[1]\n",
        "\n",
        "        # print(\"sent_len:\", sent_len)\n",
        "        emb = self.embedding(tensor).view(sent_len, num_sent, -1)\n",
        "        # print(\"tensor shape:\", tensor.shape)\n",
        "        # print(\"emb shape:\", emb.shape)\n",
        "        self.forward_gru.flatten_parameters()\n",
        "        bi_output, bi_hidden = self.forward_gru(emb)\n",
        "        rev_emb = emb[np.arange(sent_len - 1, -1, -1), :, :]\n",
        "\n",
        "        self.backward_gru.flatten_parameters()\n",
        "        rev_output, rev_hidden = self.backward_gru(rev_emb)\n",
        "        return rev_output\n",
        "\n",
        "class BiRNNAttention(nn.Module):\n",
        "    def __init__(self, *, vocab_size, emb_dim=100, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
        "\n",
        "        self.forward_gru = torch.nn.GRU(\n",
        "            input_size=self.emb_dim, hidden_size=self.emb_dim, num_layers=self.num_layers,batch_first=False, bidirectional=True)\n",
        "        # self.backward_gru = torch.nn.GRU(\n",
        "            # input_size=self.emb_dim, hidden_size=self.emb_dim, num_layers=self.num_layers, batch_first=False, bidirectional=False)\n",
        "        \n",
        "        # self.backward_gru.weight_ih_l0 = self.forward_gru.weight_ih_l0_reverse\n",
        "        # self.backward_gru.weight_hh_l0 = self.forward_gru.weight_hh_l0_reverse\n",
        "        # self.backward_gru.bias_ih_l0 = self.forward_gru.bias_ih_l0_reverse\n",
        "        # self.backward_gru.bias_hh_l0 = self.forward_gru.bias_hh_l0_reverse\n",
        "\n",
        "    def forward(self, tensor, prev_hiddens):\n",
        "        \"\"\"Return hidden states of backward RNN.\"\"\"\n",
        "        # sent_len = len(tensor)\n",
        "        # embs = self.embedding(tensor).view(sent_len, 1, -1)\n",
        "        num_sent = tensor.shape[0]\n",
        "        sent_len = tensor.shape[1]\n",
        "\n",
        "        # print(\"sent_len:\", sent_len)\n",
        "        embs = self.embedding(tensor).view(sent_len, num_sent, -1)\n",
        "        ctxs = torch.stack([get_context_vec(prev_hiddens, zh_emb)\n",
        "                            for zh_emb in embs])\n",
        "\n",
        "        self.forward_gru.flatten_parameters()\n",
        "        bi_output, bi_hidden = self.forward_gru(ctxs)\n",
        "\n",
        "        rev_ctxs = ctxs[np.arange(sent_len - 1, -1, -1), :, :]\n",
        "\n",
        "        self.backward_gru.flatten_parameters()\n",
        "        rev_output, rev_hidden = self.backward_gru(rev_ctxs)\n",
        "        return rev_output\n",
        "\n",
        "class QualVecRNN(nn.Module):\n",
        "    def __init__(self, *, emb_dim=100, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.emb_dim, hidden_size=1, num_layers=self.num_layers)\n",
        "    \n",
        "    def forward(self, tensor):\n",
        "        # self.lstm.flatten_parameters()\n",
        "        output, hidden = self.lstm(tensor)\n",
        "        return hidden[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuOb3fKGOuWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(data_set, regressor):\n",
        "  loss = 0\n",
        "  pred_scores = []\n",
        "\n",
        "  for (en_tensor, zh_tensor, score) in tqdm(data_set):\n",
        "    en_hiddens = en_rnn(en_tensor.to(device))\n",
        "    qual_vecs = zh_rnn(zh_tensor.to(device), en_hiddens)\n",
        "    pred_score = regressor(qual_vecs).squeeze()\n",
        "    pred_scores.append(pred_score.data.cpu().numpy())\n",
        "\n",
        "    loss += loss_fn(pred_score, score)\n",
        "\n",
        "  return loss, pred_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB63jQN5h-Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_epochs, regressor, components, opts):\n",
        "    trainloader = DataLoader(dataset=train_set,batch_size=1)\n",
        "    validloader = DataLoader(dataset=val_set,batch_size=1)\n",
        "\n",
        "    # print(next(iter(trainloader)))\n",
        "    for eidx in range(num_epochs):\n",
        "        print(f'Epoch {eidx + 1}')\n",
        "\n",
        "        for comp in components:\n",
        "            comp.zero_grad()\n",
        "\n",
        "        print(f'Training: {len(train_set)}')\n",
        "\n",
        "        loss, _ = forward_pass(trainloader, regressor)\n",
        "        loss /= len(train_set)\n",
        "        print(f'Training Loss={loss:.5f}\\t')\n",
        "\n",
        "        val_loss, val_pred_scores = forward_pass(validloader, regressor)\n",
        "        val_loss /= len(val_set)\n",
        "        print(f'Validation loss = {val_loss:.5f}\\t')\n",
        "\n",
        "        pearson_score, _ = pearsonr(np.array(val_pred_scores), np.array(val_scores))\n",
        "        print(f'Validation pearson = {pearson_score:.5f}\\t')\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        for opt in opts:\n",
        "            opt.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFxnzWrOfgxw",
        "colab_type": "code",
        "outputId": "9ac4cabd-fb44-43d1-f9f6-9488ec0092db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "en_rnn = BiRNN(vocab_size=len(EN))\n",
        "en_rnn.to(device)\n",
        "\n",
        "zh_rnn = BiRNNAttention(vocab_size=len(ZH))\n",
        "zh_rnn.to(device)\n",
        "\n",
        "regressor = QualVecRNN()\n",
        "regressor.to(device)\n",
        "\n",
        "LR = 0.003\n",
        "\n",
        "en_opt = torch.optim.Adam(en_rnn.parameters(), lr=LR)\n",
        "zh_opt = torch.optim.Adam(zh_rnn.parameters(), lr=LR)\n",
        "regr_opt = torch.optim.Adam(regressor.parameters(), lr=LR)\n",
        "\n",
        "loss_fn = RMSELoss\n",
        "\n",
        "print(en_rnn)\n",
        "print(zh_rnn)\n",
        "print(regressor)\n",
        "\n",
        "components = (en_rnn, zh_rnn, regressor)\n",
        "opts = (en_opt, zh_opt, regr_opt)\n",
        "\n",
        "train(10, regressor, components, opts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BiRNN(\n",
            "  (embedding): Embedding(19249, 100)\n",
            "  (forward_gru): GRU(100, 100, bidirectional=True)\n",
            "  (backward_gru): GRU(100, 100)\n",
            ")\n",
            "BiRNNAttention(\n",
            "  (embedding): Embedding(23852, 100)\n",
            "  (forward_gru): GRU(100, 100, bidirectional=True)\n",
            "  (backward_gru): GRU(100, 100)\n",
            ")\n",
            "QualVecRNN(\n",
            "  (lstm): LSTM(100, 1)\n",
            ")\n",
            "Epoch 1\n",
            "Training: 7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 3140/7000 [10:07<45:25:25, 42.36s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFiF4FSRlCx",
        "colab_type": "text"
      },
      "source": [
        "### FFNN with trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oWCA9Mdbbta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, *, en_vocab_size, zh_vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.en_vocab_size = en_vocab_size\n",
        "        self.zh_vocab_size = zh_vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.en_embedding = nn.Embedding(self.en_vocab_size, self.emb_dim)\n",
        "        self.zh_embedding = nn.Embedding(self.zh_vocab_size, self.emb_dim)\n",
        "\n",
        "        self.en_hidden = nn.Linear(self.emb_dim, 1)\n",
        "        self.zh_hidden = nn.Linear(self.emb_dim, 1)\n",
        "\n",
        "        self.out = nn.Linear(2, 1)\n",
        "    \n",
        "    def forward(self, en_tensors, zh_tensors):\n",
        "        en_emb = self.en_embedding(en_tensors)\n",
        "        zh_emb = self.zh_embedding(zh_tensors)\n",
        "\n",
        "        en_hid = F.relu(self.en_hidden(en_emb))\n",
        "        zh_hid = F.relu(self.zh_hidden(en_emb))\n",
        "\n",
        "        hid_concat = torch.stack((en_hid, zh_hid), axis=1).squeeze()\n",
        "        score = self.out(hid_concat)\n",
        "        return score.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1SoBa5csKg",
        "colab_type": "code",
        "outputId": "30927305-eccf-46b2-f535-ee5b97019c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "ffnn = FFNN(en_vocab_size=len(EN), zh_vocab_size=len(ZH), emb_dim=200)\n",
        "ffnn.to(device)\n",
        "print(ffnn)\n",
        "\n",
        "ffnn_opt = torch.optim.Adam(ffnn.parameters(), lr=0.003)\n",
        "loss_fn = RMSELoss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (en_embedding): Embedding(19249, 200)\n",
            "  (zh_embedding): Embedding(23852, 200)\n",
            "  (en_hidden): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (zh_hidden): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (out): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa9MTNFHdTJ4",
        "colab_type": "code",
        "outputId": "f4c2134b-371f-43bd-84d8-5aa0ebb6249a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "NUM_EPOCHS = 100\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_pearson = []\n",
        "\n",
        "for eidx in range(NUM_EPOCHS):\n",
        "    print(f'Epoch {eidx + 1}: \\t', end=' ')\n",
        "    ffnn.zero_grad()\n",
        "    \n",
        "    loss = 0\n",
        "    for en_tensor, zh_tensor, score in train_set:\n",
        "        pred = ffnn(en_tensor.to(device), zh_tensor.to(device))\n",
        "        loss += loss_fn(pred, score)\n",
        "\n",
        "    loss /= len(train_set)\n",
        "    train_losses.append(loss)\n",
        "    \n",
        "    print(f'train loss = {loss:.5f}\\t', end='')\n",
        "\n",
        "    # Validation loss\n",
        "    val_loss = 0\n",
        "    for (en_tensor, zh_tensor), score in val_set:\n",
        "        pred = ffnn(en_tensor.to(device), zh_tensor.to(device))\n",
        "        val_loss += loss_fn(pred, score)\n",
        "    val_loss /= len(val_set)\n",
        "    print(f'validation loss = {val_loss:.5f}\\t', end='')\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Validation score\n",
        "    val_preds, val_targets = unzip([(ffnn(en_tensor.to(device), zh_tensor.to(device)).detach().cpu().numpy(), score)\n",
        "                              for (en_tensor, zh_tensor), score in val_set])\n",
        "    \n",
        "    val_preds = np.array(val_preds)\n",
        "    val_targets = np.array(val_targets)\n",
        "\n",
        "    pearson_score, _ = pearsonr(val_preds, val_targets)\n",
        "    val_pearson.append(pearson_score)\n",
        "    print(f'validation pearson = {pearson_score:.5f}\\t')\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    ffnn_opt.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: \t "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-d54c463c66fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0men_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzh_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzh_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-9b286400a037>\u001b[0m in \u001b[0;36mRMSELoss\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUlPsvWsL4mp",
        "colab_type": "text"
      },
      "source": [
        "### RNN Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UkdfLZMoI5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNChain(nn.Module):\n",
        "\n",
        "    def __init__(self, *, en_vocab_size, zh_vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.en_vocab_size = en_vocab_size\n",
        "        self.zh_vocab_size = zh_vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.en_embedding = nn.Embedding(self.en_vocab_size, self.emb_dim)\n",
        "        self.zh_embedding = nn.Embedding(self.zh_vocab_size, self.emb_dim)\n",
        "\n",
        "        self.en_rnn = nn.GRU(self.emb_dim, self.emb_dim, bidirectional=True)\n",
        "        self.zh_rnn = nn.GRU(self.emb_dim, self.emb_dim, bidirectional=True)\n",
        "\n",
        "        self.hidden = nn.Linear(self.emb_dim, 50)\n",
        "        self.out = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, en_tensor, zh_tensor):\n",
        "        en_emb = self.en_embedding(en_tensor)\n",
        "        zh_emb = self.zh_embedding(zh_tensor)\n",
        "\n",
        "        en_hidden = torch.zeros(2, 1, self.emb_dim, device=device)\n",
        "\n",
        "        for word_idx in en_emb:\n",
        "            word_idx = word_idx.view(1, 1, -1)\n",
        "            _, en_hidden = self.en_rnn(word_idx, en_hidden)\n",
        "    \n",
        "        zh_hidden = en_hidden\n",
        "        for word_idx in zh_emb:\n",
        "            word_idx = word_idx.view(1, 1, -1)\n",
        "            _, zh_hidden = self.zh_rnn(word_idx, zh_hidden)\n",
        "\n",
        "        score = self.out(F.relu(self.hidden(zh_hidden[-1])))\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDPzyUWzp4ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_PREV = True\n",
        "\n",
        "rnn = RNNChain(en_vocab_size=len(EN), zh_vocab_size=len(ZH), emb_dim=100)\n",
        "rnn.to(device)\n",
        "\n",
        "rnn_opt = torch.optim.Adam(rnn.parameters(), lr=0.003)\n",
        "loss_fn = RMSELoss\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "state = {\n",
        "    'curr_epoch': 1,\n",
        "    'train_losses': [],\n",
        "    'val_losses': [],\n",
        "    'val_pearson': [],\n",
        "}\n",
        "\n",
        "if USE_PREV and os.path.exists(in_gdrive('rnn.pt')):\n",
        "    print('Loading from Google Drive...', end=' ')\n",
        "    rnn.load_state_dict(torch.load(in_gdrive('rnn.pt')))\n",
        "\n",
        "    with open(in_gdrive('rnn.json'), 'r') as f:\n",
        "        state = json.load(f)\n",
        "    print('done!')\n",
        "\n",
        "\n",
        "while state['curr_epoch'] <= NUM_EPOCHS:\n",
        "    print(f'Epoch {state[\"curr_epoch\"]}:')\n",
        "    rnn.zero_grad()\n",
        "    \n",
        "    loss = 0\n",
        "    print(f'Training {len(train_set)}: ', end='')\n",
        "    for idx, ((en_tensor, zh_tensor), score) in enumerate(train_set):\n",
        "        pred = rnn(en_tensor.to(device), zh_tensor.to(device)).squeeze()\n",
        "        curr_loss = loss_fn(pred, score) \n",
        "        loss += curr_loss\n",
        "        if idx % 500 == 0:\n",
        "            print('.', end='')\n",
        "    print()\n",
        "\n",
        "    loss /= len(train_set)\n",
        "    state['train_losses'].append(loss.detach().cpu().numpy().tolist())\n",
        "    \n",
        "    print(f'==>train loss = {loss:.5f}')\n",
        "\n",
        "    # Validation loss\n",
        "    val_loss = 0\n",
        "    print(f'Validating loss {len(val_set)}: ', end='')\n",
        "    for idx, ((en_tensor, zh_tensor), score) in enumerate(val_set):\n",
        "        pred = rnn(en_tensor.to(device), zh_tensor.to(device))\n",
        "        val_loss += loss_fn(pred, score)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            print('.', end='')\n",
        "    print()    \n",
        "\n",
        "    val_loss /= len(val_set)\n",
        "    print(f'==>validation loss = {val_loss:.5f}')\n",
        "    state['val_losses'].append(val_loss.detach().cpu().numpy().tolist())\n",
        "\n",
        "    # Validation score\n",
        "    val_preds, val_targets = unzip([(rnn(en_tensor.to(device), zh_tensor.to(device)).squeeze().detach().cpu().numpy(), score)\n",
        "                              for (en_tensor, zh_tensor), score in val_set])\n",
        "    val_preds = np.array(val_preds)\n",
        "    val_targets = np.array(val_targets)\n",
        "\n",
        "    pearson_score, _ = pearsonr(val_preds, val_targets)\n",
        "    state['val_pearson'].append(pearson_score)\n",
        "    print(f'==>validation pearson = {pearson_score:.5f}')\n",
        "\n",
        "    # Backpropagation\n",
        "    print('Backpropagation...', end=' ')\n",
        "    loss.backward()\n",
        "    rnn_opt.step()\n",
        "    print('done!')\n",
        "\n",
        "    # Save\n",
        "    print('Saving to Google Drive...', end=' ')\n",
        "    torch.save(rnn.state_dict(), in_gdrive('rnn.pt'))\n",
        "\n",
        "    state['curr_epoch'] += 1\n",
        "    with open(in_gdrive('rnn.json'), 'w') as f:\n",
        "        json.dump(state, f)\n",
        "\n",
        "    print('done!\\n')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxsp-O3FL3si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNChain(nn.Module):\n",
        "\n",
        "    def __init__(self, *, vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
        "        self.rnn = nn.GRU(self.emb_dim, self.emb_dim, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        emb = self.embedding(x)\n",
        "        output = emb.view(1, 1, -1)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(2, 1, self.emb_dim, device=device)\n",
        "\n",
        "class RegressorLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, *, emb_dim):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.hidden = nn.Linear(self.emb_dim, 50)\n",
        "        self.out = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(F.relu(self.hidden(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ZnU2SMa9hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_model = RNNChain(vocab_size=len(EN), emb_dim=200)\n",
        "zh_model = RNNChain(vocab_size=len(ZH), emb_dim=200)\n",
        "regressor = RegressorLayer(emb_dim=200)\n",
        "\n",
        "en_model.to(device)\n",
        "zh_model.to(device)\n",
        "regressor.to(device)\n",
        "\n",
        "print(en_model)\n",
        "print(zh_model)\n",
        "\n",
        "LR = 0.003\n",
        "\n",
        "en_opt = torch.optim.Adam(en_model.parameters(), lr=LR)\n",
        "zh_opt = torch.optim.Adam(zh_model.parameters(), lr=LR)\n",
        "regressor_opt = torch.optim.Adam(regressor.parameters(), lr=LR)\n",
        "\n",
        "def RMSELoss(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred - target) ** 2))\n",
        "\n",
        "loss_fn = RMSELoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MOiNoElbLD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(en_tensor, zh_tensor, score):\n",
        "    en_model.zero_grad()\n",
        "    zh_model.zero_grad()\n",
        "\n",
        "    en_hidden = en_model.init_hidden()\n",
        "\n",
        "    for word_idx in en_tensor:\n",
        "        hids, en_hidden = en_model(word_idx, en_hidden)\n",
        "        print('Hids', hids.shape)\n",
        "    \n",
        "    # print('EN final hidden state', en_hidden)\n",
        "\n",
        "    zh_hidden = en_hidden\n",
        "    for word_idx in zh_tensor:\n",
        "        _, zh_hidden = zh_model(word_idx, zh_hidden)\n",
        "    \n",
        "    # print('ZH final hidden state', zh_hidden)\n",
        "\n",
        "    pred_score = regressor(zh_hidden).squeeze()\n",
        "\n",
        "    loss = loss_fn(pred_score, score)\n",
        "    \n",
        "    # print('Loss', loss)    \n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    regressor_opt.step()\n",
        "    zh_opt.step()\n",
        "    en_opt.step()\n",
        "\n",
        "    return loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzmHLS82c8Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for eidx in range(100):\n",
        "    loss = 0\n",
        "    for (en_tensor, zh_tensor), score in train_set[:100]:\n",
        "        loss += train(en_tensor.to(device), zh_tensor.to(device), score)\n",
        "    loss /= 100\n",
        "    print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW6b5QrLdOnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}