{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QEV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMxyHYsX5EVGkfj/dm0J7mw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansonmiu0214/C490CW/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEKvQGwiJhlX",
        "colab_type": "text"
      },
      "source": [
        "# Coursework: BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70hpBC8gP-Ck",
        "colab_type": "code",
        "outputId": "dec31311-db0d-44a4-c0b9-099981aa8765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.15)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.15)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSN72uyXX8zr",
        "colab_type": "code",
        "outputId": "21decd3c-ee61-4dd0-ddaa-a770db05009e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sklearn\n",
        "import tqdm\n",
        "\n",
        "from pytorch_pretrained_bert import BertConfig, BertTokenizer, BertForSequenceClassification, BertModel\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'DEVICE={device}')\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=device))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE=cuda\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 4            |        cudaMalloc retries: 4         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  734822 KB |   10932 MB |    2097 GB |    2096 GB |\n",
            "|       from large pool |  734234 KB |   10930 MB |    2096 GB |    2095 GB |\n",
            "|       from small pool |     588 KB |       1 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  734822 KB |   10932 MB |    2097 GB |    2096 GB |\n",
            "|       from large pool |  734234 KB |   10930 MB |    2096 GB |    2095 GB |\n",
            "|       from small pool |     588 KB |       1 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     970 MB |   11114 MB |   11114 MB |   10144 MB |\n",
            "|       from large pool |     968 MB |   11112 MB |   11112 MB |   10144 MB |\n",
            "|       from small pool |       2 MB |       2 MB |       2 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  258457 KB |    4346 MB |    1836 GB |    1836 GB |\n",
            "|       from large pool |  256998 KB |    4345 MB |    1836 GB |    1835 GB |\n",
            "|       from small pool |    1459 KB |       1 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     209    |     952    |  177570    |  177361    |\n",
            "|       from large pool |      79    |     747    |  145075    |  144996    |\n",
            "|       from small pool |     130    |     205    |   32495    |   32365    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     209    |     952    |  177570    |  177361    |\n",
            "|       from large pool |      79    |     747    |  145075    |  144996    |\n",
            "|       from small pool |     130    |     205    |   32495    |   32365    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |     158    |     158    |     134    |\n",
            "|       from large pool |      23    |     157    |     157    |     134    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      30    |     278    |  125590    |  125560    |\n",
            "|       from large pool |      25    |     267    |  106310    |  106285    |\n",
            "|       from small pool |       5    |      14    |   19280    |   19275    |\n",
            "|===========================================================================|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW0tBzjD6zEw",
        "colab_type": "code",
        "outputId": "ae71afda-0074-4a62-bb50-c11d3f3461e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "# Google Drive authorisation\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ls /content/gdrive/My\\ Drive\n",
        "\n",
        "def in_gdrive(path):\n",
        "    return os.path.join('/content/gdrive/My Drive/Colab Notebooks', path)\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "'2009-2016 Sha Tin College'\t\t     Experience.gdoc\n",
            "\"2016 ESF Chairman's Award for Excellence\"  'Eye Travel.gslides'\n",
            "'2016 Japan Grad Trip'\t\t\t     fwsiaappchallengecountingfbwastage\n",
            " 2016_STC-Summer-Intern\t\t\t    'Google Photos'\n",
            " 2017_STC-Summer-Intern\t\t\t    'HK 2019.gdoc'\n",
            " 2018-10_SIA-Challenge\t\t\t     HKSES\n",
            " 2018_Hack-a-Project\t\t\t    'Imperial College London'\n",
            "'2018 Log.gsheet'\t\t\t     Imperial_Student-ID.PNG\n",
            "'2018 Plan.gdoc'\t\t\t    'K-Fold Cross Validation.gdoc'\n",
            " 2018_SIA-Challenge-Finale\t\t    'L7E US to UK Button Codes.gsheet'\n",
            " 2018_Summer-CX\t\t\t\t    'Level7 Education'\n",
            " 2019-2020_Accommodation\t\t     MEng_Confirmation.pdf\n",
            " 2019_Airbus-FYI\t\t\t     Miscellaneous\n",
            " 2019_Bloomberg\t\t\t\t     NewPoliceRegistration.pdf\n",
            " 2019_SIA-Accelerator\t\t\t     Personal\n",
            " 2020_Graduate\t\t\t\t    'Placement Report Plan.gdoc'\n",
            " 2020_JP-Code-for-Good\t\t\t     Portfolio\n",
            " _Archive\t\t\t\t     Private\n",
            " AWS\t\t\t\t\t     Public\n",
            " Backlog.gdoc\t\t\t\t    'Road Trip 2019 - Wales.gmap'\n",
            " C317_Graphics\t\t\t\t     RoadTrips\n",
            "'Car Comparison.gsheet'\t\t\t     ScannerPro\n",
            "'Colab Notebooks'\t\t\t    'SE Panel.gdoc'\n",
            " CV\t\t\t\t\t    'SIA challenge'\n",
            "'CV_Anson-Miu_2019 - Anson Miu.pdf'\t     StackEdit\n",
            " CV_Anson-Miu_2019.pdf\t\t\t    'Structuring Software Systems.gdoc'\n",
            "'CV Master.gdoc'\t\t\t    'td51(2017.05)_e-fillable_eng.pdf'\n",
            "'CV Tips.gdoc'\t\t\t\t    'Untitled document (1).gdoc'\n",
            " Dev\t\t\t\t\t    'Untitled document.gdoc'\n",
            "'EDF Energy.gsheet'\t\t\t    'Untitled spreadsheet.gsheet'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JV59lAFJ2Tv",
        "colab_type": "text"
      },
      "source": [
        "## Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rjr6T2qJlxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "3aa491a4-8ee3-4061-a805-13edc8050c56"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip\n",
        "\n",
        "TRAIN_EN = 'train.enzh.src'\n",
        "TRAIN_ZH = 'train.enzh.mt'\n",
        "TRAIN_SCORES = 'train.enzh.scores'\n",
        "VAL_EN = 'dev.enzh.src'\n",
        "VAL_ZH = 'dev.enzh.mt'\n",
        "VAL_SCORES = 'dev.enzh.scores'\n",
        "TEST_EN = 'test.enzh.src'\n",
        "TEST_ZH = 'test.enzh.mt'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 09:21:35--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=1b47b59cc0981dd597d58b6bed74ee91042e312e9e53150a0d31b0e0b7b2558f&X-Amz-Date=20200217T092135Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200217%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-17 09:21:35--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=1b47b59cc0981dd597d58b6bed74ee91042e312e9e53150a0d31b0e0b7b2558f&X-Amz-Date=20200217T092135Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200217%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "\renzh_data.zip         0%[                    ]       0  --.-KB/s               \renzh_data.zip       100%[===================>] 850.48K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-02-17 09:21:36 (16.3 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMBmQxeIQOPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read from file\n",
        "\n",
        "with open(TRAIN_EN) as f:\n",
        "    train_en = f.readlines()\n",
        "with open(TRAIN_ZH) as f:\n",
        "    train_zh = f.readlines()\n",
        "with open(TRAIN_SCORES) as f:\n",
        "    train_scores = [float(score.strip()) for score in f]\n",
        "with open(VAL_EN) as f:\n",
        "    val_en = f.readlines()\n",
        "with open(VAL_ZH) as f:\n",
        "    val_zh = f.readlines()\n",
        "with open(VAL_SCORES) as f:\n",
        "    val_scores = [float(score.strip()) for score in f]\n",
        "with open(TEST_EN) as f:\n",
        "    test_en = f.readlines()\n",
        "with open(TEST_ZH) as f:\n",
        "    test_zh = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkP11o2TStyB",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJWNZ4dWvFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_pairs_to_bert_input(tokenizer, *sents, max_seq_length=256):\n",
        "    assert len(sents) > 0, 'No sentences to tokenise!'\n",
        "\n",
        "    bert_inputs = []\n",
        "    num_bert_markers = 3\n",
        "\n",
        "    for sent_group in zip(*sents):\n",
        "        sent_tokens = [tokenizer.tokenize(sent) for sent in sent_group]\n",
        "\n",
        "        total_length = sum([len(sent) for sent in sent_tokens]) + num_bert_markers\n",
        "        if total_length > max_seq_length:\n",
        "            raise Exception(f'Too long ({total_length})')\n",
        "\n",
        "        tokens = ['[CLS]']\n",
        "        for sent in sent_tokens:\n",
        "            tokens += sent\n",
        "            tokens.append('[SEP]')\n",
        "\n",
        "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        padding = [0] * (max_seq_length - len(ids))\n",
        "\n",
        "        ids_tensor = torch.LongTensor(ids + padding)\n",
        "\n",
        "        bert_inputs.append(ids_tensor)\n",
        "\n",
        "    return torch.stack(bert_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah79MPhKZO9v",
        "colab_type": "text"
      },
      "source": [
        "## BERT Baseline\n",
        "\n",
        "* Multilingual tokeniser\n",
        "* Pretrained model to get sentence pair embeddings\n",
        "* Regression model on sentence pair embeddinsg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZvG5Mc2Zh6R",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e72972a-ab76-45e4-c883-7c2f8cd8c944",
        "id": "2GYOY1uxZkqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "BERT_PRETRAINED_MODEL = 'bert-base-multilingual-cased'\n",
        "\n",
        "# Load tokenizer\n",
        "print(f'Loading tokenizer <{BERT_PRETRAINED_MODEL}>...', end=' ')\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_MODEL,\n",
        "                                          do_lower_case=False)\n",
        "print('done!')\n",
        "print()\n",
        "\n",
        "# Check tokenizer\n",
        "sample_sent_id = 42\n",
        "\n",
        "print('English')\n",
        "print(train_en[sample_sent_id])\n",
        "print(tokenizer.tokenize(train_en[sample_sent_id]))\n",
        "print()\n",
        "\n",
        "print('Chinese')\n",
        "print(train_zh[sample_sent_id])\n",
        "print(tokenizer.tokenize(train_zh[sample_sent_id]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading tokenizer <bert-base-multilingual-cased>... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 2009422.95B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done!\n",
            "\n",
            "English\n",
            "All 6 of the artillerymen recorded as wounded died).\n",
            "\n",
            "['All', '6', 'of', 'the', 'artillery', '##men', 'recorded', 'as', 'wounded', 'died', ')', '.']\n",
            "\n",
            "Chinese\n",
            "据记录 ， 所有 6 名炮兵都受伤了) 。\n",
            "\n",
            "['据', '记', '录', '，', '所', '有', '6', '名', '炮', '兵', '都', '受', '伤', '了', ')', '。']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvbsh2hjZVk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = sentence_pairs_to_bert_input(tokenizer, train_en, train_zh, max_seq_length=132)\n",
        "val_inputs = sentence_pairs_to_bert_input(tokenizer, val_en, val_zh, max_seq_length=132)\n",
        "test_inputs = sentence_pairs_to_bert_input(tokenizer, test_en, test_zh, max_seq_length=132)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dyuIS-raCRl",
        "colab_type": "text"
      },
      "source": [
        "### Sentence pair embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtwLH12QaIBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def get_bert_embeddings(bert, input_ids):\n",
        "\n",
        "    idxs = np.arange(len(input_ids))\n",
        "    np.random.shuffle(idxs)\n",
        "\n",
        "    batch_size = 25\n",
        "    num_batches = math.ceil(len(input_ids) / batch_size)\n",
        "\n",
        "    embs = []\n",
        "\n",
        "    for batch_id in range(num_batches):\n",
        "        print(f'Batch {batch_id + 1}/{num_batches}...', end='')\n",
        "        start_id = batch_id * batch_size\n",
        "        end_id = (batch_id + 1) * batch_size\n",
        "        input_id_batch = input_ids[idxs[start_id:end_id]]\n",
        "\n",
        "        _, emb = bert(input_id_batch.to(device))\n",
        "        embs.append(emb.detach().cpu())\n",
        "        print('done!')\n",
        "\n",
        "    return embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9naCFrJKckUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc003644-18f0-47d6-97cf-4017bc42a65e"
      },
      "source": [
        "bert = BertModel.from_pretrained(BERT_PRETRAINED_MODEL)\n",
        "bert.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 662804195/662804195 [00:25<00:00, 26400332.42B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hr-JHZ8a5nC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73fe7359-d028-435c-a342-31115a1433f4"
      },
      "source": [
        "train_embeddings = get_bert_embeddings(bert, train_inputs)\n",
        "train_embeddings_tensor = torch.cat(train_embeddings)\n",
        "\n",
        "\n",
        "print(train_embeddings_tensor.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1/280...done!\n",
            "Batch 2/280...done!\n",
            "Batch 3/280...done!\n",
            "Batch 4/280...done!\n",
            "Batch 5/280...done!\n",
            "Batch 6/280...done!\n",
            "Batch 7/280...done!\n",
            "Batch 8/280...done!\n",
            "Batch 9/280...done!\n",
            "Batch 10/280...done!\n",
            "Batch 11/280...done!\n",
            "Batch 12/280...done!\n",
            "Batch 13/280...done!\n",
            "Batch 14/280...done!\n",
            "Batch 15/280...done!\n",
            "Batch 16/280...done!\n",
            "Batch 17/280...done!\n",
            "Batch 18/280...done!\n",
            "Batch 19/280...done!\n",
            "Batch 20/280...done!\n",
            "Batch 21/280...done!\n",
            "Batch 22/280...done!\n",
            "Batch 23/280...done!\n",
            "Batch 24/280...done!\n",
            "Batch 25/280...done!\n",
            "Batch 26/280...done!\n",
            "Batch 27/280...done!\n",
            "Batch 28/280...done!\n",
            "Batch 29/280...done!\n",
            "Batch 30/280...done!\n",
            "Batch 31/280...done!\n",
            "Batch 32/280...done!\n",
            "Batch 33/280...done!\n",
            "Batch 34/280...done!\n",
            "Batch 35/280...done!\n",
            "Batch 36/280...done!\n",
            "Batch 37/280...done!\n",
            "Batch 38/280...done!\n",
            "Batch 39/280...done!\n",
            "Batch 40/280...done!\n",
            "Batch 41/280...done!\n",
            "Batch 42/280...done!\n",
            "Batch 43/280...done!\n",
            "Batch 44/280...done!\n",
            "Batch 45/280...done!\n",
            "Batch 46/280...done!\n",
            "Batch 47/280...done!\n",
            "Batch 48/280...done!\n",
            "Batch 49/280...done!\n",
            "Batch 50/280...done!\n",
            "Batch 51/280...done!\n",
            "Batch 52/280...done!\n",
            "Batch 53/280...done!\n",
            "Batch 54/280...done!\n",
            "Batch 55/280...done!\n",
            "Batch 56/280...done!\n",
            "Batch 57/280...done!\n",
            "Batch 58/280...done!\n",
            "Batch 59/280...done!\n",
            "Batch 60/280...done!\n",
            "Batch 61/280...done!\n",
            "Batch 62/280...done!\n",
            "Batch 63/280...done!\n",
            "Batch 64/280...done!\n",
            "Batch 65/280...done!\n",
            "Batch 66/280...done!\n",
            "Batch 67/280...done!\n",
            "Batch 68/280...done!\n",
            "Batch 69/280...done!\n",
            "Batch 70/280...done!\n",
            "Batch 71/280...done!\n",
            "Batch 72/280...done!\n",
            "Batch 73/280...done!\n",
            "Batch 74/280...done!\n",
            "Batch 75/280...done!\n",
            "Batch 76/280...done!\n",
            "Batch 77/280...done!\n",
            "Batch 78/280...done!\n",
            "Batch 79/280...done!\n",
            "Batch 80/280...done!\n",
            "Batch 81/280...done!\n",
            "Batch 82/280...done!\n",
            "Batch 83/280...done!\n",
            "Batch 84/280...done!\n",
            "Batch 85/280...done!\n",
            "Batch 86/280...done!\n",
            "Batch 87/280...done!\n",
            "Batch 88/280...done!\n",
            "Batch 89/280...done!\n",
            "Batch 90/280...done!\n",
            "Batch 91/280...done!\n",
            "Batch 92/280...done!\n",
            "Batch 93/280...done!\n",
            "Batch 94/280...done!\n",
            "Batch 95/280...done!\n",
            "Batch 96/280...done!\n",
            "Batch 97/280...done!\n",
            "Batch 98/280...done!\n",
            "Batch 99/280...done!\n",
            "Batch 100/280...done!\n",
            "Batch 101/280...done!\n",
            "Batch 102/280...done!\n",
            "Batch 103/280...done!\n",
            "Batch 104/280...done!\n",
            "Batch 105/280...done!\n",
            "Batch 106/280...done!\n",
            "Batch 107/280...done!\n",
            "Batch 108/280...done!\n",
            "Batch 109/280...done!\n",
            "Batch 110/280...done!\n",
            "Batch 111/280...done!\n",
            "Batch 112/280...done!\n",
            "Batch 113/280...done!\n",
            "Batch 114/280...done!\n",
            "Batch 115/280...done!\n",
            "Batch 116/280...done!\n",
            "Batch 117/280...done!\n",
            "Batch 118/280...done!\n",
            "Batch 119/280...done!\n",
            "Batch 120/280...done!\n",
            "Batch 121/280...done!\n",
            "Batch 122/280...done!\n",
            "Batch 123/280...done!\n",
            "Batch 124/280...done!\n",
            "Batch 125/280...done!\n",
            "Batch 126/280...done!\n",
            "Batch 127/280...done!\n",
            "Batch 128/280...done!\n",
            "Batch 129/280...done!\n",
            "Batch 130/280...done!\n",
            "Batch 131/280...done!\n",
            "Batch 132/280...done!\n",
            "Batch 133/280...done!\n",
            "Batch 134/280...done!\n",
            "Batch 135/280...done!\n",
            "Batch 136/280...done!\n",
            "Batch 137/280...done!\n",
            "Batch 138/280...done!\n",
            "Batch 139/280...done!\n",
            "Batch 140/280...done!\n",
            "Batch 141/280...done!\n",
            "Batch 142/280...done!\n",
            "Batch 143/280...done!\n",
            "Batch 144/280...done!\n",
            "Batch 145/280...done!\n",
            "Batch 146/280...done!\n",
            "Batch 147/280...done!\n",
            "Batch 148/280...done!\n",
            "Batch 149/280...done!\n",
            "Batch 150/280...done!\n",
            "Batch 151/280...done!\n",
            "Batch 152/280...done!\n",
            "Batch 153/280...done!\n",
            "Batch 154/280...done!\n",
            "Batch 155/280...done!\n",
            "Batch 156/280...done!\n",
            "Batch 157/280...done!\n",
            "Batch 158/280...done!\n",
            "Batch 159/280...done!\n",
            "Batch 160/280...done!\n",
            "Batch 161/280...done!\n",
            "Batch 162/280...done!\n",
            "Batch 163/280...done!\n",
            "Batch 164/280...done!\n",
            "Batch 165/280...done!\n",
            "Batch 166/280...done!\n",
            "Batch 167/280...done!\n",
            "Batch 168/280...done!\n",
            "Batch 169/280...done!\n",
            "Batch 170/280...done!\n",
            "Batch 171/280...done!\n",
            "Batch 172/280...done!\n",
            "Batch 173/280...done!\n",
            "Batch 174/280...done!\n",
            "Batch 175/280...done!\n",
            "Batch 176/280...done!\n",
            "Batch 177/280...done!\n",
            "Batch 178/280...done!\n",
            "Batch 179/280...done!\n",
            "Batch 180/280...done!\n",
            "Batch 181/280...done!\n",
            "Batch 182/280...done!\n",
            "Batch 183/280...done!\n",
            "Batch 184/280...done!\n",
            "Batch 185/280...done!\n",
            "Batch 186/280...done!\n",
            "Batch 187/280...done!\n",
            "Batch 188/280...done!\n",
            "Batch 189/280...done!\n",
            "Batch 190/280...done!\n",
            "Batch 191/280...done!\n",
            "Batch 192/280...done!\n",
            "Batch 193/280...done!\n",
            "Batch 194/280...done!\n",
            "Batch 195/280...done!\n",
            "Batch 196/280...done!\n",
            "Batch 197/280...done!\n",
            "Batch 198/280...done!\n",
            "Batch 199/280...done!\n",
            "Batch 200/280...done!\n",
            "Batch 201/280...done!\n",
            "Batch 202/280...done!\n",
            "Batch 203/280...done!\n",
            "Batch 204/280...done!\n",
            "Batch 205/280...done!\n",
            "Batch 206/280...done!\n",
            "Batch 207/280...done!\n",
            "Batch 208/280...done!\n",
            "Batch 209/280...done!\n",
            "Batch 210/280...done!\n",
            "Batch 211/280...done!\n",
            "Batch 212/280...done!\n",
            "Batch 213/280...done!\n",
            "Batch 214/280...done!\n",
            "Batch 215/280...done!\n",
            "Batch 216/280...done!\n",
            "Batch 217/280...done!\n",
            "Batch 218/280...done!\n",
            "Batch 219/280...done!\n",
            "Batch 220/280...done!\n",
            "Batch 221/280...done!\n",
            "Batch 222/280...done!\n",
            "Batch 223/280...done!\n",
            "Batch 224/280...done!\n",
            "Batch 225/280...done!\n",
            "Batch 226/280...done!\n",
            "Batch 227/280...done!\n",
            "Batch 228/280...done!\n",
            "Batch 229/280...done!\n",
            "Batch 230/280...done!\n",
            "Batch 231/280...done!\n",
            "Batch 232/280...done!\n",
            "Batch 233/280...done!\n",
            "Batch 234/280...done!\n",
            "Batch 235/280...done!\n",
            "Batch 236/280...done!\n",
            "Batch 237/280...done!\n",
            "Batch 238/280...done!\n",
            "Batch 239/280...done!\n",
            "Batch 240/280...done!\n",
            "Batch 241/280...done!\n",
            "Batch 242/280...done!\n",
            "Batch 243/280...done!\n",
            "Batch 244/280...done!\n",
            "Batch 245/280...done!\n",
            "Batch 246/280...done!\n",
            "Batch 247/280...done!\n",
            "Batch 248/280...done!\n",
            "Batch 249/280...done!\n",
            "Batch 250/280...done!\n",
            "Batch 251/280...done!\n",
            "Batch 252/280...done!\n",
            "Batch 253/280...done!\n",
            "Batch 254/280...done!\n",
            "Batch 255/280...done!\n",
            "Batch 256/280...done!\n",
            "Batch 257/280...done!\n",
            "Batch 258/280...done!\n",
            "Batch 259/280...done!\n",
            "Batch 260/280...done!\n",
            "Batch 261/280...done!\n",
            "Batch 262/280...done!\n",
            "Batch 263/280...done!\n",
            "Batch 264/280...done!\n",
            "Batch 265/280...done!\n",
            "Batch 266/280...done!\n",
            "Batch 267/280...done!\n",
            "Batch 268/280...done!\n",
            "Batch 269/280...done!\n",
            "Batch 270/280...done!\n",
            "Batch 271/280...done!\n",
            "Batch 272/280...done!\n",
            "Batch 273/280...done!\n",
            "Batch 274/280...done!\n",
            "Batch 275/280...done!\n",
            "Batch 276/280...done!\n",
            "Batch 277/280...done!\n",
            "Batch 278/280...done!\n",
            "Batch 279/280...done!\n",
            "Batch 280/280...done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X2g2trWeCjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44e63914-825e-4cd7-e607-df9f20b86aa4"
      },
      "source": [
        "val_embeddings = get_bert_embeddings(bert, val_inputs)\n",
        "val_embeddings_tensor = torch.cat(val_embeddings)\n",
        "\n",
        "print(f'Validation embeddings', val_embeddings_tensor.shape)\n",
        "\n",
        "test_embeddings = get_bert_embeddings(bert, test_inputs)\n",
        "test_embeddings_tensor = torch.cat(test_embeddings)\n",
        "\n",
        "print(f'Test embeddings', test_embeddings_tensor.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1/40...done!\n",
            "Batch 2/40...done!\n",
            "Batch 3/40...done!\n",
            "Batch 4/40...done!\n",
            "Batch 5/40...done!\n",
            "Batch 6/40...done!\n",
            "Batch 7/40...done!\n",
            "Batch 8/40...done!\n",
            "Batch 9/40...done!\n",
            "Batch 10/40...done!\n",
            "Batch 11/40...done!\n",
            "Batch 12/40...done!\n",
            "Batch 13/40...done!\n",
            "Batch 14/40...done!\n",
            "Batch 15/40...done!\n",
            "Batch 16/40...done!\n",
            "Batch 17/40...done!\n",
            "Batch 18/40...done!\n",
            "Batch 19/40...done!\n",
            "Batch 20/40...done!\n",
            "Batch 21/40...done!\n",
            "Batch 22/40...done!\n",
            "Batch 23/40...done!\n",
            "Batch 24/40...done!\n",
            "Batch 25/40...done!\n",
            "Batch 26/40...done!\n",
            "Batch 27/40...done!\n",
            "Batch 28/40...done!\n",
            "Batch 29/40...done!\n",
            "Batch 30/40...done!\n",
            "Batch 31/40...done!\n",
            "Batch 32/40...done!\n",
            "Batch 33/40...done!\n",
            "Batch 34/40...done!\n",
            "Batch 35/40...done!\n",
            "Batch 36/40...done!\n",
            "Batch 37/40...done!\n",
            "Batch 38/40...done!\n",
            "Batch 39/40...done!\n",
            "Batch 40/40...done!\n",
            "Validation embeddings torch.Size([1000, 768])\n",
            "Batch 1/40...done!\n",
            "Batch 2/40...done!\n",
            "Batch 3/40...done!\n",
            "Batch 4/40...done!\n",
            "Batch 5/40...done!\n",
            "Batch 6/40...done!\n",
            "Batch 7/40...done!\n",
            "Batch 8/40...done!\n",
            "Batch 9/40...done!\n",
            "Batch 10/40...done!\n",
            "Batch 11/40...done!\n",
            "Batch 12/40...done!\n",
            "Batch 13/40...done!\n",
            "Batch 14/40...done!\n",
            "Batch 15/40...done!\n",
            "Batch 16/40...done!\n",
            "Batch 17/40...done!\n",
            "Batch 18/40...done!\n",
            "Batch 19/40...done!\n",
            "Batch 20/40...done!\n",
            "Batch 21/40...done!\n",
            "Batch 22/40...done!\n",
            "Batch 23/40...done!\n",
            "Batch 24/40...done!\n",
            "Batch 25/40...done!\n",
            "Batch 26/40...done!\n",
            "Batch 27/40...done!\n",
            "Batch 28/40...done!\n",
            "Batch 29/40...done!\n",
            "Batch 30/40...done!\n",
            "Batch 31/40...done!\n",
            "Batch 32/40...done!\n",
            "Batch 33/40...done!\n",
            "Batch 34/40...done!\n",
            "Batch 35/40...done!\n",
            "Batch 36/40...done!\n",
            "Batch 37/40...done!\n",
            "Batch 38/40...done!\n",
            "Batch 39/40...done!\n",
            "Batch 40/40...done!\n",
            "Test embeddings torch.Size([1000, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kFZY6_Lkd9c",
        "colab_type": "text"
      },
      "source": [
        "### Regression Layer: SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM5UpkaLfY-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "738e0add-5fca-47ed-f309-d5614f7b98d8"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "def RMSELoss(pred, target, *, numpy=False):\n",
        "    mean = np.mean if numpy else torch.mean\n",
        "    sqrt = np.sqrt if numpy else torch.sqrt\n",
        "    return sqrt(mean((pred - target) ** 2))\n",
        "\n",
        "for kernel in ('linear', 'poly', 'rbf', 'sigmoid'):\n",
        "    for epsilon in (0, 0.1, 0.2, 0.3):\n",
        "        print(f'Kernel: {kernel}, Epsilon: {epsilon}')\n",
        "        print('=' * 10)\n",
        "        clf_t = SVR(kernel=kernel, epsilon=epsilon)\n",
        "        print('Fitting...', end='')\n",
        "        clf_t.fit(train_embeddings_tensor.numpy(), train_scores)\n",
        "        print('done!')\n",
        "\n",
        "        print('Predicting...', end='')\n",
        "        predictions = clf_t.predict(val_embeddings_tensor.numpy())\n",
        "        print('done!')\n",
        "        pearson, _ = pearsonr(predictions, val_scores)\n",
        "        print(f'Pearson = {pearson}')\n",
        "        loss = RMSELoss(predictions, val_scores, numpy=True)\n",
        "        print(f'RMSE = {loss}')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kernel: linear, Epsilon: 0\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.04780753684606238\n",
            "RMSE = 0.9917875610063995\n",
            "Kernel: linear, Epsilon: 0.1\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.048342162974197314\n",
            "RMSE = 0.9882644365461329\n",
            "Kernel: linear, Epsilon: 0.2\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.03504630613384057\n",
            "RMSE = 0.9779849925656622\n",
            "Kernel: linear, Epsilon: 0.3\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.050145934730393645\n",
            "RMSE = 0.9754161159349126\n",
            "Kernel: poly, Epsilon: 0\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.06002450036091897\n",
            "RMSE = 0.9784465582539243\n",
            "Kernel: poly, Epsilon: 0.1\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.05047982830438921\n",
            "RMSE = 0.9718200251269526\n",
            "Kernel: poly, Epsilon: 0.2\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.04626553933121432\n",
            "RMSE = 0.9663320855801729\n",
            "Kernel: poly, Epsilon: 0.3\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.051814586437438685\n",
            "RMSE = 0.959958163026224\n",
            "Kernel: rbf, Epsilon: 0\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.0653140581655382\n",
            "RMSE = 0.9743474090229812\n",
            "Kernel: rbf, Epsilon: 0.1\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.04526682930173165\n",
            "RMSE = 0.9697188873694318\n",
            "Kernel: rbf, Epsilon: 0.2\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.04532607577737477\n",
            "RMSE = 0.9644448209927587\n",
            "Kernel: rbf, Epsilon: 0.3\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = -0.051176673622751065\n",
            "RMSE = 0.9569319043007609\n",
            "Kernel: sigmoid, Epsilon: 0\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = 0.022835297517929738\n",
            "RMSE = 46.7586482683057\n",
            "Kernel: sigmoid, Epsilon: 0.1\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = 0.022846244631402928\n",
            "RMSE = 46.76262865962607\n",
            "Kernel: sigmoid, Epsilon: 0.2\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = 0.022847707749839206\n",
            "RMSE = 46.754870553923624\n",
            "Kernel: sigmoid, Epsilon: 0.3\n",
            "==========\n",
            "Fitting...done!\n",
            "Predicting...done!\n",
            "Pearson = 0.022801135906283937\n",
            "RMSE = 46.79241693262682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YjFuSpDsZkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "e9b0c853-c45d-4e63-d9ac-2f13aef039e9"
      },
      "source": [
        "clf_t = SVR(kernel='sigmoid', epsilon=0.1)\n",
        "print('Fitting...', end='')\n",
        "clf_t.fit(train_embeddings_tensor.numpy(), train_scores)\n",
        "print('done!')\n",
        "\n",
        "print('Predicting...', end='')\n",
        "predictions = clf_t.predict(val_embeddings_tensor.numpy())\n",
        "print('done!')\n",
        "\n",
        "plt.scatter(predictions, val_scores)\n",
        "plt.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting...done!\n",
            "Predicting...done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdWepeVTkgwG",
        "colab_type": "text"
      },
      "source": [
        "### Regression Layer: FFNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge8Godi0kmhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNNRegression(nn.Module):\n",
        "\n",
        "    def __init__(self, *hidden_dims, input_dim=768):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "\n",
        "        prev_dim = input_dim\n",
        "        for i, hidden_dim in enumerate(self.hidden_dims):\n",
        "            setattr(self, f'hidden_{i}', nn.Linear(prev_dim, hidden_dim))\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        self.out = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        act = x\n",
        "        for i, _ in enumerate(self.hidden_dims):\n",
        "            layer = getattr(self, f'hidden_{i}')\n",
        "            act = F.relu(layer(act))\n",
        "        return self.out(act)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYcm-hTKlF6t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "031b5db0-2b36-4871-eff3-adb2ee5ba38f"
      },
      "source": [
        "regressor = FFNNRegression(100)\n",
        "opt = torch.optim.Adam(regressor.parameters(), lr=1e-3)\n",
        "\n",
        "print(regressor)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNNRegression(\n",
            "  (hidden_0): Linear(in_features=768, out_features=100, bias=True)\n",
            "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bV9jew4lSTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def get_mini_batches(inputs, scores, *, batch_size=8):\n",
        "\n",
        "    # idxs = np.arange(len(inputs))\n",
        "    # np.random.shuffle(idxs)\n",
        "\n",
        "    num_batches = math.ceil(len(inputs) / batch_size)\n",
        "\n",
        "    for batch_id in range(num_batches):\n",
        "        start_id = batch_id * batch_size\n",
        "        end_id = (batch_id + 1) * batch_size\n",
        "        yield inputs[start_id:end_id], scores[start_id:end_id]\n",
        "\n",
        "def gradient_descent(model, loss_fn, optimiser, input_ids, scores):\n",
        "    model.train()\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    predictions = model(input_ids.to(device)).squeeze()\n",
        "    \n",
        "    loss = loss_fn(predictions, torch.Tensor(scores).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimiser.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train(model, loss_fn, optimiser, train_inputs, train_scores, val_inputs, val_scores, *, num_epochs=10, batch_size=32):\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        print(f'Epoch #{epoch_idx + 1}')\n",
        "\n",
        "        model.to(device)\n",
        "        \n",
        "        processed = 0\n",
        "        increment = 0.05\n",
        "        milestone = 0.05\n",
        "        print('Training', end='')\n",
        "        for input_ids, scores in get_mini_batches(train_inputs, train_scores, batch_size=batch_size):\n",
        "            loss = gradient_descent(model, loss_fn, optimiser, input_ids, scores)\n",
        "            processed += len(input_ids)\n",
        "\n",
        "            if processed / len(train_inputs) >= milestone:\n",
        "                print('.', end='')\n",
        "                milestone += increment\n",
        "        print('done!')\n",
        "        print('Recent loss', loss)\n",
        "\n",
        "        # Check validation loss\n",
        "        model.eval()\n",
        "        model.to('cpu')\n",
        "\n",
        "        print('Getting validation predictions...', end='')\n",
        "        val_preds = model(val_inputs).squeeze().detach()\n",
        "        print(f'done! <{val_preds[0]}, {val_scores[0]}>..<{val_preds[-1]}, {val_scores[-1]}>')\n",
        "        val_loss = loss_fn(val_preds, torch.Tensor(val_scores))\n",
        "        print('Validation loss', val_loss)\n",
        "\n",
        "        pearson, _ = pearsonr(val_preds.numpy(), np.array(val_scores))\n",
        "        print('Validation Pearson', pearson)\n",
        "\n",
        "        for pred, score in zip(val_preds, val_scores):\n",
        "            print(pred, score)\n",
        "\n",
        "        plt.scatter(val_preds.numpy(), val_scores)\n",
        "        plt.show()\n",
        "        break\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUm6vRaylktd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4bbd436-455d-4332-9a66-62f480c58957"
      },
      "source": [
        "train(regressor, RMSELoss, opt,\n",
        "      train_embeddings_tensor, train_scores,\n",
        "      val_embeddings_tensor, val_scores,\n",
        "      num_epochs=100,\n",
        "      batch_size=100)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Training...................done!\n",
            "Recent loss tensor(0.8892, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! <0.9458951950073242, -1.0228233810174194>..<0.13971376419067383, 0.8825711244830375>\n",
            "Validation loss tensor(1.0797)\n",
            "Validation Pearson -0.05541662524274765\n",
            "tensor(0.9459) -1.0228233810174194\n",
            "tensor(0.0712) 0.23705687653732718\n",
            "tensor(0.2529) -2.1651576284345477\n",
            "tensor(-0.5143) 0.06793956526926585\n",
            "tensor(1.5944) 0.24884471846624953\n",
            "tensor(0.2065) 0.4156528014950242\n",
            "tensor(0.3570) 0.6267628231268476\n",
            "tensor(0.0991) 0.29931892462648196\n",
            "tensor(-0.0755) -0.7360249925919696\n",
            "tensor(0.3427) 0.28170765930435837\n",
            "tensor(1.1194) -1.2674395933311737\n",
            "tensor(0.3334) 0.7168480321858524\n",
            "tensor(0.4514) -1.5238878331324646\n",
            "tensor(-0.2561) 0.03345201987868618\n",
            "tensor(0.4850) 1.02165011148501\n",
            "tensor(-0.7899) 0.48750168026045876\n",
            "tensor(-0.2372) 0.93458780429065\n",
            "tensor(-0.2183) 0.9864170373172753\n",
            "tensor(0.3995) 0.5649320569757931\n",
            "tensor(0.0107) -0.876397895925973\n",
            "tensor(0.3406) -0.5033999688994414\n",
            "tensor(0.1916) 0.7800139500861561\n",
            "tensor(0.2459) -0.2912585116473676\n",
            "tensor(0.7080) 0.07316514209614873\n",
            "tensor(0.2914) 0.24909656207202283\n",
            "tensor(-0.0078) 0.3484771651385466\n",
            "tensor(0.2229) -0.08221713349914939\n",
            "tensor(0.3206) -0.29148666018589814\n",
            "tensor(-0.3837) 0.7658223371589753\n",
            "tensor(-0.0296) 0.5615365429155388\n",
            "tensor(0.5986) 0.7030531713090804\n",
            "tensor(-0.0409) -1.158172817896222\n",
            "tensor(-0.3599) -0.8504679214324153\n",
            "tensor(-0.6470) 0.7343750133114296\n",
            "tensor(0.1831) 0.21936673041543034\n",
            "tensor(0.4672) 1.3232938698705434\n",
            "tensor(-0.0622) -0.5470600463194736\n",
            "tensor(0.8578) 0.5040689196328542\n",
            "tensor(-1.4987) 0.43045168091633296\n",
            "tensor(1.1235) 0.23128707198449908\n",
            "tensor(-0.3302) 0.35068255532711806\n",
            "tensor(-0.1403) 0.41122965949373375\n",
            "tensor(-0.2324) 0.49844803619475314\n",
            "tensor(0.4471) 1.0775821415130438\n",
            "tensor(-0.0485) 0.24181702161139976\n",
            "tensor(-0.9380) 0.4693494418639686\n",
            "tensor(-0.0608) -0.7254015987248118\n",
            "tensor(-0.2821) 0.6390161848366332\n",
            "tensor(0.3692) 1.2060213933417754\n",
            "tensor(0.1537) -0.05769711459858747\n",
            "tensor(0.0943) 1.2121463381627968\n",
            "tensor(0.2591) 0.4354664329177827\n",
            "tensor(-0.1300) 1.1001628652531332\n",
            "tensor(0.1418) 0.4960873740525547\n",
            "tensor(0.5360) 0.19659039468648762\n",
            "tensor(-0.1098) 1.1467502481928225\n",
            "tensor(0.3869) -0.11470117843350318\n",
            "tensor(-0.4804) -0.9446330804251818\n",
            "tensor(-0.3793) -1.0798321892347342\n",
            "tensor(0.3927) 0.30966184358111715\n",
            "tensor(0.0167) -2.0241838851434015\n",
            "tensor(0.2805) 1.013451295598382\n",
            "tensor(-1.1664) 0.47185857007854226\n",
            "tensor(0.4974) -0.8913108724836244\n",
            "tensor(-0.7091) 0.5345882918215583\n",
            "tensor(0.6088) 0.24884471846624953\n",
            "tensor(0.7304) 0.9271872939423437\n",
            "tensor(0.1750) 0.06785489809433014\n",
            "tensor(-0.1177) 0.13293574990380846\n",
            "tensor(-1.1150) 0.3357823425753119\n",
            "tensor(0.0937) 0.31707045424961416\n",
            "tensor(0.0274) -1.1643627557788137\n",
            "tensor(0.2105) -1.6579353022816115\n",
            "tensor(0.0122) 0.17092853748122927\n",
            "tensor(-0.5086) 0.5279395644663522\n",
            "tensor(0.5300) 0.28157638140050006\n",
            "tensor(-0.3958) -1.7361981319155706\n",
            "tensor(-0.3232) 0.1941587366615428\n",
            "tensor(0.0426) 0.3402092117820337\n",
            "tensor(0.5092) 0.8372695579362396\n",
            "tensor(0.1282) 0.26815816115893765\n",
            "tensor(-0.6307) 0.658008958605718\n",
            "tensor(-0.0156) 0.02139165945018201\n",
            "tensor(0.7739) 1.2218366043055333\n",
            "tensor(-0.4677) 0.4949539894576009\n",
            "tensor(-0.1053) 0.4519676854470713\n",
            "tensor(-0.5418) 0.29479997442300915\n",
            "tensor(0.0559) 0.7657439049812904\n",
            "tensor(0.6514) 0.8509579443032967\n",
            "tensor(0.3710) -0.2907636592598379\n",
            "tensor(0.3698) 0.8275928203316397\n",
            "tensor(-0.5260) 0.34585522896104287\n",
            "tensor(-0.2889) 0.2781147613996215\n",
            "tensor(0.0616) -0.06338041265147144\n",
            "tensor(0.0906) -0.07510789121831314\n",
            "tensor(-0.5288) 0.526890881243871\n",
            "tensor(0.7224) 0.3509343989328914\n",
            "tensor(0.4662) 0.7071149843401581\n",
            "tensor(0.4933) -1.257193922823213\n",
            "tensor(-0.1426) -1.3313603935723493\n",
            "tensor(-0.2400) 0.9441996925437532\n",
            "tensor(-0.0646) -1.737469435654212\n",
            "tensor(-0.4947) 0.1604439308644495\n",
            "tensor(0.5799) -1.5612818526180634\n",
            "tensor(0.0821) 0.5861917619439901\n",
            "tensor(-0.4227) 0.014206899300173192\n",
            "tensor(0.3256) -0.7113590217249696\n",
            "tensor(-0.6997) -1.3438850002246638\n",
            "tensor(0.2866) 0.01185804911065419\n",
            "tensor(0.0859) 0.6355184623217515\n",
            "tensor(-0.3102) -0.7487570145874359\n",
            "tensor(0.0278) 0.23872156764682817\n",
            "tensor(0.2948) 0.3498657815237592\n",
            "tensor(-0.0560) -0.10144734521577754\n",
            "tensor(0.4268) -0.37235218067735354\n",
            "tensor(0.5917) -1.4589179700698673\n",
            "tensor(0.6809) -1.44368387407211\n",
            "tensor(0.1746) 1.2082718259365433\n",
            "tensor(-0.7282) 0.24583508765679038\n",
            "tensor(-0.4745) -0.18611774523065852\n",
            "tensor(-0.4710) 0.5813404708227566\n",
            "tensor(-0.3894) 0.4094293943048705\n",
            "tensor(-0.1882) 1.2167220968124264\n",
            "tensor(0.4873) 0.7504843972594836\n",
            "tensor(0.1588) -0.7187648903218355\n",
            "tensor(0.0839) 1.021701265996829\n",
            "tensor(0.0976) 0.8354538407046451\n",
            "tensor(0.3174) -0.08708720449953865\n",
            "tensor(0.4291) -1.4711172289541574\n",
            "tensor(-0.7469) 0.25959118386209673\n",
            "tensor(-0.0681) -0.9459605012138829\n",
            "tensor(0.2604) 0.8059727295745663\n",
            "tensor(-0.4797) 0.6328214858152057\n",
            "tensor(-0.3290) 0.5338780654742742\n",
            "tensor(0.5412) -0.8127796768679719\n",
            "tensor(-1.5051) -0.875937162333393\n",
            "tensor(0.0690) 0.8887773851051491\n",
            "tensor(0.5930) 0.5661339871405273\n",
            "tensor(-0.6852) -1.7477656938147257\n",
            "tensor(-0.5730) 0.40529390052342956\n",
            "tensor(-0.0834) 1.2317311440642864\n",
            "tensor(0.2274) 0.06422227211696245\n",
            "tensor(0.1779) 0.2721758466075774\n",
            "tensor(0.5723) -0.006127639094581805\n",
            "tensor(0.4027) 0.1871244143234483\n",
            "tensor(-0.9447) 1.1838903443932287\n",
            "tensor(0.3554) 0.48575574233378377\n",
            "tensor(0.1562) -1.0467227608918237\n",
            "tensor(-0.0991) -1.8866946676495624\n",
            "tensor(0.4391) 0.6561960659097238\n",
            "tensor(-0.8220) 0.2776473559949963\n",
            "tensor(-0.1089) 0.979196534590748\n",
            "tensor(-0.5137) 0.5107091947951335\n",
            "tensor(-0.8679) 0.8475267699076742\n",
            "tensor(0.7139) 0.20480836713932038\n",
            "tensor(0.7266) -1.5763681029161658\n",
            "tensor(0.2182) 1.1525328637612142\n",
            "tensor(0.3348) 0.23207303638857682\n",
            "tensor(0.0127) 0.4521426267793267\n",
            "tensor(0.1398) 0.21038211656126507\n",
            "tensor(-0.2398) 0.19457371055438852\n",
            "tensor(-0.6766) 0.3835529622201768\n",
            "tensor(0.9923) 0.5325460468891773\n",
            "tensor(-0.5868) 0.5362968953555581\n",
            "tensor(-0.3100) 0.3697775438073549\n",
            "tensor(0.5302) 0.4067960829541503\n",
            "tensor(-1.6868) 0.17999038814540488\n",
            "tensor(-0.4594) 0.843900870261091\n",
            "tensor(-0.1543) 0.7055313408200492\n",
            "tensor(0.6283) 0.9394354762189723\n",
            "tensor(0.0585) 0.13284286979331952\n",
            "tensor(-1.0873) 0.8359510209475017\n",
            "tensor(-0.0627) -0.7676246724372678\n",
            "tensor(-0.0425) 0.24884471846624953\n",
            "tensor(0.5622) 0.1290714697149706\n",
            "tensor(0.4731) 0.7515835557731156\n",
            "tensor(1.0864) 0.9217040325561369\n",
            "tensor(0.1168) 1.069468944448376\n",
            "tensor(-0.6631) -1.148308754146919\n",
            "tensor(-0.8590) 0.23184684906982644\n",
            "tensor(0.3440) 0.9144154055512076\n",
            "tensor(-0.2007) 0.35068255532711806\n",
            "tensor(-0.0834) 0.008684598321751743\n",
            "tensor(-0.3902) -0.06916795417456456\n",
            "tensor(0.0715) -1.912992975303596\n",
            "tensor(0.6043) -1.0044320262735047\n",
            "tensor(-0.1733) -0.5683764124784318\n",
            "tensor(-1.4600) 0.7767692968745631\n",
            "tensor(0.0467) -2.3352783021368\n",
            "tensor(-0.6208) 0.7647120466353986\n",
            "tensor(-0.3637) -1.1488124413584655\n",
            "tensor(-0.8204) 0.656447909515497\n",
            "tensor(1.0481) 0.24407319578899525\n",
            "tensor(-0.0011) 0.24934840567779612\n",
            "tensor(-0.5184) -0.033276857004291865\n",
            "tensor(0.1064) -0.11863102863709842\n",
            "tensor(0.2399) 0.5031874670126476\n",
            "tensor(-1.2089) 0.2619724265138667\n",
            "tensor(0.2344) 0.7880206537500404\n",
            "tensor(0.2752) 0.12881962610919725\n",
            "tensor(0.2823) -0.09120718932818823\n",
            "tensor(0.1901) 0.503439310618421\n",
            "tensor(0.2707) 0.4729148036711148\n",
            "tensor(0.2014) 0.3444609963379122\n",
            "tensor(0.2295) 0.3825327656411622\n",
            "tensor(-0.1870) 0.8508105123590046\n",
            "tensor(0.7158) -0.009120546850257464\n",
            "tensor(0.9036) 0.5421001703628455\n",
            "tensor(-0.5535) 0.6558183005010637\n",
            "tensor(-1.4916) 0.40219197698627035\n",
            "tensor(0.0598) 1.1324595082461422\n",
            "tensor(0.8009) 0.5232208363714105\n",
            "tensor(-0.4551) -1.839518946614681\n",
            "tensor(-0.2831) -1.2492378214755682\n",
            "tensor(-0.5190) 0.053581585934122954\n",
            "tensor(0.3592) -0.4747603574985227\n",
            "tensor(0.6007) -1.2401644339918056\n",
            "tensor(-0.8177) 0.041421229062784226\n",
            "tensor(-0.2371) -1.6610703804640787\n",
            "tensor(-0.1965) 0.2982887763996529\n",
            "tensor(0.6657) 0.6894314160555209\n",
            "tensor(0.4757) 0.6189022514778874\n",
            "tensor(-0.0271) 0.42938626768765475\n",
            "tensor(-0.3348) -1.774425550496251\n",
            "tensor(0.2503) -1.6996678815324915\n",
            "tensor(0.1779) -0.19019516836877748\n",
            "tensor(-0.7618) 0.6747994132921799\n",
            "tensor(0.3940) 0.9201900400328015\n",
            "tensor(-1.9460) 0.6286028939286633\n",
            "tensor(0.3485) -0.613201624331242\n",
            "tensor(0.3307) -0.01069845721679604\n",
            "tensor(0.5291) -0.22693747957002172\n",
            "tensor(0.3426) -0.1864188092722859\n",
            "tensor(-0.2346) -0.49139238288992154\n",
            "tensor(0.0510) 0.584696047485251\n",
            "tensor(0.8147) 0.577563018981211\n",
            "tensor(0.6480) 0.32682865913894876\n",
            "tensor(-0.0219) -2.542682730753373\n",
            "tensor(0.1271) 0.061501675218365116\n",
            "tensor(0.3467) 0.5102640718586869\n",
            "tensor(0.4261) 1.0381072064428676\n",
            "tensor(0.5498) 0.5996098921982371\n",
            "tensor(-0.6401) 0.22602628197371943\n",
            "tensor(-0.4340) 0.6568881674646101\n",
            "tensor(0.5442) 0.5125143432337824\n",
            "tensor(-1.1976) 0.7399725690772953\n",
            "tensor(-0.4946) -1.5392152116464335\n",
            "tensor(-0.0188) 0.7916482201066882\n",
            "tensor(-0.0456) 0.17961262273674494\n",
            "tensor(0.3968) 0.8812410222461281\n",
            "tensor(0.6575) -0.30285031992337613\n",
            "tensor(0.6248) -0.024063050984992212\n",
            "tensor(0.1305) -0.12836755021075316\n",
            "tensor(-0.7747) 0.10010623905228705\n",
            "tensor(-0.2133) -2.0907296812074487\n",
            "tensor(0.6031) 0.3372472432449254\n",
            "tensor(0.9389) -0.2746568706308675\n",
            "tensor(-1.0309) 0.15900251414370414\n",
            "tensor(-0.7714) 0.06558762509549239\n",
            "tensor(0.6098) 1.3279551060746149\n",
            "tensor(0.0687) 0.9315028788639009\n",
            "tensor(0.1468) -0.03319327635640964\n",
            "tensor(0.6515) 0.773164970023223\n",
            "tensor(0.0638) -0.8980784798704375\n",
            "tensor(-0.9578) 1.1247303053453346\n",
            "tensor(0.7185) -2.1515425733843427\n",
            "tensor(-0.2349) -1.4646964481197615\n",
            "tensor(-0.0871) 0.7381814166530528\n",
            "tensor(-0.3089) 0.1388803284853091\n",
            "tensor(-0.0350) 0.24815358234757956\n",
            "tensor(-0.1741) -1.2627451024810312\n",
            "tensor(0.3555) -0.08233866578228945\n",
            "tensor(0.0847) 0.4535059173590909\n",
            "tensor(0.3881) -1.328691489205598\n",
            "tensor(-0.0659) 1.1476009518039854\n",
            "tensor(0.2570) -0.06850447436861062\n",
            "tensor(0.3166) 0.11025159331657554\n",
            "tensor(0.6941) -0.1495685037304425\n",
            "tensor(-0.5360) -0.8966208173737398\n",
            "tensor(0.8661) 0.04752629140219533\n",
            "tensor(-0.7744) 1.181674167805051\n",
            "tensor(1.0834) -0.337462865632496\n",
            "tensor(-0.4187) 0.0661324297093993\n",
            "tensor(0.2409) -1.317326886987103\n",
            "tensor(0.3338) 0.6880233222889576\n",
            "tensor(0.2784) -1.6638948030543432\n",
            "tensor(-0.6593) 0.414229793681612\n",
            "tensor(-0.4377) 0.542723992425879\n",
            "tensor(0.1407) -0.06159451595657619\n",
            "tensor(0.0074) 0.3653862123591238\n",
            "tensor(0.9520) 4.719281924111159e-05\n",
            "tensor(-1.4221) -0.15950804897915094\n",
            "tensor(0.6954) 0.5535445391592225\n",
            "tensor(0.3750) 0.14461513599918333\n",
            "tensor(0.4357) -0.5727918473203556\n",
            "tensor(0.3470) -1.0046054421762356\n",
            "tensor(-0.6653) -1.7669095967987063\n",
            "tensor(-0.6672) 0.33180219539440453\n",
            "tensor(-0.9000) -1.2266414881432013\n",
            "tensor(-0.5618) 0.245593376324892\n",
            "tensor(0.0765) 0.45878996324065446\n",
            "tensor(-0.1093) 0.8725664540185382\n",
            "tensor(-0.4510) 0.6239662814067001\n",
            "tensor(0.3403) 0.34235375200735874\n",
            "tensor(0.3570) 0.4781704876191097\n",
            "tensor(0.1104) -0.9345604588788757\n",
            "tensor(0.8698) 0.1468468192022382\n",
            "tensor(-0.0599) 1.1584580224000012\n",
            "tensor(0.5315) -0.6985107882862601\n",
            "tensor(0.4657) 0.21285456239223358\n",
            "tensor(0.0094) 0.18735007703407067\n",
            "tensor(0.5453) 0.9770036414581971\n",
            "tensor(-1.3140) -0.11431677250968471\n",
            "tensor(-0.8905) 1.2365141042890107\n",
            "tensor(0.8661) -0.34736073933733896\n",
            "tensor(-0.3195) -1.0787151899930452\n",
            "tensor(-0.4110) 0.03805990352336702\n",
            "tensor(0.9672) 0.7291133563883375\n",
            "tensor(-1.6288) 0.21658869879868695\n",
            "tensor(0.0786) 0.8011306728763601\n",
            "tensor(0.2401) 0.30360185125727157\n",
            "tensor(1.4462) 0.33556092257660675\n",
            "tensor(-0.3655) 0.9116856385026605\n",
            "tensor(-0.1996) 0.5917139659958676\n",
            "tensor(0.2428) 0.2439897514627859\n",
            "tensor(0.5569) -0.8081670094556448\n",
            "tensor(0.5046) 0.6659323732703092\n",
            "tensor(0.2871) 0.23643148375069115\n",
            "tensor(-0.4110) 0.3253966328136556\n",
            "tensor(0.0076) 0.9845742415499901\n",
            "tensor(0.2976) -0.1311957671472976\n",
            "tensor(0.3920) 0.9228621317488971\n",
            "tensor(0.3204) -0.019034888685389866\n",
            "tensor(0.5543) 0.3340311146163894\n",
            "tensor(-0.2645) 0.5822215757156225\n",
            "tensor(0.4449) -0.7197477247582902\n",
            "tensor(0.0174) 0.5781534857021552\n",
            "tensor(-0.4044) 0.714927187911843\n",
            "tensor(0.6177) -1.4056116324400858\n",
            "tensor(0.0058) -2.1895651598199137\n",
            "tensor(0.6949) -1.8777002248124244\n",
            "tensor(0.0724) 0.5856901640821391\n",
            "tensor(0.2086) 0.009801587218524274\n",
            "tensor(-0.5332) 0.5870957117486775\n",
            "tensor(0.8464) 0.6986824142576612\n",
            "tensor(0.1645) -1.5019500484621517\n",
            "tensor(-0.2338) 0.35122637301926574\n",
            "tensor(0.7330) -1.5538884633883299\n",
            "tensor(0.3488) 0.1359591110640855\n",
            "tensor(0.4675) -0.6061658044032837\n",
            "tensor(-0.2463) 0.41242490857897945\n",
            "tensor(0.4879) 1.3565908058542437\n",
            "tensor(-0.5752) 0.8897522007562061\n",
            "tensor(-0.0394) -1.032973476923433\n",
            "tensor(0.1090) -1.3002781081485832\n",
            "tensor(-1.9192) 0.38896059586661086\n",
            "tensor(-0.2032) 0.7719977125719609\n",
            "tensor(0.8232) 0.8868945562232322\n",
            "tensor(0.0685) -0.754407832284302\n",
            "tensor(-0.3782) 1.168881297141369\n",
            "tensor(0.3097) 0.32068195150555723\n",
            "tensor(-0.2314) 0.5379220815441296\n",
            "tensor(0.7545) -0.06293775417167209\n",
            "tensor(-0.6387) -0.46167078459588584\n",
            "tensor(0.0098) 0.8355821968531099\n",
            "tensor(1.0214) -1.7411116139588938\n",
            "tensor(0.4604) 0.1388803284853091\n",
            "tensor(0.1615) -0.8685869478293698\n",
            "tensor(0.5279) 0.8495514265998442\n",
            "tensor(0.1862) 0.14542786707816477\n",
            "tensor(-0.2184) -1.2204321878759066\n",
            "tensor(0.1755) -0.5366908041006321\n",
            "tensor(0.6017) 0.3385461839212251\n",
            "tensor(0.3852) -0.07005684901227371\n",
            "tensor(-0.2003) -1.9617215068073917\n",
            "tensor(0.2627) -1.7152550431295162\n",
            "tensor(-0.0637) 0.9499537558964858\n",
            "tensor(0.1325) -0.05392187174418733\n",
            "tensor(0.0227) 0.6559669615206596\n",
            "tensor(0.6357) 0.5257476257330692\n",
            "tensor(0.1731) -1.4260884556748668\n",
            "tensor(0.1300) 0.8393499397919436\n",
            "tensor(0.0153) 0.1997979777208462\n",
            "tensor(-0.1417) 0.7401149909489758\n",
            "tensor(0.7444) 0.25388506062666555\n",
            "tensor(0.1351) -1.1358517310391953\n",
            "tensor(-0.1723) -1.732677352293799\n",
            "tensor(0.1245) 0.8617763258436608\n",
            "tensor(-0.4697) -1.9365283034875123\n",
            "tensor(0.2360) -0.8714666771613865\n",
            "tensor(-0.2219) -0.5067822798672084\n",
            "tensor(0.2406) 0.2643827579371924\n",
            "tensor(-0.5999) 0.04525223364250621\n",
            "tensor(0.1715) 0.9047997008851238\n",
            "tensor(-0.2251) 0.25220553250914235\n",
            "tensor(1.0482) 0.06751242976533182\n",
            "tensor(-0.5151) 0.448188123222933\n",
            "tensor(-0.1620) 0.9206361486655558\n",
            "tensor(-0.3020) -0.1319562236265123\n",
            "tensor(-0.0229) -1.3372947881056974\n",
            "tensor(0.2044) 0.8539515788001458\n",
            "tensor(-0.2491) -1.0383112361850644\n",
            "tensor(-0.9741) -0.34791599161381165\n",
            "tensor(-0.0941) -0.18749586244863767\n",
            "tensor(-0.1853) -0.1190790120355012\n",
            "tensor(-0.3015) -1.0084271317161941\n",
            "tensor(-0.2194) -0.6501162716010797\n",
            "tensor(0.1030) 1.0651268182119322\n",
            "tensor(-0.1804) -2.1416459820984315\n",
            "tensor(-0.0858) -1.4386215428166362\n",
            "tensor(0.3539) 0.4942959593016127\n",
            "tensor(0.5006) 0.9189051836495848\n",
            "tensor(0.2403) 0.8284805479827604\n",
            "tensor(0.2852) 0.671356976153051\n",
            "tensor(0.9269) 0.3912576374304135\n",
            "tensor(0.0475) -1.5324632567417495\n",
            "tensor(0.1718) 1.1630129310123813\n",
            "tensor(0.7335) -2.1873028975033755\n",
            "tensor(-0.4658) 0.5991153632699756\n",
            "tensor(0.4613) -0.1657027954764719\n",
            "tensor(0.2010) -1.1754691077310324\n",
            "tensor(0.0099) 0.6848289601326879\n",
            "tensor(0.3144) -1.3572023246324363\n",
            "tensor(-0.4369) -1.499476463083165\n",
            "tensor(0.5003) -0.2532573889932565\n",
            "tensor(0.4293) -0.987868885866134\n",
            "tensor(-0.0582) -0.3740585946444736\n",
            "tensor(-0.2817) -1.527779962216055\n",
            "tensor(0.1207) 1.085670800454788\n",
            "tensor(-0.0445) 0.7176680843436571\n",
            "tensor(-0.0461) -1.9587280932188573\n",
            "tensor(-0.3849) 0.5924221182913384\n",
            "tensor(-0.0823) 1.097653391181993\n",
            "tensor(0.1073) -1.0598568182892338\n",
            "tensor(0.1586) -1.9608473708276082\n",
            "tensor(-0.1570) -0.347308198352098\n",
            "tensor(0.5316) 0.7183891179437848\n",
            "tensor(0.8820) -1.1922057629458223\n",
            "tensor(0.0321) 1.1289251212534122\n",
            "tensor(-0.2657) 0.9428436364514617\n",
            "tensor(0.7546) -2.1423572479477997\n",
            "tensor(0.9055) 0.6922199876958216\n",
            "tensor(0.4519) -1.7727188036646726\n",
            "tensor(-0.3305) -2.073164199616049\n",
            "tensor(0.4927) 0.21285456239223358\n",
            "tensor(-0.1906) 0.5444851946830412\n",
            "tensor(0.2583) -1.5243690293306063\n",
            "tensor(-0.1754) -0.9310840828989179\n",
            "tensor(1.1428) 0.6662816502668504\n",
            "tensor(0.5715) 0.22353810345333436\n",
            "tensor(-0.0813) 0.6910589376799653\n",
            "tensor(0.2055) 0.3221542405591539\n",
            "tensor(0.0812) -1.2553652439666003\n",
            "tensor(0.1295) 0.5785120080588575\n",
            "tensor(0.5815) -1.499476463083165\n",
            "tensor(-0.6228) 0.6990194522657122\n",
            "tensor(-0.1205) 0.9277887230033546\n",
            "tensor(0.4865) 0.27019602151695876\n",
            "tensor(-0.1889) 0.06184144971970018\n",
            "tensor(-0.0747) -2.4623442577689096\n",
            "tensor(0.4789) 0.23813750711468953\n",
            "tensor(-0.0420) -1.5702187437192237\n",
            "tensor(0.7324) -1.7198774550079723\n",
            "tensor(0.1987) 0.5193510763767096\n",
            "tensor(1.3177) -2.158246363946032\n",
            "tensor(0.0286) 0.722062451978121\n",
            "tensor(-0.0641) 0.8727660735669948\n",
            "tensor(-0.1715) 1.0290041508253382\n",
            "tensor(-0.0620) 0.41320438949190413\n",
            "tensor(0.8105) -1.3657889340993303\n",
            "tensor(0.3596) 0.43301662192537127\n",
            "tensor(-0.4941) 0.9154432536176431\n",
            "tensor(0.2458) 1.2044903808762932\n",
            "tensor(0.6751) 0.40194987503781143\n",
            "tensor(-0.1950) -1.4784069406785416\n",
            "tensor(0.8182) 0.4220642192059212\n",
            "tensor(0.3247) 0.6160775188397106\n",
            "tensor(-0.3037) 0.051818221093203075\n",
            "tensor(-0.6713) -1.310077862408129\n",
            "tensor(0.8583) 0.5449775236039168\n",
            "tensor(-0.2532) -1.0545259102272369\n",
            "tensor(-0.0326) 0.5224221689448595\n",
            "tensor(0.1522) -1.2616759906958335\n",
            "tensor(0.3056) 0.6282977769054039\n",
            "tensor(0.5478) 0.28471820606826787\n",
            "tensor(1.0454) -1.6337304378065596\n",
            "tensor(0.3988) 0.7202127104694953\n",
            "tensor(0.3704) 0.27465519718301346\n",
            "tensor(0.1471) 0.3211669100543484\n",
            "tensor(-0.0528) -0.00392863049677263\n",
            "tensor(-0.7286) -1.436113952615105\n",
            "tensor(0.3857) 0.6271109395110468\n",
            "tensor(0.6361) 0.27576314029198096\n",
            "tensor(0.7071) 0.5935078562913364\n",
            "tensor(0.4968) 0.5149383185454958\n",
            "tensor(0.8147) 0.615061500982908\n",
            "tensor(0.9191) 0.9361685242440577\n",
            "tensor(0.5428) 0.11123714259715393\n",
            "tensor(0.0580) 1.010942817132041\n",
            "tensor(0.0562) 0.051739904967725035\n",
            "tensor(0.0943) 0.19636594622963552\n",
            "tensor(-0.1452) -1.0180852574041799\n",
            "tensor(0.3681) 0.24537813065858896\n",
            "tensor(0.0266) 0.8865850585679725\n",
            "tensor(0.3210) -0.11666404934370944\n",
            "tensor(0.4413) 0.7891294022934138\n",
            "tensor(-1.0114) 0.5739687295606622\n",
            "tensor(-0.4426) -1.6567798609382622\n",
            "tensor(-0.1744) 0.28200974783854044\n",
            "tensor(0.4403) -1.7444233543291379\n",
            "tensor(-0.3365) 0.3128358048456119\n",
            "tensor(-0.5093) -2.385250251389303\n",
            "tensor(0.3062) 0.4469270672670305\n",
            "tensor(-0.1803) -1.6388969277691394\n",
            "tensor(-0.1641) -0.36501968560823433\n",
            "tensor(0.4716) 0.7649575644361478\n",
            "tensor(-0.3750) -0.945515275434425\n",
            "tensor(0.2064) -0.10900365263649624\n",
            "tensor(-0.9898) 0.17624693502265273\n",
            "tensor(-0.0450) 0.6528096042466537\n",
            "tensor(0.6021) 0.2183452084054216\n",
            "tensor(0.2904) -0.22681879447114203\n",
            "tensor(-0.4275) 0.25250013235527563\n",
            "tensor(0.4624) 0.6872785156582487\n",
            "tensor(0.6635) 0.7331283812937962\n",
            "tensor(0.4401) 0.821461725491471\n",
            "tensor(-1.1711) 0.7971238664388597\n",
            "tensor(0.8286) 0.3356637207280991\n",
            "tensor(-0.2410) 0.6154209545164387\n",
            "tensor(-0.2708) 0.4494325757046315\n",
            "tensor(0.4675) 0.2855792767509662\n",
            "tensor(0.0250) -1.8655366949907126\n",
            "tensor(-0.7319) 0.427102126465621\n",
            "tensor(0.1760) 0.5918920962329385\n",
            "tensor(0.2084) 0.19597763363685528\n",
            "tensor(0.2483) -1.6802251717316692\n",
            "tensor(-0.3075) 0.8538366498175264\n",
            "tensor(0.8489) 0.551365880536211\n",
            "tensor(0.6870) 0.7458884648167935\n",
            "tensor(-0.1612) 0.9240765305774913\n",
            "tensor(-0.1145) -2.60120367177142\n",
            "tensor(-0.1221) 0.1957517775666031\n",
            "tensor(-0.0196) 0.773286860964243\n",
            "tensor(-0.1231) -0.8288854315386865\n",
            "tensor(0.1596) 0.9350287861440546\n",
            "tensor(-0.0265) 0.3422734230476134\n",
            "tensor(0.4595) -1.0029111684489396\n",
            "tensor(0.2277) -0.0427595854677126\n",
            "tensor(-0.1631) 0.7405461400592274\n",
            "tensor(0.8660) 1.0039508042442622\n",
            "tensor(0.3328) -0.48885398529111784\n",
            "tensor(0.0550) -0.21456006193346297\n",
            "tensor(-1.1620) 0.20963663639093486\n",
            "tensor(0.8401) 0.9448984173094672\n",
            "tensor(0.1286) 0.4512881081706716\n",
            "tensor(0.2437) -1.526488306939357\n",
            "tensor(-0.2094) 0.41718855341843764\n",
            "tensor(0.2007) -2.1996980090384928\n",
            "tensor(-0.7024) -1.6986748180177464\n",
            "tensor(0.3539) -1.363355331925182\n",
            "tensor(-0.2990) 0.8693262367161437\n",
            "tensor(-0.2825) -0.20407923419280102\n",
            "tensor(-0.6228) 0.6101810278750307\n",
            "tensor(0.3748) -1.45610936493477\n",
            "tensor(0.3591) 0.45970167340031076\n",
            "tensor(-0.1933) 1.103732134148085\n",
            "tensor(-0.3954) 0.36764897125671475\n",
            "tensor(-0.4754) -0.11352759552189534\n",
            "tensor(-0.2019) -0.2006209299585887\n",
            "tensor(-1.0221) -1.6309223728056155\n",
            "tensor(0.1867) 0.4156945185352061\n",
            "tensor(0.6834) 1.010942817132041\n",
            "tensor(-0.3802) 0.35695864841254005\n",
            "tensor(-0.6808) 1.0428106531443866\n",
            "tensor(0.4428) -1.4259811033965255\n",
            "tensor(-0.2030) 0.2252819393376672\n",
            "tensor(-0.0613) 1.2352233630724154\n",
            "tensor(-0.1158) 1.3631834314516038\n",
            "tensor(0.4945) 0.2608515955420983\n",
            "tensor(-0.3409) 0.7201200829597557\n",
            "tensor(-0.4490) 1.0350301254529153\n",
            "tensor(-0.2935) 0.6894183098463481\n",
            "tensor(0.6215) -2.017197409844775\n",
            "tensor(-0.1304) 0.028971621320229107\n",
            "tensor(-0.1576) 0.11687193926879463\n",
            "tensor(0.0968) 1.2001530272127878\n",
            "tensor(0.0969) 0.3176023870348021\n",
            "tensor(0.6835) -0.6083732470876853\n",
            "tensor(0.6051) -0.24593456818696116\n",
            "tensor(-0.5870) 0.09632910283299345\n",
            "tensor(-0.0444) -0.8688119486673186\n",
            "tensor(0.2698) 0.14061129350128002\n",
            "tensor(0.1653) 0.5473692382231042\n",
            "tensor(0.3117) 0.8540313933173568\n",
            "tensor(-0.0469) 1.1522660357276104\n",
            "tensor(-0.2399) 1.0423780857787577\n",
            "tensor(-0.6012) 0.40574223804168735\n",
            "tensor(0.4914) -2.3676685887864184\n",
            "tensor(0.1562) 0.8094771245097719\n",
            "tensor(0.0364) 0.12439849398566594\n",
            "tensor(0.5927) 0.6694919728481237\n",
            "tensor(0.1725) -0.9842424664083788\n",
            "tensor(-0.1074) 0.32112322372901536\n",
            "tensor(-0.3158) 0.9004170444288637\n",
            "tensor(0.1469) -0.30003179995288026\n",
            "tensor(0.3478) 0.2520520917990054\n",
            "tensor(-0.1356) 0.49798441740942195\n",
            "tensor(-0.0208) 0.6672197290109506\n",
            "tensor(0.1226) -1.017604977034025\n",
            "tensor(-0.5987) -0.030040105390631514\n",
            "tensor(-0.3161) 0.838942531548286\n",
            "tensor(-0.3385) 0.3049115964567413\n",
            "tensor(0.8126) -1.103548815557345\n",
            "tensor(-0.1955) 0.3636854520575343\n",
            "tensor(0.8326) 0.8364707727433309\n",
            "tensor(0.4915) 0.5572306209064025\n",
            "tensor(-0.0125) 0.7869445234597577\n",
            "tensor(-0.1807) -0.05692378227509789\n",
            "tensor(-0.9860) -1.5686481804171912\n",
            "tensor(1.3316) 0.28488787089692996\n",
            "tensor(-0.1960) 0.3177210088820149\n",
            "tensor(0.7326) 0.0954871837256052\n",
            "tensor(0.1526) 0.6792790583073942\n",
            "tensor(0.0133) -0.20288792034680733\n",
            "tensor(-0.5755) 0.8035372478738857\n",
            "tensor(-0.1129) -1.2244787254637295\n",
            "tensor(-1.2106) -0.9154735189634012\n",
            "tensor(0.2180) -1.755452876200555\n",
            "tensor(0.5282) -0.5290206868869771\n",
            "tensor(-0.6325) 0.2626768743660017\n",
            "tensor(0.1101) 0.04040380024897183\n",
            "tensor(0.3669) 0.014294986011645324\n",
            "tensor(-0.5809) 0.43914915265144433\n",
            "tensor(0.1666) 0.3624687835765941\n",
            "tensor(0.5402) -1.434981982431081\n",
            "tensor(-0.1489) -0.38041201885764964\n",
            "tensor(0.0667) -1.6732460451460718\n",
            "tensor(0.7654) 0.4081035667168453\n",
            "tensor(-0.0512) 0.47791316774047266\n",
            "tensor(-0.4950) 0.9810616913718678\n",
            "tensor(-0.0755) 0.14764883426857986\n",
            "tensor(-0.0346) 0.8186806339678191\n",
            "tensor(0.3892) -1.7825984941740523\n",
            "tensor(-0.2850) 0.07894258605031777\n",
            "tensor(-0.1987) 0.5367059690712181\n",
            "tensor(-0.6431) 1.2202492185337097\n",
            "tensor(0.2804) -1.4032065963849967\n",
            "tensor(0.6113) 0.863019658124753\n",
            "tensor(0.6638) 0.10591138870678692\n",
            "tensor(0.1987) -1.3911125389040002\n",
            "tensor(0.2199) 0.3078202235547349\n",
            "tensor(-0.0420) 0.8315102171812369\n",
            "tensor(-0.1215) 0.3226164184042931\n",
            "tensor(-0.0791) 0.609996259433441\n",
            "tensor(-1.5494) 0.36428870468775615\n",
            "tensor(0.1626) -1.1828426875115918\n",
            "tensor(-0.3588) -1.3465515542666064\n",
            "tensor(-0.0694) -1.7840120896677558\n",
            "tensor(0.4202) -0.40810425565693603\n",
            "tensor(0.6785) 0.3722998767339933\n",
            "tensor(0.4973) 0.5831394755025531\n",
            "tensor(-0.0759) -1.5723957101447061\n",
            "tensor(1.2057) -2.0128877967487546\n",
            "tensor(0.1494) 0.5316891210517698\n",
            "tensor(0.0995) -2.502305316410094\n",
            "tensor(-0.0987) 0.8332035693295495\n",
            "tensor(-0.2884) -1.1016959067888068\n",
            "tensor(0.0746) -0.023535498504166735\n",
            "tensor(0.1243) 0.7409767675773001\n",
            "tensor(0.3212) 0.8781905298145182\n",
            "tensor(-0.5523) 0.37768096029284143\n",
            "tensor(0.2078) 0.9051023047801383\n",
            "tensor(-0.1660) 0.8184094390500339\n",
            "tensor(-0.4320) -1.1338665412833497\n",
            "tensor(0.2300) 0.6048256563550045\n",
            "tensor(0.3078) -1.4714460012906934\n",
            "tensor(-0.2735) 0.40694248622783685\n",
            "tensor(0.8115) -2.025232368385548\n",
            "tensor(-0.3219) 0.45519242074080224\n",
            "tensor(-0.0311) -0.4261091321626282\n",
            "tensor(0.0071) 0.1487081163485016\n",
            "tensor(0.8453) 0.39616420504693556\n",
            "tensor(-0.1452) 0.2269542592897631\n",
            "tensor(0.9482) 0.076305308670908\n",
            "tensor(0.1819) 0.7806685476323838\n",
            "tensor(-0.1933) 0.5441591892469182\n",
            "tensor(0.6026) -1.8406763176695389\n",
            "tensor(0.5692) 0.40467868113637034\n",
            "tensor(-0.2004) 0.4382246292364993\n",
            "tensor(-0.5713) -1.918077246364179\n",
            "tensor(-0.8683) 0.44731632155629913\n",
            "tensor(0.3654) 0.31410702000871166\n",
            "tensor(0.0110) -1.679948727947659\n",
            "tensor(0.3978) 0.9629697548722064\n",
            "tensor(0.4874) -2.1050820547622346\n",
            "tensor(0.8784) 0.22265366640164985\n",
            "tensor(0.1527) -1.373586549436438\n",
            "tensor(0.5728) -1.5470591278340642\n",
            "tensor(-0.6068) 0.8886212789344254\n",
            "tensor(-0.3039) 0.018638288577201362\n",
            "tensor(-0.7283) 0.5160141150412513\n",
            "tensor(0.7338) 0.4659418421998886\n",
            "tensor(-0.3251) 0.9485729327263009\n",
            "tensor(0.4057) 0.9528759855001542\n",
            "tensor(0.1707) 0.4119630738018744\n",
            "tensor(-0.3603) 0.5375682785702562\n",
            "tensor(-0.8947) 0.7613295515241966\n",
            "tensor(0.3662) 0.6370615291744227\n",
            "tensor(0.0663) -0.5979506840273991\n",
            "tensor(0.1432) 0.07506870821010961\n",
            "tensor(0.4166) 0.9255699306392282\n",
            "tensor(0.0920) -1.678668050798508\n",
            "tensor(-0.8972) 0.6685835225076477\n",
            "tensor(0.3354) -1.3435710982255549\n",
            "tensor(-0.1345) -0.16160719107751498\n",
            "tensor(0.5330) 0.43287927401148146\n",
            "tensor(-0.7485) 0.5670758590846774\n",
            "tensor(-0.6630) -0.5562331038818431\n",
            "tensor(-0.0520) 0.35545469577205946\n",
            "tensor(0.3094) -0.08277980866950545\n",
            "tensor(-0.1724) -1.085980944935748\n",
            "tensor(0.4951) -0.3428063791495477\n",
            "tensor(-1.5089) -1.6046240651515815\n",
            "tensor(0.6545) -0.23019748180984526\n",
            "tensor(0.3564) -0.257751295963267\n",
            "tensor(0.3993) 0.48916852742052436\n",
            "tensor(-0.0367) -0.9426289950066069\n",
            "tensor(0.3161) -1.133492659229571\n",
            "tensor(-0.3308) 1.101396474024382\n",
            "tensor(0.0241) 0.47282952926644733\n",
            "tensor(-0.3918) 1.1923498815605351\n",
            "tensor(0.2371) 0.3028233708048973\n",
            "tensor(0.4061) -1.1751613071395488\n",
            "tensor(-0.0615) -0.4321771007128045\n",
            "tensor(0.6808) 0.50786859222534\n",
            "tensor(0.3520) 0.6558750943539421\n",
            "tensor(0.5081) 0.653737262347976\n",
            "tensor(-0.2953) -0.007070206248146932\n",
            "tensor(-0.2320) -0.09472641391217725\n",
            "tensor(0.5409) -1.5129112360295256\n",
            "tensor(0.1687) 0.14105722939880713\n",
            "tensor(0.1135) 0.7654587773065523\n",
            "tensor(0.5168) -0.48308398464577457\n",
            "tensor(0.4708) 0.8896373450094098\n",
            "tensor(0.5180) 0.021881734073121318\n",
            "tensor(0.6057) 1.0501230397257808\n",
            "tensor(0.6080) 0.421057216151783\n",
            "tensor(0.2132) -0.29640446890883954\n",
            "tensor(0.1425) -1.034744515282285\n",
            "tensor(0.1249) 0.7833042004772522\n",
            "tensor(0.2466) 0.24179639378240422\n",
            "tensor(0.1066) 0.5060369916047568\n",
            "tensor(-0.6961) -0.45472332210255867\n",
            "tensor(-0.1560) -0.050000411667751044\n",
            "tensor(-0.9128) 1.2169607253383858\n",
            "tensor(0.2111) 0.42476964717692683\n",
            "tensor(1.0458) -1.9147880956173111\n",
            "tensor(0.3487) 0.7982300752523761\n",
            "tensor(0.4934) -0.06142170457139329\n",
            "tensor(0.5858) -1.03525705278291\n",
            "tensor(0.9428) 0.5399093876162554\n",
            "tensor(0.6132) 0.01764198290570773\n",
            "tensor(0.0856) -0.01788215165674334\n",
            "tensor(0.7581) 0.38759773582345386\n",
            "tensor(0.2251) -0.06336282610035614\n",
            "tensor(-0.1381) 0.24538403629038555\n",
            "tensor(0.3930) 0.4691594690135487\n",
            "tensor(0.6748) -1.0915982417262216\n",
            "tensor(0.9677) -1.249794032158419\n",
            "tensor(-0.3872) 0.20556035431854003\n",
            "tensor(0.1942) 0.8715760113161126\n",
            "tensor(-0.5936) 0.8113762902499712\n",
            "tensor(-0.9370) -1.119006610670709\n",
            "tensor(0.4179) 1.0912589472117524\n",
            "tensor(-0.1373) -1.5428680207649041\n",
            "tensor(0.0719) 0.26126039780887766\n",
            "tensor(-0.4326) 0.40220217792172513\n",
            "tensor(-0.7181) 0.7082410257441266\n",
            "tensor(-0.5613) 0.36706968584296756\n",
            "tensor(0.0887) -1.1071008545375598\n",
            "tensor(-0.0923) 0.23702566873991912\n",
            "tensor(0.2977) 0.3600378172258254\n",
            "tensor(-0.4688) 0.44463435397794754\n",
            "tensor(-0.6826) 0.5354495003634193\n",
            "tensor(-0.2969) 0.8690694962324645\n",
            "tensor(0.9961) 1.076350351403392\n",
            "tensor(1.1207) -1.5513047686143038\n",
            "tensor(-0.2314) 0.056455082865462025\n",
            "tensor(-0.1608) 0.9366993443926143\n",
            "tensor(0.0711) -1.7220102633320262\n",
            "tensor(0.4307) -0.18041101447620744\n",
            "tensor(0.6188) -0.9426289950066069\n",
            "tensor(0.2642) 0.171492790082713\n",
            "tensor(-0.0196) -1.4349662384166606\n",
            "tensor(0.1530) -0.16341168023329744\n",
            "tensor(-0.2747) -1.2036286111860794\n",
            "tensor(-0.4421) 1.1581343923890282\n",
            "tensor(0.2721) -0.15184226815842727\n",
            "tensor(-0.8556) 0.8112042648766112\n",
            "tensor(0.1654) 0.23226646699168918\n",
            "tensor(-0.3542) -1.1208542791561997\n",
            "tensor(-0.3642) 0.7626233843787212\n",
            "tensor(0.2048) -0.88657835660682\n",
            "tensor(0.7193) 0.7986987417921124\n",
            "tensor(-1.0754) 0.7375785685070202\n",
            "tensor(0.9114) 0.8715760113161126\n",
            "tensor(0.2631) 0.07401505062640534\n",
            "tensor(-0.3196) -2.369801908084955\n",
            "tensor(0.0934) -1.3971895850202796\n",
            "tensor(-0.2440) 0.8724008173007284\n",
            "tensor(-0.1193) 0.7444712727124565\n",
            "tensor(0.0079) 0.7192158206243519\n",
            "tensor(-0.2627) 1.114785244989851\n",
            "tensor(0.0857) 0.030901357020870972\n",
            "tensor(-0.3620) -0.00871731983192444\n",
            "tensor(-0.0487) 0.182476766734292\n",
            "tensor(0.9914) 0.7189425297370184\n",
            "tensor(-0.9460) 0.6560911882168212\n",
            "tensor(-0.3493) -1.4238127314970856\n",
            "tensor(1.1059) 0.5939642373709632\n",
            "tensor(-0.2304) -1.674757243420986\n",
            "tensor(0.2313) 1.09492199435302\n",
            "tensor(-0.1305) 0.23670702786240758\n",
            "tensor(-0.9308) 0.05160925772162853\n",
            "tensor(0.2490) 0.8104915806858766\n",
            "tensor(-0.3476) 0.24867942098420479\n",
            "tensor(0.2609) 0.874078884529201\n",
            "tensor(-0.8169) -1.420325736301776\n",
            "tensor(-0.4471) 0.6855473876329238\n",
            "tensor(-0.2435) 0.27779713602021455\n",
            "tensor(0.5218) -1.2938331519157729\n",
            "tensor(0.7106) 0.10579325880151969\n",
            "tensor(0.2958) 0.2259435833122353\n",
            "tensor(-0.6955) 0.7131827057110892\n",
            "tensor(-0.0709) 0.6990699048864387\n",
            "tensor(0.2030) 0.35070004600927707\n",
            "tensor(0.5647) 0.03531706075325978\n",
            "tensor(0.4260) -0.23710244014113713\n",
            "tensor(-0.8509) 0.14338560337178452\n",
            "tensor(0.5047) 0.18331584747405816\n",
            "tensor(0.4071) -0.21031413758218562\n",
            "tensor(-0.0555) 0.8806367562841544\n",
            "tensor(-0.8245) 0.09736561568495938\n",
            "tensor(1.2734) -2.6895459054718684\n",
            "tensor(0.2757) -1.3880696694056922\n",
            "tensor(1.0746) 1.1762569316950537\n",
            "tensor(-0.0430) 0.5126365800331222\n",
            "tensor(0.1445) -1.9015221734846497\n",
            "tensor(0.0813) -0.10808844871856198\n",
            "tensor(0.8768) 0.45966935035177786\n",
            "tensor(0.1943) 0.7453972531904217\n",
            "tensor(-0.9116) -2.010418989805672\n",
            "tensor(0.2111) 0.47587959988049516\n",
            "tensor(0.3632) -0.2652275913108965\n",
            "tensor(0.4711) 0.10609437757746377\n",
            "tensor(-1.2080) -1.5830154058373775\n",
            "tensor(0.0604) -0.7452018612795185\n",
            "tensor(0.0111) 0.48614137825067577\n",
            "tensor(-1.2139) -0.006996399912695554\n",
            "tensor(-1.2472) 0.5797947707372485\n",
            "tensor(0.3712) 0.7542035421337295\n",
            "tensor(-0.0766) 1.046158887546315\n",
            "tensor(0.7809) 0.364528786964229\n",
            "tensor(-0.3870) -2.115073178956875\n",
            "tensor(-0.2047) -1.712836514797856\n",
            "tensor(-0.9191) -0.1744724515760423\n",
            "tensor(-0.3813) -0.8853378347815933\n",
            "tensor(0.5637) 0.4879224603936039\n",
            "tensor(0.7524) -1.826207018588874\n",
            "tensor(0.7430) -1.5274390130635143\n",
            "tensor(0.2506) 0.9750041373809301\n",
            "tensor(0.3409) -0.6503817325800079\n",
            "tensor(0.0349) -1.656773208603848\n",
            "tensor(-0.3825) -0.8734940691946792\n",
            "tensor(0.7079) -1.327429803478226\n",
            "tensor(0.0300) 0.799133751142603\n",
            "tensor(0.9261) 0.685540462985731\n",
            "tensor(-0.4103) 0.6919148586636664\n",
            "tensor(-0.2508) -1.7704854221856543\n",
            "tensor(-0.1588) 0.42415169686441007\n",
            "tensor(-0.6455) -1.2461108896025452\n",
            "tensor(0.2776) 0.6603654225680803\n",
            "tensor(-0.1913) 0.8472807504289749\n",
            "tensor(0.6664) -1.1197452384243525\n",
            "tensor(0.5764) 0.8101868360627988\n",
            "tensor(0.3369) 0.43950058296925887\n",
            "tensor(1.0148) 0.4332496046371935\n",
            "tensor(0.2292) 0.6234303468302776\n",
            "tensor(0.0110) 0.3336561411297924\n",
            "tensor(0.3683) -1.3326658630140875\n",
            "tensor(-1.0736) 0.6990699048864387\n",
            "tensor(2.0775) 0.21037659022744712\n",
            "tensor(0.5043) 0.5656062581707882\n",
            "tensor(0.2873) -0.14949685648641575\n",
            "tensor(0.0498) 1.033678864977132\n",
            "tensor(0.3177) -1.8963707690152332\n",
            "tensor(0.2422) -0.2501633647529358\n",
            "tensor(0.5487) -0.08136108306834629\n",
            "tensor(0.2700) 0.6807622383163846\n",
            "tensor(0.3146) 0.5512770406221259\n",
            "tensor(0.3681) 0.14468006338641728\n",
            "tensor(-0.3453) -1.1277372421806688\n",
            "tensor(0.7749) 0.8155367318167449\n",
            "tensor(0.1362) 1.020822744490821\n",
            "tensor(-0.0089) -1.5637851880188454\n",
            "tensor(0.1578) 0.8635881482614548\n",
            "tensor(-0.5479) 0.2811670478410586\n",
            "tensor(-0.1594) 1.0778049176379194\n",
            "tensor(-1.4559) 0.10530738353743052\n",
            "tensor(-0.4430) 0.656075513695622\n",
            "tensor(0.3468) 0.6465466522492007\n",
            "tensor(0.3085) 0.44666637932102615\n",
            "tensor(0.1973) -1.3960854845491077\n",
            "tensor(1.0157) 0.3356185880564002\n",
            "tensor(-0.4641) 0.6533398828030454\n",
            "tensor(0.3054) 0.9962084970206965\n",
            "tensor(-0.0016) 0.6668142141554115\n",
            "tensor(0.3622) 1.1163492331209528\n",
            "tensor(0.4819) -0.7710383938753619\n",
            "tensor(-0.2029) -0.1712728776322523\n",
            "tensor(-0.3907) -0.033138994767361374\n",
            "tensor(0.2483) -1.8801840325249206\n",
            "tensor(0.4264) -1.3745801210036575\n",
            "tensor(0.1045) 0.8141065018510093\n",
            "tensor(0.0633) 0.2770973325858091\n",
            "tensor(-0.5632) 0.9079356060195005\n",
            "tensor(0.4955) 0.09768499377905375\n",
            "tensor(-0.1371) 0.3041598744351313\n",
            "tensor(0.4813) -1.290868487266515\n",
            "tensor(-1.0395) 0.577563018981211\n",
            "tensor(-1.0292) 0.4288933891174002\n",
            "tensor(-0.3374) 0.15371552264523616\n",
            "tensor(0.4561) -0.889008852216207\n",
            "tensor(0.7512) 0.28261725039899854\n",
            "tensor(1.2359) -0.06590259168670876\n",
            "tensor(-0.0632) 0.7661571961111987\n",
            "tensor(0.4769) 0.04535796449197616\n",
            "tensor(-0.5325) -1.5650276054379626\n",
            "tensor(-0.6471) 0.11323658056289217\n",
            "tensor(-0.4128) 0.9036127963574921\n",
            "tensor(-0.7075) 0.6881276660337433\n",
            "tensor(0.2344) 0.04742417797948565\n",
            "tensor(0.3516) 0.6261874624842668\n",
            "tensor(-0.5455) -1.5837958616456256\n",
            "tensor(-0.7631) 0.05291451250634277\n",
            "tensor(-0.2494) 0.2440661226182609\n",
            "tensor(-0.2436) 0.03473052490786362\n",
            "tensor(0.4376) 0.8663870585280984\n",
            "tensor(0.3234) 0.7149309416648416\n",
            "tensor(0.2678) 0.8378363227347695\n",
            "tensor(-1.7366) 0.6920905532145699\n",
            "tensor(-0.0645) 0.2629647453429495\n",
            "tensor(-0.5228) -1.8096863439481965\n",
            "tensor(0.4289) -0.3248307105248236\n",
            "tensor(-0.4313) -0.7873316795012851\n",
            "tensor(-0.4883) 0.8630182252001518\n",
            "tensor(-0.3530) 0.09089934686074057\n",
            "tensor(0.4277) -0.9256214207994855\n",
            "tensor(-0.1470) 0.8287030265793587\n",
            "tensor(0.1654) 0.7934972985778354\n",
            "tensor(0.4297) 0.8614405848379844\n",
            "tensor(1.1250) -0.7627276492013132\n",
            "tensor(-0.0586) -0.45724172630933735\n",
            "tensor(-0.2920) 1.0785746286748836\n",
            "tensor(0.6394) -0.02870919948659832\n",
            "tensor(-0.0883) 0.7429526624892603\n",
            "tensor(-0.0098) 0.2046271745712891\n",
            "tensor(-2.4683) 0.36532867832169363\n",
            "tensor(-0.0949) -1.3446938091650082\n",
            "tensor(0.2875) -1.757260190000224\n",
            "tensor(0.8993) 0.630982266546017\n",
            "tensor(0.3293) -1.865145346478658\n",
            "tensor(-0.1643) 0.13185350452312458\n",
            "tensor(-0.0637) 1.0592164338999959\n",
            "tensor(-0.8824) -0.9656396913606837\n",
            "tensor(0.2866) 1.043302168124326\n",
            "tensor(0.8713) -1.3379864229764573\n",
            "tensor(-0.4944) -0.46193733319015795\n",
            "tensor(0.1683) 0.3974787346343078\n",
            "tensor(-0.8416) -1.3708331234043711\n",
            "tensor(-0.5763) 0.5829162463794436\n",
            "tensor(0.4576) -4.494291459817248\n",
            "tensor(-0.1602) 0.021521891815105387\n",
            "tensor(0.6932) -1.0861848264147003\n",
            "tensor(-0.1606) -1.0040427421688773\n",
            "tensor(-0.1991) -1.8296934755028713\n",
            "tensor(-0.0923) -1.6364240429497334\n",
            "tensor(-0.0919) 0.7418887314426499\n",
            "tensor(0.3172) -0.1937501523156243\n",
            "tensor(-0.3637) 0.8012063788244008\n",
            "tensor(0.4073) -0.7260244814746256\n",
            "tensor(0.2391) 0.15966442550758847\n",
            "tensor(0.1543) 0.4818794216532984\n",
            "tensor(-0.3122) 0.7764359464747194\n",
            "tensor(0.7793) -1.3306860737275759\n",
            "tensor(0.7242) 0.390588943180506\n",
            "tensor(0.3170) 0.5528307471102338\n",
            "tensor(-0.7463) 0.6328094588325713\n",
            "tensor(-0.0404) 0.27775212827941914\n",
            "tensor(0.1397) 0.8825711244830375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2df4wc53nfv8/uDck9ytVR0LmWVqLI\nOAZZ07LIkrXVEmhLxRWV0JIvZlTGtVukKSAEaIBKEGiQlWCSrgoTONRS0QQohMYoCqsKZUs5S6IL\nyoYIGFBLQaSPFM2YTOzIpLRSYbrU0RFvqdvbe/vH3bs3O/u+M+/82tm5+34AQbz9MfPOzM73feZ5\nnx+ilAIhhJDyUil6AIQQQtJBISeEkJJDISeEkJJDISeEkJJDISeEkJIzVMROb775ZrVu3boidk0I\nIaXl1KlTv1JKjQZfL0TI161bh5MnTxaxa0IIKS0ictH0Ol0rhBBScijkhBBScijkhBBScijkhBBS\ncijkhBBScijkhBBScgoJPySkLExMNjB+7ALenWri1pEa9u7cgLEt9aKH1WHQx0f6A4WcEAsTkw3s\nf+Esmq02AKAx1cT+F84CwECI5eMTZ/HMiUvQhahN46PQLw/oWiHEwvixCx0R1zRbbYwfu1DQiBaZ\nmGx0ibjGPz49ETWmmlBYFPqJyUbfx0vyhUJOiIV3p5qxXu8n48cu9Ii4Ro9vkCciki10rZBMWUqP\n8reO1NAwiLYCsOXrr+DA/ZsKO7awyeTWkVroZ2yvL6Vrt9ygkJPMcPHZRpGFmCTdRvB7OzaO4vlT\njR6rFgDen25h73fPWI/t8YmzePb1t9FWClURfOmzt2PbHTf1jAtA6Fhtx2KbZATobNf2GS30wWMf\n5PUAEo4U0bNz27ZtikWzlgZaaEyCoamP1PDavnu6Pm8SrqCYAEDNq+IbX7wz1kQQ3IYA+PLda/HE\n2J2xvlfzqti9td4R5Khj0zw+cRbfPnEpcqwVANWqoNVe3Lb/eMPOB4DI44xzPrcfftV4DU3H5wot\n/OwRkVNKqW3B12mRk8SYhMKEfpSPsvrCfLpRAhA2oSgAz5y4hG133GTdjm3fR998zyri+hhcxmBi\nDsBcu3vb/uMNOx9aXMOEUv/bRUyzXg+ghd9fKOQkMSahMaEf5aOEOqmYuEwoamH/trA8m/i+P90K\n3bfe1smLV4xRJEnQxxt1Psa21CNF0eUzQDw3jAtpJmUSHwo5SYyL5en32UYJU1IxcZ1QGlNNbD70\nClrtOVybaXe9noZDL53D1HQrExEHFo/3xpqHqWbvRKIAfHz/99FWCvWMXBZ7d24wumH0tYvCdWIc\nhIifpQiFnCSmKhLqdtA+Wy0yYUI9MdnAtQ9ne95zEZM44mASxrS4WO1xmJ6ZxcRkAyL2z+jzHuWy\ncPVTx3HDmPYRdKMIYJzYklr4JBwudpLErNt31PqeyVIMW1A0RYesGfacQvy2fP2VzMW0aLzAIqgL\nwXOexeKxC7aF0qCY57Hv5Uaui50i8i0AnwfwS6XUp7LYJhl86hYL2xbpYLP6bK6R4RVDToucH1zv\nteSrFcFHVg7lYoEHsVmfaYgr4oD74vGhl87FsryjrHrbE5HC/G+BUSv5k4lFLiL/GMAHAP6Hi5DT\nIl8aZGXxrd931CiEAuCtw7us+w6LEBmpeTh94F6rtVgk9ZEapmdmc3uKqIpgTinnySXsmrlc4zxC\nF4kZm0WeSYq+UupHAK5ksS1SHsa21PGNL96J+kgNgvkbN8mjs81vanvdX0PExtUFS3zvzg2oedVY\n48mbxlQTuz59C7xKiBPch9unFmnHEHFg3kp/+MhpbD/8alcdlonJBh597ozRqn/0uTNYv+8oth9+\nFTs2jvac4zgLpSQ9mfnIRWQdgJdtFrmIPATgIQBYu3bt1osXjc2gyTLExerzP95XIhZZNWuGPSiV\nzwLnUiUs4SjsO7u31nH8/OUeNwqTgrLFZpH3Tcj90LVCgphueACxEmxINiRx/ZjcKP1abF1OMLOT\nxCarmiUu3wsmrrhmjZLsSTJxmhY8mRTUP1jGlhhJWsva9L1HjpzG4xNnY+3fNcmHDAam9YxBLgO8\n1Mgq/PBZAP8UwM0i8g6AA0qpP8ti28RMlr5H07aSWlOm77nUOgnCm71cmBY2s077J3ayilr5klLq\nFqWUp5S6jSKeL1l2frFtK2mKte17utaJKyPDnvNnSbGsGfaME7QpYojRLPlAH3kJydL3eOilc8Zt\n2dLvb6zZBXZishGaHONiZU9MNnDwxXOMNBlQgtfXqwqut9qdLF9/Nm6atH8SDwp5n8nCJZKV73Fi\nsmGNTGgrBa8iaM11y/Kvr7cwMdkwjjms/RjQ+0idVrRrXpV+9D4iAH7zo6vxs19e61znVlt1ZaEG\nG264Vl8k6aCQ95GsajRn5XsMc3XUR2p4/9qHPUI+p4CDL54zxneHibhg/nh1AsnLZ95LZXVryy9t\neGIe6fVLhWpF0PZdfwXgr395LfJ7rbayPh0yrjwfGLWSkInJBrYffrWT3ebin86qGW5WvscwC37v\nzg2Ybs0Z39MCHPSvh+Fv//btE5dSu04+uD6LkxevGCsmxkEBcEywXPJ4FcGaYa+TpfuRlcntPNNv\nK8u1HdINhTwBSX+QWblE8k6NH6mZF6+CFBki2JpTmUwIwPxTxnJnzbCH8QfvwuTX7sVbh3dh784N\nqc6t6beVlSFDeqFrJQFJFxuzDMdK6nv0P9qODHs9fvCaV8XBBzYBmL+5bT70sBK2pHwMr1iUgonJ\nRsfPnQSvKl1Ph1EFzhhqmh5a5AlIalkXHY4VfJJ4f7oFyLwFbrLsD9y/CV6VfoflgP+p8tBL5xKV\n0QUWLPvfu6unJnrYOgbjytNDizwBSS3rosOxTE8SrbbC6pVDOH3gXut4H33ujFORKhNeVQCFnkXT\nsjBiabe2FNFPlUnK63pVweoVQ5iabnVcJbaa6H4YV54NFPIEpOlvWGQ4VpInibEtdTxy5HTktgXz\nE9mOjaM9VfCA8ha/mmq2IlvaLSWSXqNWW3UmPH80VthvK6t+o4SulURktdjYb8LqfodF4UQ9aawZ\n9nDrQieY4+cvY8fG0c7f2jobxLrgriwXEQfi1z63oa17229HV0sc9HumLLBnZ4lIG4Mbp2emv9xo\nWCVCF9dJzati5VBl2bgoyCJP7dnMUrYZkmuHoOVKkljyNPtKW1XQ9iRx/Pzl0LCwsS117N5a77HW\nBMCQIfszSLPVTiTiK4f484xikJei9djK+PRaNkrlIx+krLCssjRd9mPzLyepKmjy0dt84H7/5vHz\nl3uSfhSApiVpKAs+nM1v20uFL9+9Fs+cuDSQ2am6UBpdKPlTGpNn0LLC+pHc4BK6FbeqoAmbH1MB\nnSeNtLG+a4a90vrIB5U1wx6eGLsTT+7ZXPRQrOQdI97Pp+JBpjRCPmhZYf0omu+aOZl2n2ELkXrC\ntJWVdRHomlfFgfs3dT1iV2WQnQKDT7UiOHD/fOLWIFu7YdUy0zJoxl2RlMa1MmjdRvpRNN/12NLu\n0x/fbjqmZquNlUOVnmqDWqD1d7XLKxiCuGPjaNf72oJkK7fkVACcvHgFh146lyjuO2tEAFPcRNr5\nOsydylZyi5RGyAet20iaWHJXbMfsJ80+TTfJI0dOG/2tV5stPLlns/Wmst04trWE+cXTQfTslgNd\na6YITGUdbBPy+9P2ssdRRK1DxTHuBml9LQ9KI+T9EM445JGlGfyx7dg42hMWqDPorjZbqfZpukls\nIg7MTypxkpnCFmmbrXamIqQTSx52SFwi6blh1RCUQtdvMCzha/8LZ3Hy4pWeRLGo31KUxe1q3PUr\nMCGMvCeS0gh50enttjFlJdw31jxcm5nt1LhoTDXx/KkGdm+tx74BXLD11jQRd8IMizvPAy0kJFtM\njUWAeSu75lXx5J7NXb9F2zVvttpdkTWuQhplcbsad0W7YPoxkZRGyIFi09uzJnhxTXHWzVYbx89f\nxmv77sl8/67+d38atd/K1mnrpjTrfpe3LWv6/6Az/uBdoU9VfiHU/7c9FQWngyyqhboad0Wvr/Vj\nIimVkC8l+hWRYsPF/y5AZxIJTjw6bd1kXUSNOeuuPI2pJjv9ZIyuST+2pW4tWRy8zrpIluuk6lIt\nNMridjHuil5f68dEUprww6VG2oiUtPGzLrVP/PsOm3iCYaBhN4hXAYZXmPfrVQRewl+kwmBnOZYJ\nryKdmvS6obYJ03U2/a7ifN9PVjWNii4fHVbjKCtokRdEmoiULHxuwZDDoEUb3HfUxON/Pyz6pTUH\ntGZ6J4SRmofP33ULjrzxNpLa1gpYVpUK82LPZ27v+n2YzqYA2LFxFNsPv9rj1jh58Qqeff1ttJVC\nVQR3/8Ya/PjS1cKqhRa9vtaPQA0WzSoI04KgVxHcsGq+pnPYj2374VeNk4CuKJd0PGE/dNs+NVUR\nzCnV+W6cCJKRmofTB+6N3EcUdK+E4zrJ+X9H6/cdDV0Edy3AlteifVnIKmrFVjQrE4tcRO4D8J8B\nVAH8N6XU4Sy2u5RJYyXk4XOLsnxMVoWfoM88rE1cEL3Qm9ZnSBEPx/VJxX8dRizXsSIwLuBpSzz4\nel6L9mUh70CN1EIuIlUAfwrgnwF4B8AbIvKiUuov0257qZP04maxeBPXQgi6YrR1Z7LydCZoXAvZ\nxd1EgE98dDX++pfXctu+/3dk035bwUvbZMG+nPmSxWLnZwD8TCn1N0qpGQB/DuALGWyXWEi7eJO0\nRsXYljpe23cPfnF4F37+jd/BLw7vwpzlxr3abOHLd691Go+mzM0n+sn0TL5VIf2/o6sZ1ZBnX858\nyULI6wDe9v39zsJrXYjIQyJyUkROXr58OYPdLl/SruZnWYAsbEX+ibE7MeJQNGnNQkEu/3ERO42p\nZq7naPzYhc6knkSAkxgZrGKYjr6FHyqlnlZKbVNKbRsdHe3Xbpcs2jp+6/Cu0HrPphskSx971NPB\nwQc2hVrZXnWxih+weFxrLNUWGWI4z46No/PdmXLA/4Rmu76266ONijhGBqsYpieLxc4GgNt9f9+2\n8BopGFuYom0BK4n1FbVoG3z/xpoHEURG5kxZFkoHdUGz3xEzR998b75rfU7t8/QTml6gDF5foDcl\nX0/gcdd+ik6hXwpkIeRvAPiEiKzHvID/PoB/kcF2iSO2hUvbDWIrSZs0rjUo1v4Wcfr/cW9I28Jn\nUXHiYUJdRNhj2tK1FbEvWGr0E1rY9csipK7oFPqlQGohV0rNisgfAziG+fDDbymlzqUeGXEiLDnI\ndiNElaTNcgxJt2lLojDFKfeDMM0b1KcEjQhQG6pgeqEt3+oVVczMzlkXqjUumZdZWMxFp9AvBTKJ\nI1dKfR/A97PYFolH2GNp2A2SZVxrHo/GYS6bbXfcFKumh62K33LA37E+rLSw6Xv9SmEftBLVZYQp\n+iUn7LH0yT2b+3KD5PVobJts9OubD71i9BHXvApuWr2yawJYjhUSBcDfX3sjxo9dwMNHTju5gATo\ne+Zl0Sn0SwEKecmJsrqB/G+Qoh6NbTHO11tzxizC5dZaTgH43z+/0hHvKBGPU+Ih60YJS6lEdRFQ\nyEtO1GNpP26Qoh6N40wg+hw8+tyZTBZLdRy3m5uigmYr3yQeG65HmiShrMiOO6QbCnnJKeqxNGiR\nFVEUKe4EosdjKlbWVioyisO0jygrf6Tm4cPZYkTcFVNzED/Ba33tw1nnNZGl3itzUGD1QxIbU+VG\n/6Jav8cSVyhM3wHsnYYqAvydVR6uNls9cfA7No7i+PnL1lLAq7xKYV3uXXziuvKkjTht+wTAW4d3\nhX63qN/JUiHX6odkeTFICRxJXEdh3zGV39WWenDxWPdVDUaF+CeIRwpqCK3ru0eFakbVUonTti/o\n0hqk38lSh0JOYrNUEzjCas1MNVs4+OI5ozAdfPFcZ3IICpTNyrdZyxUA1ap0mnAnRVvZUaGaUQvS\nrtfU5NJaqr+TQYRCTmKTdZRK0JLV7oowd0kevtcogbGlw081W5iYbBj3H5bYpF0ywUbWgK9zk9hL\nydrwF9TSk4vNzRG1wGm71muGPQyvGAo9/0z06R8UchKbLKNUTBEQ3z5xqfO+KSIiy6gJ/4RQSZH+\nb3MXJF2M/s7JS/NFpBIMZ3pmtmdiSToO27U+cP+mxN9lok/2cLGTJCIri9i1vZs/xjmrVneuC3nV\niqAdEdISXOhLw+MTZ7smsyiGvcX0e/94vnz3Wjwxdmfq8aS51oxayRYudpJMySo+3dVf6v9cVr5X\nl4W8NcMeDty/KbIHqYu7wFXUnn39bcO3e9Fiffz8ZUwHjl0BeObEJWy746ZMmhenKbVA4c6fvtUj\nJ8SEq7/U/7mwZhZxiBL++kit40JYvcJeU921cYJrze0w9474SpDfWPOw7Y6brMehEL6AS5YOFHJS\nKC7t3YJCmbbVnSZK+P1i61XNt4oAXXHRtk43cboyVcXeMGKosvjeVLOFR46cDo0VN4k8u/EsPSjk\npFB0e7cwggkkaVvdafbu3BDZcUiLbVi8dXAR1mR1x3EHfemztxs+OU8wLDFqhSs4WbEbz9KEQk4K\nZ2xL3dqDsu4r/hX8jkuru6j9/qOP3xT5Oe3TNuF/PaqkcNT3NdvuiB6TC6anlCz7tZLBgUJOBoKs\n3CVx+cX/i14g1QuTUeMLs7rjHN+hl9L3ZamKYPfW3oVGJuksTSjkZCDIyl0SlygB6xbbRUdGRdAj\nlFFW98qhxdttzbBnPL6JyUYmtVnaSuH5U40el0lWC8VksGD4IRkY8g5VM4X/2bIPAXRlWu79zpmu\nLkNzCvifr3eH9+3YONoT/+1VBTs2jvbEq1+3lLXN0sVhqmtSVJIO48nzhRY5WTKERWPYFvl2bBw1\nujye2rO543sfP3bB2CpuTgEHXzzX2f6RN3rjv9tthZfPvOfsl87axRHcXhFPPlxgzR9a5GRJEJW2\nb1vkO37+Mr7xxTuN1qJLj0tdf2X82AVjoas52Gu0mEQ77AkhCRURY7p+P61hVkHMHwo5WRLYxOLR\n586Exlq/O9U0ClucOtx6O3Ex+aVtro+kdc3bShXevYcLrPlDIS8R9DPasYlCVBEs2yKfax3uFVXB\n9sOvhsZzrxn28MH12R73TGOqiY/v/35X5UNbcSsgec/Roq1fVkHMHwp5ScijT2IZJwbbmJO4JMIW\n+VytxZm2Ct1vRYBdn77F6D8HFiea4PW0XYcoV4+NIq1fVkHMH1Y/LAlZVfzTlLENl2nMukHDSM3D\ntZlZp4YMAlgnLhe/eJ5EXc+JyUaiBtJVEcwpVdiEXUajYRBh9cOSk7WfMc8FqLxuWtOYtZxNNVvw\nKoI1w16oLzlMKOP6xfMg7Hrq8SWpmW6z/PsFqyDmSyohF5EHARwE8PcAfEYpRTM7J7L2M+a1AJWH\nC8h1bK05heEVQ6FCPj0zi/X7jhonmDC/eDVF04k4+K9ncEKcnuntXh9FRRZ7jmqK9pmT7EkbR/4T\nAF8E8KMMxkJCyDqFPa8MvzxrebiMLUrs359udWKZ9373DDYfeqUTdx7mTumHiPuvpyn22jVqpeZV\n8ZW712LNsNcj4posJmxWUBwcUgm5UuqnSilW2+kDWSdy5FXbJM9QM5eSt7eO1KwFuIK02gpTzUVh\nj6qEGMVIzYNXTbaV4PWM070+uJ3dW+t4/lR4qn+aCZsJPoNH33zkIvIQgIcAYO3atf3a7ZIiSz9j\n0h6OUeQZauYfsxZev8Hpn4j2fvdM7E70UZ8O7s/PSM3D6QP3xl4stbWISzLxCYDX9t2D7YdfjZwE\nwkIfo2CCz+ARKeQi8kMAHzO89ZhS6nuuO1JKPQ3gaWA+asV5hCQ3spoY/L7cGxesUr+IZhlq5h+z\nbVF1YrIRrcoJCNukrleux+YaWRLm4oobOaO35ToJhC2Ahi1YM8Fn8IgUcqXU5/oxEFJOgoub/uiR\nqelWLEs/brRLcCLSftu8Qge1yybsiSNuZElwgvNb9MEnAK8igPQ2lwC6J8skk4Dfoo5asGaCz+DB\nolkkFabHbB09EqfpQ1q/q//7eaCFcsfGUeP771/7EOv3HcWjz52J5dseP3ahc4zBY1BAx28/UvNw\nw6ohtNqq0wpO/z/oX3dZSzChLeqoBeuiascTO2nDD38XwH8BMArgqIicVkrtzGRkpBTk2dE+jt81\n6eKgC1WRjlDaom+mF8rSxo1u8Vu7tjj5kZqHD2fnOu+1lQpN3rKtfzx85HToWKJcM42pZid0c/fW\nOo6fv8wEnwEhbdTKXyilblNKrVRK/V2K+PLD9jitq+65EiYeab6fFq8q+MiqITxy5HRubhs9YdmO\nYarZih3SObal3qm3/u5UE+PHLmD1CruVHnTN2NBPS8+famDvzg14cs9mAOicH0auFANdKyQVtsd4\nXXXP9ca2iYcATtvIwz8rArTnsg1RtBHWFzTsO7Z4bpOramZ2DtVK7xEEuxW5uGaarTYOvnjO2R3G\nuPN8Ya0Vkpqw+h+utWAmJhvWR3+XbfQzvT4sDDEpOgQwTgnbYa+CZmuuZyxrhj0oZa6DPlLzsHrl\nUKRLxL/wHPdYg9erjHV9BhVbrRVa5CQ1Y1vqmLMYBFm4PFy2YUqY+srda7v+fmrBDZAW7bfOkmsf\nzgJAzzHs3lrHzGxvWzivIpg2iDgwn71qa2ZxtdnCa/vuwVuHd2Hvzg0YP3ahYyU/PnG2YzWPH7uA\nvTs34K3Du2Ifa/B65ZntS+Zh0SySCWlC0rRFH7ZtF0zhiMfPX+76zEjNs4qcK8NeBR8GxDWtlT7V\nbGH/C2exe+v8+LWrItgDVLNiqILWTPynj2CYpD/E0L+vxlQTDx85jf0vvIkZQ7ijVxHcsMpc1yZ4\nvRh3nj+0yEkmmPyqXkU6RapsflGXuOskYW22cMbP33XLfDy2jwrm3RGuTLfmjNEl9RjlAUw0W218\n+8QlpwXVawlE3L+g6Rrl02zNoW0o2HLDqiEcuH+TUxhiXnV9yCIUcpIJQdfGSM0DpLtI1cNHTmPz\noVe6BD1KUEZqXiI/qu1x/uUz72H8wbu63Bff3LMZk1+7F0/t2Zwo/lrz7lQTe3du6JkoimLNsNeZ\nWKoiHXfGxGQjtTU8Nd1yrv/DuPP8oWuFZIbftbH98KtGF4Z2IWjCrM+aV8XBBzYlGktYKB8A4+Jp\n3PT6ILeO1DC2pY5DL51L1F8zSwTAJ2/5CM69+7cAutPx9373TGpfkLamXco85FXXhyxCISe5EGbx\nNVttHHrpHK63ehfxNP4knCSEpan7k4z8KfG65ritx2YUOutzqgARD+qyAvDaz68YPxu3mFiQJNY0\nG0vkC10rJBei/J/vT/cmuWhqXhX/6Z/fFevGD8Yp21LpgcVJJpgSr63W96dbgMSPTNELq0X4ftOG\nQ1ZFOjXMg+jaOVmUTyb5wDhykgtp4rqf2rM5UiiCFReD/TprXhUVMS8Krhn2MLxiKHJRUfe5jHOH\n1BeeBPKINc8b3ct0x8ZRpt8PKOzZSTIlqlKh/rfJX1zzqlg5VDH60OsLfuaw/Qa3adpOs9XGSM1D\nzUPXZOJVBR9cn3XyYSfxkwcLXin0p01cFhOHP/0+a6ubzZfzhUJeIrK+GZJuz7Uvp/aLmvYDwJjt\nF+Z7fXziLJ45cclZsK42W3hyz+aufV/7cDZ1HLkrOiSxH/HSrudk9YoqZmbnQv3/WTeJyLOPK5mH\nQl4Ssr4ZkmwvrPtN2M0fttDlOpFMTDZiiTiwGEXi3+b6fUdjbCE9+tiSFtvyqoLVK4ZwtdnCyLCX\nOhrm3Nfvc+pilGbycWkazY5C2UIhLwlZt9eKuz0Xn3fcmz9OJMP4sQuxRNyriNG6TyOqSdATVPDc\nubhCTC3YNh96xVpDxV/qNgx93teFTGpJF2xNBoINZnZmB6NWSkLWac5xt+eSCZhntEbs47Tk5CRt\nupCUxkIJ2d1b653kHBcR1xNRcKI7+IA5m/LgA5s6yTk2glE4ujGFiaTJOnHqwjOzMzso5CUh6zTn\nuNuLEtI0mXouJU7DjtMkR622MhZl8mcj9gt//e76SM3pyaI1Fz3+YDjg2JY6Xtt3D57as7knu9Sr\nSE9y1Zc+e7tx39s/flNil4frhMvMzmyhkJeErNOc424vTEjTxBabaqI8cuQ0Hp842/U503gFwFfu\nXmvddmOqaZwYtOD1k6jmESaSPm2Nban3lCEYf7A3Ln/bHTf11CevVgQPbrOf0yhGLDVrRmpeZCo/\nSQ595CUh6zTnuNuz1cpOe0Pa2pvpSnxPjN0ZOd7j5y9bfbGNqSb2fucMDr10LnYzaI1XldTZkED8\nhc8bF1wh/sXDkUDWqZ74Tl680jlXgNv6w/ixCz0FsdoLTwJJJ+UPrs/2vO5V558GKNz5wYQg4kwe\nscDr9x21uhoEwJOG5KDgOHZsHMXzpxrOvtmaV8XurXVridggIzUPV6+3kPZW0YuXjxw57eRe8aqC\nPf/gdqdjs52rMGznXgC8dXiX83Y0tlZ4IzUPpw/cG3t7pBcmBJHU5FEvI8xCVUCPdWiKijjyxtsY\nilFxsNlq49nX33b+/NWFVm9p0G6rsS31yCbImlZb4dnX33ZKJjKdKyB88rWde91vNe61trmCrvYp\nbn85Qx85KZS9OzeE9sF06TbTais0QwpwmYiTaRm1oBxVy3yk1t0TM85Ca5xxBs+VrSa7XjPIqt+q\nhnXHi4NCTgplbEsdXw5ZsHTtNhOXsNA7PzWvih0bR2Ez+FevqPZUcdQf1e3lDj6wqaul2o6No7mE\nQAbPVViugLbUbS6bJK3YWHe8OOhaIYWjF+mCmZtaBPzugUqMuiUCWAtq7d5ad/I968+ZMtq9qsCr\n9taM8X/05MUrXfvRoYi7t9bx8pn3EpULqFakZ5HSJJi2SU9b5lHHniTBCwAOvniuc1yrPNqK/YBC\nTgaCJ8buxLY7boqsyeIq4v5O7jY/sd6fzUdfH6nh+PnLVsFbvWIoVIgbU01jWYFmq42jb76H1SvD\nvx9EVyf0t2sLW3i2+cB1t6AokrpE/P1M359usa5KH2DUChlobJEQYRUFvYoY46ZtPD5x1hjB8pW7\n10bWd+lnudpfxIwkMZVVqKQdqSkAAApzSURBVHlVJxFPGlpqu17+iZUkJ5eoFREZB3A/gBkAPwfw\nr5VSU2m2ScpLHuGJtsf7OaU6tb+D3LBqKNZ+dUMI0+tRcd9pRDxOedu4TS4AcynhlUMVawlhXX89\nzbWLW/qB5W2zIa0D6wcAPqWU+jSAvwKwP/2QSBmJipBISlgkhLUvZ8wKgWHik2dtlrZSPdv2qtJz\nU5rS6+PgX4ydarZwbWa2J4Vfd2V66/CujuUcVTbBRJzIlbx+M8uRVEKulHpFKaVTuU4AuC39kEgZ\nCYuQSENYJISraETVcgnbTtraLDWvimHLgp9OVe9Kpf+9u/DNPZsj0+tdsYVr3rBqyJoyn0Zg40Su\n5PWbWY5kudj5hwCO2N4UkYcAPAQAa9cmr+VABpOsqzNqokoJRDWncKm7vmPjqDViRn9ON8hwbV8X\nXJi0jdOWZOUq3FGuibCnlsmvmbMt05RMjlP6Ia/fzHIkUshF5IcAPmZ46zGl1PcWPvMYgFkAz9i2\no5R6GsDTwPxiZ6LRkoHF5kvOIhkkSuxMouHaBGNisoHnTzV6fN2muPHg/myhkLaFvax9wS6TlOt1\n8U8ItpvTVWBdM4Dz/M0sNyKFXCn1ubD3ReQPAHwewG+pIkJgyEBgK6qVdzKISTTiNMGwJcVcm2mH\ntq+z7cd2zDZxS7PY52I5h10X/2TnEn2TtcBG/Wa4EOpO2qiV+wB8FcA/UUpNZzMkUkayrs6YhjhN\nMMKszCh3wtiWOk5evNKph1IVwe6t0daoTUDjtu9zcU3YrgvQ7e6JEvE8JuWoJyr2+XQnrY/8TwCs\nBPADmU95PqGU+qPUoyKlJI+iWkmI0wQjKrwwbFvaLaPdK22l8PypBrbdYW/MEBQoU7KQaxlZV9eE\n6bpsP/xqbF9/HtfW9pvJurXhUidt1MpvKqVuV0ptXviPIk4KJ04TjKjwwrBtJYm6cHlacPVFp6lt\n4rKP+kitE47Yb/HkQmg8WAiBLDlsAvfUns09oqTDC00JN1GimERsXITI1Rcd1vYt7T6KLnbFSorx\nYK0VsuSI66/3hxfG8fEnibqIcuXEFdCk7qywBhdVkcJbsRW1eF5WWGuFkITYomNGap61tZnpO3rB\ns97nBeLHJ84a4+eLFnENo1Z6sdVaoZCTZUFeojAx2eiqZaIJE8Qsx5J2WxTLckEhJ0uGuOJji/fO\nyvIsquJf3sdFBg+bkHOxk5SKJHVA8q7pUVSEBWuVEA2FnJSKJOKVhdCGFd4qKsKCIXpEQyEnpSKJ\neKUV2iRNjPsRYcEQPaKhkJNSkUS80gpt1FOAKZ5799Z6V8PlPGpss9kx0TCOnJSKJPHFaevAhDUx\nXr/vaGd7/h6h/agTMkj1bUixMGqFlI5+h8zZolL8+KNF2LeS5EUuPTsJKYJ+FueamGxgemY28nP+\ngk5chCT9hj5yQixoF0kw2ceGFmouQpJ+QyEnxIKtUmFVDO2DsCjUXIQk/YauFUIs2FwhbaVQ86rW\nBVcuQiaHJQOSQSEnxIKtUqEubhUmOIPSZKNMsCtQcijkhFgIC3WkUGcPuwIlh0JOiAW6SPoLo32S\nQyEnJARa3v0jSaMOMg+jVgghAwGjfZJDi5wQMhDQlZUcCjkhZGCgKysZdK0QQkjJoZATQkjJSSXk\nIvIfRORNETktIq+IyK1ZDYwQQogbaS3ycaXUp5VSmwG8DOBrGYyJEEJIDFIJuVLq174/VwPof3Fz\nQghZ5qSOWhGR/wjgXwG4CmBHyOceAvAQAKxduzbtbgkhhCwQ2SFIRH4I4GOGtx5TSn3P97n9AFYp\npQ5E7ZQdggghJD6JOwQppT7nuI9nAHwfQKSQE0IIyY60USuf8P35BQDn0w2HEEJIXNL6yA+LyAYA\ncwAuAvij9EMihBASh1RCrpTandVACCGEJIOZnYQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo\n5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQ\nUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo5IQQUnIo\n5IQQUnIyEXIReVRElIjcnMX2CCGEuJNayEXkdgD3AriUfjiEEELikoVF/iSArwJQGWyLEEJITFIJ\nuYh8AUBDKXXG4bMPichJETl5+fLlNLslhBDiYyjqAyLyQwAfM7z1GIB/j3m3SiRKqacBPA0A27Zt\no/VOCCEZESnkSqnPmV4XkTsBrAdwRkQA4DYAPxaRzyil/m+moySEEGIlUshtKKXOAvio/ltEfgFg\nm1LqVxmMixBCiCOMIyeEkJKT2CIPopRal9W2CCGEuEOLnBBCSg6FnBBCSg6FnBBCSg6FnBBCSg6F\nnBBCSg6FnBBCSg6FnBBCSk5mceSEkP4xMdnA+LELeHeqiVtHati7cwPGttSLHhYpCAo5ISVjYrKB\n/S+cRbPVBgA0pprY/8JZAKCYL1PoWiGkZIwfu9ARcU2z1cb4sQsFjYgUDYWckJLx7lQz1utk6UMh\nJ6Rk3DpSi/U6WfpQyAkpGXt3bkDNq3a9VvOq2LtzQ0EjIkXDxU5CSoZe0GTUCtFQyAkpIWNb6hRu\n0oGuFUIIKTkUckIIKTkUckIIKTkUckIIKTkUckIIKTkUckIIKTmilOr/TkUuA7jY9x1nw80AflX0\nIAYEnot5eB4W4blYJI9zcYdSajT4YiFCXmZE5KRSalvR4xgEeC7m4XlYhOdikX6eC7pWCCGk5FDI\nCSGk5FDI4/N00QMYIHgu5uF5WITnYpG+nQv6yAkhpOTQIieEkJJDISeEkJJDIU+AiIyLyHkReVNE\n/kJERooeUxGIyIMick5E5kRkWYacich9InJBRH4mIvuKHk9RiMi3ROSXIvKTosdSJCJyu4gcF5G/\nXLg3/l0/9kshT8YPAHxKKfVpAH8FYH/B4ymKnwD4IoAfFT2QIhCRKoA/BfDbAD4J4Esi8sliR1UY\n/x3AfUUPYgCYBfCoUuqTAO4G8G/78ZugkCdAKfWKUmp24c8TAG4rcjxFoZT6qVJqObdu/wyAnyml\n/kYpNQPgzwF8oeAxFYJS6kcArhQ9jqJRSr2nlPrxwr//FsBPAeTeAYRCnp4/BPC/ih4EKYQ6gLd9\nf7+DPty0pByIyDoAWwC8nve+2OrNgoj8EMDHDG89ppT63sJnHsP8o9Qz/RxbP3E5D4SQbkTkBgDP\nA3hYKfXrvPdHIbeglPpc2Psi8gcAPg/gt9QSDsaPOg/LnAaA231/37bwGlnGiIiHeRF/Rin1Qj/2\nSddKAkTkPgBfBfCAUmq66PGQwngDwCdEZL2IrADw+wBeLHhMpEBERAD8GYCfKqW+2a/9UsiT8ScA\nPgLgByJyWkT+a9EDKgIR+V0ReQfAPwRwVESOFT2mfrKw4P3HAI5hflHrOaXUuWJHVQwi8iyA/wNg\ng4i8IyL/pugxFcR2AP8SwD0L2nBaRH4n750yRZ8QQkoOLXJCCCk5FHJCCCk5FHJCCCk5FHJCCCk5\nFHJCCCk5FHJCCCk5FHJCCCk5/x8VjdSwpMXVrgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F94yQV2HkmyX",
        "colab_type": "text"
      },
      "source": [
        "### Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zrSFbr_SvM6",
        "colab_type": "code",
        "outputId": "d391318a-6885-4d8a-dbe7-899e2929b78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "BERT_PRETRAINED_MODEL = 'bert-base-multilingual-cased'\n",
        "\n",
        "# Load tokenizer\n",
        "print(f'Loading tokenizer <{BERT_PRETRAINED_MODEL}>...', end=' ')\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_MODEL,\n",
        "                                          do_lower_case=False)\n",
        "print('done!')\n",
        "print()\n",
        "\n",
        "# Check tokenizer\n",
        "sample_sent_id = 42\n",
        "\n",
        "print('English')\n",
        "print(train_en[sample_sent_id])\n",
        "print(tokenizer.tokenize(train_en[sample_sent_id]))\n",
        "print()\n",
        "\n",
        "print('Chinese')\n",
        "print(train_zh[sample_sent_id])\n",
        "print(tokenizer.tokenize(train_zh[sample_sent_id]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading tokenizer <bert-base-multilingual-cased>... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 2564866.59B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done!\n",
            "\n",
            "English\n",
            "All 6 of the artillerymen recorded as wounded died).\n",
            "\n",
            "['All', '6', 'of', 'the', 'artillery', '##men', 'recorded', 'as', 'wounded', 'died', ')', '.']\n",
            "\n",
            "Chinese\n",
            "据记录 ， 所有 6 名炮兵都受伤了) 。\n",
            "\n",
            "['据', '记', '录', '，', '所', '有', '6', '名', '炮', '兵', '都', '受', '伤', '了', ')', '。']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnhlGJz8klTp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIwHlUgIdYbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def get_mini_batches(inputs, scores, *, batch_size=8):\n",
        "\n",
        "    idxs = np.arange(len(inputs))\n",
        "    np.random.shuffle(idxs)\n",
        "\n",
        "    num_batches = math.ceil(len(inputs) / batch_size)\n",
        "\n",
        "    for batch_id in range(num_batches):\n",
        "        start_id = batch_id * batch_size\n",
        "        end_id = (batch_id + 1) * batch_size\n",
        "        yield inputs[idxs[start_id:end_id]], scores[idxs[start_id:end_id]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-yHLtLaRuK7",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4aXOvi_blZ7",
        "colab_type": "code",
        "outputId": "3e0a0dbf-1512-4d2f-e7fe-c0abbe7e3a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "# Utilities\n",
        "\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "def RMSELoss(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred - target) ** 2))\n",
        "\n",
        "print(torch.cuda.memory_summary(device=device))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzJ3LQfVXMcr",
        "colab_type": "text"
      },
      "source": [
        "### BERT fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlJHziqpRwT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BertConfig(vocab_size_or_config_json_file=30522)\n",
        "\n",
        "class BertForQE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(BERT_PRETRAINED_MODEL, num_labels=1)\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # self.out = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        score = self.bert(input_ids)\n",
        "        # pooled = self.dropout(pooled_output)\n",
        "        # score = self.out(pooled_output)\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VRC6uyOgUOM",
        "colab_type": "code",
        "outputId": "3d65ab4f-49a8-437e-dc46-27c5c1f7f65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForQE()\n",
        "print(model)\n",
        "\n",
        "LR = 0.003\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# optimiser = BertAdam(model.parameters(), lr=LR, schedule='warmup_linear', warmup=0.1, t_total=1000)\n",
        "\n",
        "loss_fn = RMSELoss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertForQE(\n",
            "  (bert): BertForSequenceClassification(\n",
            "    (bert): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): BertLayerNorm()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCGWXTLNZ0tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(model, loss_fn, optimiser, input_ids, scores):\n",
        "    model.train()\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    predictions = model(input_ids.to(device)).squeeze()\n",
        "    \n",
        "    loss = loss_fn(predictions, torch.Tensor(scores).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimiser.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train(model, loss_fn, optimiser, train_inputs, train_scores, val_inputs, val_scores, *, num_epochs=10, batch_size=32):\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        print(f'Epoch #{epoch_idx + 1}')\n",
        "\n",
        "        model.to(device)\n",
        "        \n",
        "        processed = 0\n",
        "        increment = 0.05\n",
        "        milestone = 0.05\n",
        "        print('Training', end='')\n",
        "        for input_ids, scores in get_mini_batches(train_inputs, train_scores, batch_size=batch_size):\n",
        "            loss = gradient_descent(model, loss_fn, optimiser, input_ids, scores)\n",
        "            processed += len(input_ids)\n",
        "\n",
        "            if processed / len(train_inputs) >= milestone:\n",
        "                print('.', end='')\n",
        "                milestone += increment\n",
        "        print('done!')\n",
        "        print('Recent loss', loss)\n",
        "\n",
        "        # Check validation loss\n",
        "        model.eval()\n",
        "        model.to('cpu')\n",
        "\n",
        "        print('Getting validation predictions...', end='')\n",
        "        val_preds = model(val_inputs[:50]).squeeze().detach()\n",
        "        print(f'done! {val_preds[0]}..{val_preds[-1]}')\n",
        "        val_loss = loss_fn(val_preds, torch.Tensor(val_scores[:50]))\n",
        "        print('Validation loss', val_loss)\n",
        "\n",
        "        pearson, _ = pearsonr(val_preds.numpy(), np.array(val_scores[:50]))\n",
        "        print('Validation Pearson', pearson)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSoCgQ7YmKvW",
        "colab_type": "code",
        "outputId": "1c2a51a1-651b-415e-b17c-b90edd6c5e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(model, loss_fn, optimiser, train_inputs[:100], train_scores[:100], val_inputs, val_scores, batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Training.............done!\n",
            "Recent loss tensor(0.8049, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! -0.7593356966972351..-0.7593356966972351\n",
            "Validation loss tensor(1.1567)\n",
            "Validation Pearson 0.020010134978328507\n",
            "Epoch #2\n",
            "Training.............done!\n",
            "Recent loss tensor(1.2965, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.27154621481895447..0.27154621481895447\n",
            "Validation loss tensor(0.7492)\n",
            "Validation Pearson -0.15317945884856743\n",
            "Epoch #3\n",
            "Training.............done!\n",
            "Recent loss tensor(1.0521, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.06314269453287125..0.06314270198345184\n",
            "Validation loss tensor(0.7395)\n",
            "Validation Pearson 0.0981606542187859\n",
            "Epoch #4\n",
            "Training.............done!\n",
            "Recent loss tensor(1.3717, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.17315824329853058..0.17315824329853058\n",
            "Validation loss tensor(0.7373)\n",
            "Validation Pearson nan\n",
            "Epoch #5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training.............done!\n",
            "Recent loss tensor(1.1355, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! -0.018087113276124..-0.018087105825543404\n",
            "Validation loss tensor(0.7515)\n",
            "Validation Pearson 0.11258339568929353\n",
            "Epoch #6\n",
            "Training.............done!\n",
            "Recent loss tensor(1.1619, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.05061214044690132..0.05061214044690132\n",
            "Validation loss tensor(0.7408)\n",
            "Validation Pearson nan\n",
            "Epoch #7\n",
            "Training.............done!\n",
            "Recent loss tensor(1.2263, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.14069204032421112..0.14069204032421112\n",
            "Validation loss tensor(0.7363)\n",
            "Validation Pearson -0.08381997168645508\n",
            "Epoch #8\n",
            "Training.............done!\n",
            "Recent loss tensor(1.2192, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.05778025463223457..0.05778025463223457\n",
            "Validation loss tensor(0.7400)\n",
            "Validation Pearson 0.1565319490376306\n",
            "Epoch #9\n",
            "Training.............done!\n",
            "Recent loss tensor(1.3176, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.17504118382930756..0.17504118382930756\n",
            "Validation loss tensor(0.7374)\n",
            "Validation Pearson nan\n",
            "Epoch #10\n",
            "Training.............done!\n",
            "Recent loss tensor(1.1004, device='cuda:0', grad_fn=<SqrtBackward>)\n",
            "Getting validation predictions...done! 0.06424802541732788..0.06424804031848907\n",
            "Validation loss tensor(0.7394)\n",
            "Validation Pearson -0.02817262831633989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQJ06kXdK-qc",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZU6c-b6Lgp1",
        "colab_type": "text"
      },
      "source": [
        "### English\n",
        "\n",
        "1. Tokenise with spaCy language model\n",
        "2. Remove stop words and punctuation\n",
        "3. Normalise - lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYS01soSJ8DP",
        "colab_type": "code",
        "outputId": "11dbb763-1100-40f7-b603-d45194c5c7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Downloading spacy models for English\n",
        "\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300 --force"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 78.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=23895519225c05941575a08fb0e866165e23bab8626baa761c44bbd91c99504b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7ik_891b/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysk8sSgwOMNJ",
        "colab_type": "code",
        "outputId": "4cb0d399-305f-4a9a-f286-c171e1ed8e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Downloading stop words for English\n",
        "\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "download('stopwords')\n",
        "stop_words_en = set(stopwords.words('english'))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFh-oANJOv9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get tokenizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh5Bst_uRXzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_en(sentence=None, *, keep_stopwords=False):\n",
        "    def wrapper(sentence):\n",
        "        text = sentence.lower()\n",
        "        processed = [token.lemma_ for token in nlp_en.tokenizer(text)]\n",
        "        processed = [token for token in processed if token.isalpha()]\n",
        "        if not keep_stopwords:\n",
        "            processed = [token for token in processed if token not in stop_words_en]\n",
        "        return processed\n",
        "\n",
        "    return wrapper if sentence is None else wrapper(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UsvjK3mMGEW",
        "colab_type": "text"
      },
      "source": [
        "### Chinese\n",
        "\n",
        "1. Tokenise with jieba\n",
        "2. Remove stop words and punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaaRTU3XMPmA",
        "colab_type": "code",
        "outputId": "8b1e1efd-cb51-4c0c-d5e9-9fd4a72da2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Download stop words\n",
        "FILE_STOP_WORDS_ZH = './chinese_stop_words.txt'\n",
        "\n",
        "if not os.path.exists(FILE_STOP_WORDS_ZH):\n",
        "    !wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "with open(FILE_STOP_WORDS_ZH, 'r', encoding='utf-8') as f:\n",
        "    stop_words_zh = [line.rstrip() for line in f]"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 10:41:47--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "\rchinese_stop_words.     [<=>                 ]       0  --.-KB/s               \rchinese_stop_words.     [ <=>                ] 416.75K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-02-17 10:41:47 (17.8 MB/s) - ‘chinese_stop_words.txt’ saved [426748]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmS0Q6fOUO2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jieba\n",
        "\n",
        "def preprocess_zh(sentence=None, *, keep_stopwords=False):\n",
        "    def wrapper(sentence):\n",
        "        tokens = jieba.cut(sentence, cut_all=True)\n",
        "        processed = [token for token in tokens if token.isalnum()]\n",
        "        if not keep_stopwords:\n",
        "            processed = [token for token in processed if token not in stop_words]\n",
        "        return processed\n",
        "\n",
        "    return wrapper if sentence is None else wrapper(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meeee7ttmJD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn3Ar3-6MZWV",
        "colab_type": "text"
      },
      "source": [
        "## Language Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgPvP2XYMr5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Language(object):\n",
        "\n",
        "    SOS_TOKEN = '<SOS>'\n",
        "    EOS_TOKEN = '<EOS>'\n",
        "    UNK_TOKEN = '<UNK>'\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2idx = {}\n",
        "        self.word2count = {}\n",
        "        self.idx2word = {0: self.SOS_TOKEN,\n",
        "                         1: self.EOS_TOKEN,\n",
        "                         2: self.UNK_TOKEN}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for token in sentence:\n",
        "            self.add_word(token)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            idx = len(self)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "        \n",
        "        count = self.word2count.get(word, 0)\n",
        "        self.word2count[word] = count + 1\n",
        "\n",
        "    def sent_to_idxs(self, sent):\n",
        "        return [self.word2idx.get(word, 2) for word in sent]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f'Language(name={self.name}) with {len(self)} words'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DZWJs-CQMIb",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aaR6q37RMsO",
        "colab_type": "code",
        "outputId": "501a8e94-04f1-4e92-ca4b-b5b6bc0453f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# English data\n",
        "\n",
        "preprocess_english = preprocess_en(keep_stopwords=True)\n",
        "\n",
        "train_en_sents = [preprocess_english(sent) for sent in train_en]\n",
        "val_en_sents = [preprocess_english(sent) for sent in val_en]\n",
        "test_en_sents = [preprocess_english(sent) for sent in test_en]\n",
        "\n",
        "EN = Language('EN')\n",
        "for sent in train_en_sents:\n",
        "    EN.add_sentence(sent)\n",
        "\n",
        "print(EN)\n",
        "\n",
        "print()\n",
        "print('Sample sentence')\n",
        "sample_sent_en = train_en_sents[42]\n",
        "print(sample_sent_en)\n",
        "print(EN.sent_to_idxs(sample_sent_en))\n",
        "\n",
        "train_en_idxs = [EN.sent_to_idxs(sent) for sent in train_en_sents]\n",
        "val_en_idxs = [EN.sent_to_idxs(sent) for sent in val_en_sents]\n",
        "test_en_idxs = [EN.sent_to_idxs(sent) for sent in test_en_sents]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language(name=EN) with 19248 words\n",
            "\n",
            "Sample sentence\n",
            "['all', 'of', 'the', 'artilleryman', 'record', 'a', 'wound', 'die']\n",
            "[340, 31, 3, 341, 342, 59, 343, 344]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DEZK9El_U9v3",
        "outputId": "5de6464f-c983-4f7e-f988-5340ccca075e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Chinese data\n",
        "\n",
        "preprocess_chinese = preprocess_zh(keep_stopwords=True)\n",
        "\n",
        "train_zh_sents = [preprocess_chinese(sent) for sent in train_zh]\n",
        "val_zh_sents = [preprocess_chinese(sent) for sent in val_zh]\n",
        "test_zh_sents = [preprocess_chinese(sent) for sent in test_zh]\n",
        "\n",
        "ZH = Language('ZH')\n",
        "for sent in train_zh_sents:\n",
        "    ZH.add_sentence(sent)\n",
        "\n",
        "print(ZH)\n",
        "\n",
        "print()\n",
        "print('Sample sentence')\n",
        "sample_sent_zh = train_zh_sents[42]\n",
        "print(sample_sent_zh)\n",
        "print(ZH.sent_to_idxs(sample_sent_zh))\n",
        "\n",
        "train_zh_idxs = [ZH.sent_to_idxs(sent) for sent in train_zh_sents]\n",
        "val_zh_idxs = [ZH.sent_to_idxs(sent) for sent in val_zh_sents]\n",
        "test_zh_idxs = [ZH.sent_to_idxs(sent) for sent in test_zh_sents]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.921 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Language(name=ZH) with 23851 words\n",
            "\n",
            "Sample sentence\n",
            "['据', '记录', '所有', '6', '名', '炮兵', '都', '受伤', '了']\n",
            "[483, 484, 485, 267, 486, 487, 488, 489, 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCR41KviW6Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process scores\n",
        "\n",
        "def prepare_score(score):\n",
        "    return float(score)\n",
        "\n",
        "train_scores = [prepare_score(score) for score in train_scores]\n",
        "val_scores = [prepare_score(score) for score in val_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qYQlzaEWViN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datasets\n",
        "\n",
        "train_en_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in train_en_idxs]\n",
        "train_zh_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in train_zh_idxs]\n",
        "\n",
        "train_pairs = list(zip(train_en_tensors, train_zh_tensors))\n",
        "train_set = list(zip(train_pairs, train_scores))\n",
        "\n",
        "val_en_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in val_en_idxs]\n",
        "val_zh_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in val_zh_idxs]\n",
        "\n",
        "val_pairs = list(zip(val_en_tensors, val_zh_tensors))\n",
        "val_set = list(zip(val_pairs, val_scores))\n",
        "\n",
        "test_en_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in test_en_idxs]\n",
        "test_zh_tensors = [torch.LongTensor(sent_idxs) for sent_idxs in test_zh_idxs]\n",
        "\n",
        "test_pairs = list(zip(test_en_tensors, test_zh_tensors))\n",
        "\n",
        "\n",
        "# val_pairs = list(zip(val_en_idxs, val_zh_idxs))\n",
        "# test_pairs = list(zip(test_en_idxs, test_zh_idxs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBU5dUInLx-R",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtsfA_hdPmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities\n",
        "\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "def unzip(args):\n",
        "    return zip(*args)\n",
        "\n",
        "def RMSELoss(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred - target) ** 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFiF4FSRlCx",
        "colab_type": "text"
      },
      "source": [
        "### FFNN with trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oWCA9Mdbbta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, *, en_vocab_size, zh_vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.en_vocab_size = en_vocab_size\n",
        "        self.zh_vocab_size = zh_vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.en_embedding = nn.Embedding(self.en_vocab_size, self.emb_dim)\n",
        "        self.zh_embedding = nn.Embedding(self.zh_vocab_size, self.emb_dim)\n",
        "\n",
        "        self.en_hidden = nn.Linear(self.emb_dim, 1)\n",
        "        self.zh_hidden = nn.Linear(self.emb_dim, 1)\n",
        "\n",
        "        self.out = nn.Linear(2, 1)\n",
        "    \n",
        "    def forward(self, en_tensors, zh_tensors):\n",
        "        en_emb = self.en_embedding(en_tensors)\n",
        "        zh_emb = self.zh_embedding(zh_tensors)\n",
        "\n",
        "        en_hid = F.relu(self.en_hidden(en_emb))\n",
        "        zh_hid = F.relu(self.zh_hidden(en_emb))\n",
        "\n",
        "        hid_concat = torch.stack((en_hid, zh_hid), axis=1).squeeze()\n",
        "        score = self.out(hid_concat)\n",
        "        return score.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1SoBa5csKg",
        "colab_type": "code",
        "outputId": "6fb3fc7f-8e45-494d-bc20-ec352a0efeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "ffnn = FFNN(en_vocab_size=len(EN), zh_vocab_size=len(ZH), emb_dim=200)\n",
        "ffnn.to(device)\n",
        "print(ffnn)\n",
        "\n",
        "ffnn_opt = torch.optim.Adam(ffnn.parameters(), lr=0.003)\n",
        "loss_fn = RMSELoss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (en_embedding): Embedding(19248, 200)\n",
            "  (zh_embedding): Embedding(23851, 200)\n",
            "  (en_hidden): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (zh_hidden): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (out): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa9MTNFHdTJ4",
        "colab_type": "code",
        "outputId": "9914e71c-2679-4927-a496-bec543c87efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "NUM_EPOCHS = 100\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_pearson = []\n",
        "\n",
        "for eidx in range(NUM_EPOCHS):\n",
        "    print(f'Epoch {eidx + 1}: \\t', end=' ')\n",
        "    ffnn.zero_grad()\n",
        "    \n",
        "    loss = 0\n",
        "    for (en_tensor, zh_tensor), score in train_set:\n",
        "        pred = ffnn(en_tensor.to(device), zh_tensor.to(device))\n",
        "        loss += loss_fn(pred, score)\n",
        "\n",
        "    loss /= len(train_set)\n",
        "    train_losses.append(loss)\n",
        "    \n",
        "    print(f'train loss = {loss:.5f}\\t', end='')\n",
        "\n",
        "    # Validation loss\n",
        "    val_loss = 0\n",
        "    for (en_tensor, zh_tensor), score in val_set:\n",
        "        pred = ffnn(en_tensor.to(device), zh_tensor.to(device))\n",
        "        val_loss += loss_fn(pred, score)\n",
        "    val_loss /= len(val_set)\n",
        "    print(f'validation loss = {val_loss:.5f}\\t', end='')\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Validation score\n",
        "    val_preds, val_targets = unzip([(ffnn(en_tensor.to(device), zh_tensor.to(device)).detach().cpu().numpy(), score)\n",
        "                              for (en_tensor, zh_tensor), score in val_set])\n",
        "    \n",
        "    val_preds = np.array(val_preds)\n",
        "    val_targets = np.array(val_targets)\n",
        "\n",
        "    pearson_score, _ = pearsonr(val_preds, val_targets)\n",
        "    val_pearson.append(pearson_score)\n",
        "    print(f'validation pearson = {pearson_score:.5f}\\t')\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    ffnn_opt.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: \t train loss = 0.87688\tvalidation loss = 0.88592\tvalidation pearson = 0.01748\t\n",
            "Epoch 2: \t train loss = 0.86025\tvalidation loss = 0.87088\tvalidation pearson = 0.02019\t\n",
            "Epoch 3: \t train loss = 0.84733\tvalidation loss = 0.85943\tvalidation pearson = 0.01939\t\n",
            "Epoch 4: \t train loss = 0.83801\tvalidation loss = 0.85154\tvalidation pearson = 0.02470\t\n",
            "Epoch 5: \t train loss = 0.82957\tvalidation loss = 0.84433\tvalidation pearson = 0.03254\t\n",
            "Epoch 6: \t train loss = 0.82264\tvalidation loss = 0.83841\tvalidation pearson = 0.03838\t\n",
            "Epoch 7: \t train loss = 0.81701\tvalidation loss = 0.83367\tvalidation pearson = 0.04050\t\n",
            "Epoch 8: \t train loss = 0.81212\tvalidation loss = 0.82953\tvalidation pearson = 0.04263\t\n",
            "Epoch 9: \t train loss = 0.80755\tvalidation loss = 0.82555\tvalidation pearson = 0.04476\t\n",
            "Epoch 10: \t train loss = 0.80332\tvalidation loss = 0.82175\tvalidation pearson = 0.04683\t\n",
            "Epoch 11: \t train loss = 0.79937\tvalidation loss = 0.81817\tvalidation pearson = 0.04816\t\n",
            "Epoch 12: \t train loss = 0.79570\tvalidation loss = 0.81483\tvalidation pearson = 0.04975\t\n",
            "Epoch 13: \t train loss = 0.79230\tvalidation loss = 0.81172\tvalidation pearson = 0.05156\t\n",
            "Epoch 14: \t train loss = 0.78910\tvalidation loss = 0.80876\tvalidation pearson = 0.05291\t\n",
            "Epoch 15: \t train loss = 0.78611\tvalidation loss = 0.80595\tvalidation pearson = 0.05509\t\n",
            "Epoch 16: \t train loss = 0.78332\tvalidation loss = 0.80328\tvalidation pearson = 0.05735\t\n",
            "Epoch 17: \t train loss = 0.78071\tvalidation loss = 0.80075\tvalidation pearson = 0.05934\t\n",
            "Epoch 18: \t train loss = 0.77822\tvalidation loss = 0.79836\tvalidation pearson = 0.06081\t\n",
            "Epoch 19: \t train loss = 0.77587\tvalidation loss = 0.79610\tvalidation pearson = 0.06205\t\n",
            "Epoch 20: \t train loss = 0.77364\tvalidation loss = 0.79395\tvalidation pearson = 0.06280\t\n",
            "Epoch 21: \t train loss = 0.77155\tvalidation loss = 0.79191\tvalidation pearson = 0.06318\t\n",
            "Epoch 22: \t train loss = 0.76956\tvalidation loss = 0.78997\tvalidation pearson = 0.06354\t\n",
            "Epoch 23: \t train loss = 0.76765\tvalidation loss = 0.78811\tvalidation pearson = 0.06419\t\n",
            "Epoch 24: \t train loss = 0.76582\tvalidation loss = 0.78636\tvalidation pearson = 0.06420\t\n",
            "Epoch 25: \t train loss = 0.76406\tvalidation loss = 0.78469\tvalidation pearson = 0.06371\t\n",
            "Epoch 26: \t train loss = 0.76236\tvalidation loss = 0.78310\tvalidation pearson = 0.06218\t\n",
            "Epoch 27: \t train loss = 0.76072\tvalidation loss = 0.78159\tvalidation pearson = 0.06035\t\n",
            "Epoch 28: \t train loss = 0.75912\tvalidation loss = 0.78014\tvalidation pearson = 0.05821\t\n",
            "Epoch 29: \t train loss = 0.75758\tvalidation loss = 0.77875\tvalidation pearson = 0.05548\t\n",
            "Epoch 30: \t train loss = 0.75608\tvalidation loss = 0.77741\tvalidation pearson = 0.05306\t\n",
            "Epoch 31: \t train loss = 0.75461\tvalidation loss = 0.77611\tvalidation pearson = 0.05023\t\n",
            "Epoch 32: \t train loss = 0.75317\tvalidation loss = 0.77483\tvalidation pearson = 0.04724\t\n",
            "Epoch 33: \t train loss = 0.75175\tvalidation loss = 0.77359\tvalidation pearson = 0.04478\t\n",
            "Epoch 34: \t train loss = 0.75036\tvalidation loss = 0.77236\tvalidation pearson = 0.04302\t\n",
            "Epoch 35: \t "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUlPsvWsL4mp",
        "colab_type": "text"
      },
      "source": [
        "### RNN Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UkdfLZMoI5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNChain(nn.Module):\n",
        "\n",
        "    def __init__(self, *, en_vocab_size, zh_vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.en_vocab_size = en_vocab_size\n",
        "        self.zh_vocab_size = zh_vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.en_embedding = nn.Embedding(self.en_vocab_size, self.emb_dim)\n",
        "        self.zh_embedding = nn.Embedding(self.zh_vocab_size, self.emb_dim)\n",
        "\n",
        "        self.en_rnn = nn.GRU(self.emb_dim, self.emb_dim, bidirectional=True)\n",
        "        self.zh_rnn = nn.GRU(self.emb_dim, self.emb_dim, bidirectional=True)\n",
        "\n",
        "        self.hidden = nn.Linear(self.emb_dim, 50)\n",
        "        self.out = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, en_tensor, zh_tensor):\n",
        "        en_emb = self.en_embedding(en_tensor)\n",
        "        zh_emb = self.zh_embedding(zh_tensor)\n",
        "\n",
        "        en_hidden = torch.zeros(2, 1, self.emb_dim, device=device)\n",
        "\n",
        "        for word_idx in en_emb:\n",
        "            word_idx = word_idx.view(1, 1, -1)\n",
        "            _, en_hidden = self.en_rnn(word_idx, en_hidden)\n",
        "    \n",
        "        zh_hidden = en_hidden\n",
        "        for word_idx in zh_emb:\n",
        "            word_idx = word_idx.view(1, 1, -1)\n",
        "            _, zh_hidden = self.zh_rnn(word_idx, zh_hidden)\n",
        "\n",
        "        score = self.out(F.relu(self.hidden(zh_hidden[-1])))\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u68m8cGjuZcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNNChain(en_vocab_size=len(EN), zh_vocab_size=len(ZH), emb_dim=100)\n",
        "\n",
        "rnn.load_state_dict(torch.load(in_gdrive('rnn.pt')))\n",
        "rnn.to(device)\n",
        "\n",
        "preds = []\n",
        "for idx, (en_tensor, zh_tensor) in enumerate(test_pairs):\n",
        "    pred = rnn(en_tensor.to(device), zh_tensor.to(device)).squeeze()\n",
        "    preds.append(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfG9o6GCv3su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('predictions.txt', 'w') as f:\n",
        "    for score in preds:\n",
        "        f.write(f'{score}\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDPzyUWzp4ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_PREV = True\n",
        "\n",
        "rnn = RNNChain(en_vocab_size=len(EN), zh_vocab_size=len(ZH), emb_dim=100)\n",
        "rnn.to(device)\n",
        "\n",
        "rnn_opt = torch.optim.Adam(rnn.parameters(), lr=0.003)\n",
        "loss_fn = RMSELoss\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "state = {\n",
        "    'curr_epoch': 1,\n",
        "    'train_losses': [],\n",
        "    'val_losses': [],\n",
        "    'val_pearson': [],\n",
        "}\n",
        "\n",
        "if USE_PREV and os.path.exists(in_gdrive('rnn.pt')):\n",
        "    print('Loading from Google Drive...', end=' ')\n",
        "    rnn.load_state_dict(torch.load(in_gdrive('rnn.pt')))\n",
        "\n",
        "    with open(in_gdrive('rnn.json'), 'r') as f:\n",
        "        state = json.load(f)\n",
        "    print('done!')\n",
        "\n",
        "\n",
        "while state['curr_epoch'] <= NUM_EPOCHS:\n",
        "    print(f'Epoch {state[\"curr_epoch\"]}:')\n",
        "    rnn.zero_grad()\n",
        "    \n",
        "    loss = 0\n",
        "    print(f'Training {len(train_set)}: ', end='')\n",
        "    for idx, ((en_tensor, zh_tensor), score) in enumerate(train_set):\n",
        "        pred = rnn(en_tensor.to(device), zh_tensor.to(device)).squeeze()\n",
        "        curr_loss = loss_fn(pred, score) \n",
        "        loss += curr_loss\n",
        "        if idx % 500 == 0:\n",
        "            print('.', end='')\n",
        "    print()\n",
        "\n",
        "    loss /= len(train_set)\n",
        "    state['train_losses'].append(loss.detach().cpu().numpy().tolist())\n",
        "    \n",
        "    print(f'==>train loss = {loss:.5f}')\n",
        "\n",
        "    # Validation loss\n",
        "    val_loss = 0\n",
        "    print(f'Validating loss {len(val_set)}: ', end='')\n",
        "    for idx, ((en_tensor, zh_tensor), score) in enumerate(val_set):\n",
        "        pred = rnn(en_tensor.to(device), zh_tensor.to(device))\n",
        "        val_loss += loss_fn(pred, score)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            print('.', end='')\n",
        "    print()    \n",
        "\n",
        "    val_loss /= len(val_set)\n",
        "    print(f'==>validation loss = {val_loss:.5f}')\n",
        "    state['val_losses'].append(val_loss.detach().cpu().numpy().tolist())\n",
        "\n",
        "    # Validation score\n",
        "    val_preds, val_targets = unzip([(rnn(en_tensor.to(device), zh_tensor.to(device)).squeeze().detach().cpu().numpy(), score)\n",
        "                              for (en_tensor, zh_tensor), score in val_set])\n",
        "    val_preds = np.array(val_preds)\n",
        "    val_targets = np.array(val_targets)\n",
        "\n",
        "    pearson_score, _ = pearsonr(val_preds, val_targets)\n",
        "    state['val_pearson'].append(pearson_score)\n",
        "    print(f'==>validation pearson = {pearson_score:.5f}')\n",
        "\n",
        "    # Backpropagation\n",
        "    print('Backpropagation...', end=' ')\n",
        "    loss.backward()\n",
        "    rnn_opt.step()\n",
        "    print('done!')\n",
        "\n",
        "    # Save\n",
        "    print('Saving to Google Drive...', end=' ')\n",
        "    torch.save(rnn.state_dict(), in_gdrive('rnn.pt'))\n",
        "\n",
        "    state['curr_epoch'] += 1\n",
        "    with open(in_gdrive('rnn.json'), 'w') as f:\n",
        "        json.dump(state, f)\n",
        "\n",
        "    print('done!\\n')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxsp-O3FL3si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNChain(nn.Module):\n",
        "\n",
        "    def __init__(self, *, vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
        "        self.rnn = nn.GRU(self.emb_dim, self.emb_dim, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        emb = self.embedding(x)\n",
        "        output = emb.view(1, 1, -1)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(2, 1, self.emb_dim, device=device)\n",
        "\n",
        "class RegressorLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, *, emb_dim):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.hidden = nn.Linear(self.emb_dim, 50)\n",
        "        self.out = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(F.relu(self.hidden(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ZnU2SMa9hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_model = RNNChain(vocab_size=len(EN), emb_dim=200)\n",
        "zh_model = RNNChain(vocab_size=len(ZH), emb_dim=200)\n",
        "regressor = RegressorLayer(emb_dim=200)\n",
        "\n",
        "en_model.to(device)\n",
        "zh_model.to(device)\n",
        "regressor.to(device)\n",
        "\n",
        "print(en_model)\n",
        "print(zh_model)\n",
        "\n",
        "LR = 0.003\n",
        "\n",
        "en_opt = torch.optim.Adam(en_model.parameters(), lr=LR)\n",
        "zh_opt = torch.optim.Adam(zh_model.parameters(), lr=LR)\n",
        "regressor_opt = torch.optim.Adam(regressor.parameters(), lr=LR)\n",
        "\n",
        "def RMSELoss(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred - target) ** 2))\n",
        "\n",
        "loss_fn = RMSELoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MOiNoElbLD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(en_tensor, zh_tensor, score):\n",
        "    en_model.zero_grad()\n",
        "    zh_model.zero_grad()\n",
        "\n",
        "    en_hidden = en_model.init_hidden()\n",
        "\n",
        "    for word_idx in en_tensor:\n",
        "        hids, en_hidden = en_model(word_idx, en_hidden)\n",
        "        print('Hids', hids.shape)\n",
        "    \n",
        "    # print('EN final hidden state', en_hidden)\n",
        "\n",
        "    zh_hidden = en_hidden\n",
        "    for word_idx in zh_tensor:\n",
        "        _, zh_hidden = zh_model(word_idx, zh_hidden)\n",
        "    \n",
        "    # print('ZH final hidden state', zh_hidden)\n",
        "\n",
        "    pred_score = regressor(zh_hidden).squeeze()\n",
        "\n",
        "    loss = loss_fn(pred_score, score)\n",
        "    \n",
        "    # print('Loss', loss)    \n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    regressor_opt.step()\n",
        "    zh_opt.step()\n",
        "    en_opt.step()\n",
        "\n",
        "    return loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzmHLS82c8Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for eidx in range(100):\n",
        "    loss = 0\n",
        "    for (en_tensor, zh_tensor), score in train_set[:100]:\n",
        "        loss += train(en_tensor.to(device), zh_tensor.to(device), score)\n",
        "    loss /= 100\n",
        "    print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW6b5QrLdOnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}