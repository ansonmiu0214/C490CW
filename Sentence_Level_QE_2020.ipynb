{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence_Level_QE_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMSq7H2U4vdvxsj+xgd9sOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansonmiu0214/C490CW/blob/master/Sentence_Level_QE_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZxVjhzS2c-V",
        "colab_type": "text"
      },
      "source": [
        "# CO490 Coursework: Quality Estimation\n",
        "\n",
        "__Team__\n",
        "* Anson Miu (kcm116)\n",
        "* Cheryl Chen (czc16)\n",
        "* Clara Gila (acg116)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWQnFCxu4dY2",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3D_kGEs2Ytf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVbHKGnn5rMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "6f8e7fb8-2dd1-4945-da6f-037c2a34a60d"
      },
      "source": [
        "# Setup CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch_device = getattr(torch, str(device))\n",
        "torch_device.empty_cache()\n",
        "print(f'DEVICE={torch_device.get_device_name()}')\n",
        "print(torch.cuda.memory_summary(device=device))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE=Tesla P100-PCIE-16GB\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRPKYtOY4b_R",
        "colab_type": "text"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ0QmO6854oV",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7YdlRu67oI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "add712e9-d7fa-4f2e-82a0-112afb21d18c"
      },
      "source": [
        "if not os.path.exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip\n",
        "\n",
        "TRAIN_EN = 'train.enzh.src'\n",
        "TRAIN_ZH = 'train.enzh.mt'\n",
        "TRAIN_SCORES = 'train.enzh.scores'\n",
        "VAL_EN = 'dev.enzh.src'\n",
        "VAL_ZH = 'dev.enzh.mt'\n",
        "VAL_SCORES = 'dev.enzh.scores'\n",
        "TEST_EN = 'test.enzh.src'\n",
        "TEST_ZH = 'test.enzh.mt'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-25 10:38:06--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=a1907a9deeca8e4ee6c14b3a15238ed6d1ee1aa80cd794e18656aa56b494ee79&X-Amz-Date=20200225T103807Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200225%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-25 10:38:07--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=a1907a9deeca8e4ee6c14b3a15238ed6d1ee1aa80cd794e18656aa56b494ee79&X-Amz-Date=20200225T103807Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200225%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K  1.00MB/s    in 0.8s    \n",
            "\n",
            "2020-02-25 10:38:09 (1.00 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iueDTtRB6_FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read from file\n",
        "\n",
        "with open(TRAIN_EN) as f:\n",
        "    train_en = f.readlines()\n",
        "with open(TRAIN_ZH) as f:\n",
        "    train_zh = f.readlines()\n",
        "with open(TRAIN_SCORES) as f:\n",
        "    train_scores = [float(score.strip()) for score in f]\n",
        "with open(VAL_EN) as f:\n",
        "    val_en = f.readlines()\n",
        "with open(VAL_ZH) as f:\n",
        "    val_zh = f.readlines()\n",
        "with open(VAL_SCORES) as f:\n",
        "    val_scores = [float(score.strip()) for score in f]\n",
        "with open(TEST_EN) as f:\n",
        "    test_en = f.readlines()\n",
        "with open(TEST_ZH) as f:\n",
        "    test_zh = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4aobJfx-8Du",
        "colab_type": "text"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfzhFEZU-9dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "def RMSELoss(pred, target, *, is_numpy=False):\n",
        "    mean = np.mean if is_numpy else torch.mean\n",
        "    sqrt = np.sqrt if is_numpy else torch.sqrt\n",
        "    return sqrt(mean((pred - target) ** 2))\n",
        "\n",
        "def pearson():\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI1jjStOB712",
        "colab_type": "text"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74W4mB7XB-aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def suppress_log(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "def debug_log(*args, **kwargs):\n",
        "    print('[Debug]:', end='')\n",
        "    print(*args, **kwargs)\n",
        "\n",
        "\n",
        "class SentencePairTestDataset(Dataset):\n",
        "    def __init__(self, en, zh):\n",
        "        self.en = en\n",
        "        self.zh = zh\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.en[index], self.zh[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "\n",
        "class SentencePairTrainDataset(Dataset):\n",
        "    def __init__(self, en, zh, scores):\n",
        "        self.en = en\n",
        "        self.zh = zh\n",
        "        self.scores = scores\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.en[index], self.zh[index]), self.scores[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "\n",
        "def build_dataset(en_inputs, zh_inputs, scores=None, idxs=None):\n",
        "    if idxs is not None:\n",
        "        en_inputs = en_inputs[idxs]\n",
        "        zh_inputs = zh_inputs[idxs]\n",
        "        scores = scores[idxs]\n",
        "\n",
        "    en_tensors = [torch.LongTensor(data) for data in en_inputs]\n",
        "    zh_tensors = [torch.LongTensor(data) for data in zh_inputs]\n",
        "\n",
        "    if scores is None:\n",
        "        return SentencePairTestDataset(en_tensors, zh_tensors)\n",
        "    else:\n",
        "        return SentencePairTrainDataset(en_tensors, zh_tensors, scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZb79a39LTYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def torch_to_kfold(cls=None, *, opt, lr, num_epochs, batch_size, loss_fn, **metrics):\n",
        "    def wrapper(cls):\n",
        "\n",
        "        def fit(self, dataset):\n",
        "            # Enter train mode\n",
        "            self.train()\n",
        "            self.to(device)\n",
        "\n",
        "            # Construct data loader\n",
        "            loader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
        "\n",
        "            optimiser = opt(self.parameters(), lr=lr)\n",
        "            \n",
        "            for epoch in range(1, num_epochs + 1):\n",
        "                header = f'Epoch {epoch}'\n",
        "                print(header)\n",
        "                print('=' * len(header))\n",
        "\n",
        "                for X, scores in tqdm(loader, desc='Mini-Batch'):\n",
        "                    optimiser.zero_grad()\n",
        "\n",
        "                    pred = self(*(x.to(device) for x in X)).squeeze()\n",
        "                    loss = loss_fn(pred, scores.to(device))\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimiser.step()\n",
        "\n",
        "        def predict(self, dataset):\n",
        "            # Enter evaluation mode\n",
        "            self.eval()\n",
        "            self.to(device)\n",
        "\n",
        "            loader = DataLoader(dataset=dataset, batch_size=1)\n",
        "\n",
        "            preds = []\n",
        "            scores = []\n",
        "            with torch.no_grad():\n",
        "                for X, score in tqdm(loader):\n",
        "                    pred = self(*(x.to(device) for x in X)).squeeze().cpu()\n",
        "                    preds.append(pred)\n",
        "                    scores.append(score)\n",
        "            \n",
        "                preds = torch.stack(preds)\n",
        "                scores = torch.cat(scores)\n",
        "\n",
        "                loss = loss_fn(preds, scores)\n",
        "                metrics = {name: metric_fn(preds, scores)\n",
        "                           for name, metric in metrics.items()}\n",
        "            return loss, metrics\n",
        "\n",
        "\n",
        "        cls.fit = fit\n",
        "        cls.predict = predict\n",
        "\n",
        "        return cls\n",
        "\n",
        "    return wrapper if cls is None else wrapper(cls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuPigQ4-it1",
        "colab_type": "text"
      },
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptJJuDL-xJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def kfold_cross_validate(model, train_en_inputs, train_zh_inputs, train_scores,\n",
        "                         n_splits=2,\n",
        "                         random_state=0,\n",
        "                         **kwargs):\n",
        "    \n",
        "    if not isinstance(train_scores, torch.FloatTensor):\n",
        "        train_scores = torch.FloatTensor(train_scores)\n",
        "\n",
        "    cv_split = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for train_idxs, test_idxs in cv_split.split(train_en_inputs):\n",
        "\n",
        "        train_set = build_dataset(train_en_inputs, train_zh_inputs, train_scores,\n",
        "                                  idxs=train_idxs)\n",
        "        test_set = build_dataset(train_en_inputs, train_zh_inputs, train_scores,\n",
        "                                 idxs=test_idxs)\n",
        "\n",
        "        # Training\n",
        "        model.fit(train_set)\n",
        "\n",
        "        # Evaluation\n",
        "        predicted = model.predict(test_set)\n",
        "        scores = test_set.scores\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veQBkjct8ZaG",
        "colab_type": "text"
      },
      "source": [
        "## Building Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZ93sp77m7A",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_lrVXI97s2b",
        "colab_type": "text"
      },
      "source": [
        "#### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUPhA4_O7v9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "2567c26d-8337-46c8-d6ea-6b6dafb9b516"
      },
      "source": [
        "# Downloading spacy models for English\n",
        "\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300 --force\n",
        "\n",
        "# Downloading stop words for English\n",
        "\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "download('stopwords')\n",
        "stop_words_en = set(stopwords.words('english'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 63.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=7b6f734d234a8a4f868235bb95b6995173d0435fc9055129f62126b0ccdaf0a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e6vmp3j0/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXtrvOrY7yky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get tokenizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en300')\n",
        "\n",
        "def preprocess_en(sentence=None, *, keep_stopwords=False):\n",
        "    \"\"\"Preprocess English sentence using spaCy for tokenisation.\n",
        "    Toggle `keep_stopwords=True` to preserve stopwords.\"\"\"\n",
        "\n",
        "    def wrapper(sentence):\n",
        "        text = sentence.lower()\n",
        "        processed = [token.lemma_ for token in nlp_en.tokenizer(text)]\n",
        "        processed = [token for token in processed if token.isalpha()]\n",
        "        if not keep_stopwords:\n",
        "            processed = [token for token in processed if token not in stop_words_en]\n",
        "        return processed\n",
        "\n",
        "    return wrapper if sentence is None else wrapper(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgiLUG9H7u1w",
        "colab_type": "text"
      },
      "source": [
        "#### Chinese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-AdPRx28I88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "0b5c7f2c-5c70-49f1-ae2c-46a0d4ba07a2"
      },
      "source": [
        "# Download stopwords\n",
        "FILE_STOP_WORDS_ZH = './chinese_stop_words.txt'\n",
        "\n",
        "if not os.path.exists(FILE_STOP_WORDS_ZH):\n",
        "    !wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "with open(FILE_STOP_WORDS_ZH, 'r', encoding='utf-8') as f:\n",
        "    stop_words_zh = [line.rstrip() for line in f]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-25 10:44:18--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [  <=>               ] 419.55K  1.30MB/s    in 0.3s    \n",
            "\n",
            "2020-02-25 10:44:19 (1.30 MB/s) - ‘chinese_stop_words.txt’ saved [429623]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHaP5WNq8Ktu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jieba\n",
        "\n",
        "def preprocess_zh(sentence=None, *, keep_stopwords=False):\n",
        "    \"\"\"Preprocess Chinese sentence using jieba for tokenisation.\n",
        "    Toggle `keep_stopwords=True` to preserve stopwords.\"\"\"\n",
        "    \n",
        "    def wrapper(sentence):\n",
        "        tokens = jieba.cut(sentence, cut_all=False)\n",
        "        processed = [token for token in tokens if token.isalnum()]\n",
        "        if not keep_stopwords:\n",
        "            processed = [token for token in processed if token not in stop_words_zh]\n",
        "        return processed\n",
        "\n",
        "    return wrapper if sentence is None else wrapper(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPR81o4a8h61",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIphelUd8lw4",
        "colab_type": "text"
      },
      "source": [
        "#### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sthoro0t_U_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z_zZxYe8nGR",
        "colab_type": "text"
      },
      "source": [
        "#### Chinese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMh7P9lM93vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ZH_MODEL_BIN = 'model.bin'\n",
        "\n",
        "if not os.path.exists(ZH_MODEL_BIN):\n",
        "    !wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "    !unzip zh.zip \n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(ZH_MODEL_BIN, binary=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha-UrrMBEsCO",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDQXVmCnFBP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentences(sents, *, pad_token=0):\n",
        "    \"\"\"Pad sentences with `pad_token` to the longest sentence in `sents.\"\"\"\n",
        "\n",
        "    # Get max sentence length\n",
        "    sent_lengths = [len(sent) for sent in sents]\n",
        "    max_sent_len = max(sent_lengths)\n",
        "    \n",
        "    # Create empty matrix with padding tokens\n",
        "    padded_sents = np.ones((len(sents), max_sent_len)) * pad_token\n",
        "\n",
        "    # Copy over the sequences\n",
        "    for i, (sent_len, sent) in enumerate(zip(sent_lengths, sents)):\n",
        "        padded_sents[i, 0:sent_len] = sent[:sent_len]\n",
        "    return padded_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES0WJSe394RK",
        "colab_type": "text"
      },
      "source": [
        "### Vocabulary Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPM1ZsCy97Fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Language(object):\n",
        "\n",
        "    PAD_TOKEN = '<PAD>'\n",
        "    UNK_TOKEN = '<UNK>'\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {0: self.PAD_TOKEN,\n",
        "                         1: self.UNK_TOKEN}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for token in sentence:\n",
        "            self.add_word(token)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            idx = len(self)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "    \n",
        "    def sent_to_idxs(self, sent):\n",
        "        return [self.word2idx.get(word, 1) for word in sent]\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, int):\n",
        "            return self.idx2word[key]\n",
        "        if isinstance(key, str):\n",
        "            return self.word2idx[key]\n",
        "        raise KeyError(key)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f'Language(name={self.name}) with {len(self)} words'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6OH0y463VAB",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA6hjS-d3W8V",
        "colab_type": "text"
      },
      "source": [
        "### 1) Fine-tuning Baseline Regressor\n",
        "---\n",
        "\n",
        "__Pipeline__\n",
        "\n",
        "1. Manual preprocessing\n",
        "    * EN - tokenisation with [spaCy](https://spacy.io),\n",
        "    stopword removal\n",
        "    * ZH - tokenisation with [jieba](https://github.com/fxsjy/jieba),\n",
        "    stopword removal, \n",
        "2. Pretrained embeddings\n",
        "    * EN - GloVe\n",
        "    * ZH - TODO\n",
        "3. Regression model\n",
        "    * SVR\n",
        "    * LinearRegression\n",
        "\n",
        "__Model selection__\n",
        "\n",
        "We perform 2-fold cross validation to select\n",
        "\n",
        "__Evaluation__\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UjgOXZu3WJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C26nlnf3dP2",
        "colab_type": "text"
      },
      "source": [
        "### 2) Baseline with FFNN Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCN0Ys0l3saM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FcYJx1h390-",
        "colab_type": "text"
      },
      "source": [
        "### 3) Autoencoder with Quality Estimation Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-BjP81TD92L",
        "colab_type": "text"
      },
      "source": [
        "#### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-WsPDb8D_cN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a2b078e3-109f-4a5a-ca7c-2b741e157af5"
      },
      "source": [
        "##########\n",
        "# ENGLISH\n",
        "##########\n",
        "\n",
        "preprocess_english = preprocess_en(keep_stopwords=False)\n",
        "train_en_sents = [preprocess_english(sent) for sent in train_en]\n",
        "val_en_sents = [preprocess_english(sent) for sent in val_en]\n",
        "test_en_sents = [preprocess_english(sent) for sent in test_en]\n",
        "\n",
        "EN = Language('EN')\n",
        "for sent in train_en_sents:\n",
        "    EN.add_sentence(sent)\n",
        "print(EN)\n",
        "\n",
        "print()\n",
        "print('Sample sentence')\n",
        "sample_sent_en = train_en_sents[42]\n",
        "print(sample_sent_en)\n",
        "print(EN.sent_to_idxs(sample_sent_en))\n",
        "\n",
        "train_en_idxs = pad_sentences([EN.sent_to_idxs(sent) for sent in train_en_sents])\n",
        "val_en_idxs = pad_sentences([EN.sent_to_idxs(sent) for sent in val_en_sents])\n",
        "test_en_idxs = pad_sentences([EN.sent_to_idxs(sent) for sent in test_en_sents])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language(name=EN) with 19141 words\n",
            "\n",
            "Sample sentence\n",
            "['artilleryman', 'record', 'wound', 'die']\n",
            "[292, 293, 294, 295]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0AZicNdEHXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "3f74aa2f-1375-4b0e-9cad-e738266393a4"
      },
      "source": [
        "##########\n",
        "# CHINESE\n",
        "##########\n",
        "\n",
        "preprocess_chinese = preprocess_zh(keep_stopwords=False)\n",
        "train_zh_sents = [preprocess_chinese(sent) for sent in train_zh]\n",
        "val_zh_sents = [preprocess_chinese(sent) for sent in val_zh]\n",
        "test_zh_sents = [preprocess_chinese(sent) for sent in test_zh]\n",
        "\n",
        "ZH = Language('ZH')\n",
        "for sent in train_zh_sents:\n",
        "    ZH.add_sentence(sent)\n",
        "print(ZH)\n",
        "\n",
        "print()\n",
        "print('Sample sentence')\n",
        "sample_sent_zh = train_zh_sents[0]\n",
        "print(sample_sent_zh)\n",
        "print(ZH.sent_to_idxs(sample_sent_zh))\n",
        "\n",
        "train_zh_idxs = pad_sentences([ZH.sent_to_idxs(sent) for sent in train_zh_sents])\n",
        "val_zh_idxs = pad_sentences([ZH.sent_to_idxs(sent) for sent in val_zh_sents])\n",
        "test_zh_idxs = pad_sentences([ZH.sent_to_idxs(sent) for sent in test_zh_sents])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.800 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Language(name=ZH) with 21992 words\n",
            "\n",
            "Sample sentence\n",
            "['最后', '的', '征服者', '骑着', '他', '的', '剑', '继续前进']\n",
            "[2, 3, 4, 5, 6, 3, 7, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boPVjjtKDpH9",
        "colab_type": "text"
      },
      "source": [
        "#### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcKqF9EG4EvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch_to_kfold(opt=torch.optim.Adam,\n",
        "                lr=1e-3,\n",
        "                num_epochs=10,\n",
        "                batch_size=2,\n",
        "                loss_fn=RMSELoss)\n",
        "class AutoencoderQEV(nn.Module):\n",
        "\n",
        "    def __init__(self, *, en_vocab_size, zh_vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.en_vocab_size = en_vocab_size\n",
        "        self.zh_vocab_size = zh_vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        \"\"\"Source sentence: embedding layer + encoder.\"\"\"\n",
        "        self.source_embedding = nn.Embedding(self.en_vocab_size, self.emb_dim)\n",
        "        self.source_rnn = nn.GRU(input_size=self.emb_dim,\n",
        "                                 hidden_size=self.emb_dim,\n",
        "                                 bidirectional=True)\n",
        "\n",
        "        \"\"\"Target sentence: embedding layer + decoder with attention.\"\"\"\n",
        "        self.target_embedding = nn.Embedding(self.zh_vocab_size, self.emb_dim)\n",
        "        self.target_rnn = nn.GRU(input_size=self.emb_dim,\n",
        "                                 hidden_size=self.emb_dim * 2,\n",
        "                                 bidirectional=False)\n",
        "\n",
        "        \"\"\"RNN for producing summary unit.\"\"\"\n",
        "        self.qualvec_rnn = nn.GRU(input_size=self.emb_dim * 2,\n",
        "                                  hidden_size=self.emb_dim,\n",
        "                                  bidirectional=False)\n",
        "\n",
        "        \"\"\"Regression output layer.\"\"\"\n",
        "        self.regressor_output = nn.Linear(in_features=self.emb_dim,\n",
        "                                          out_features=1)\n",
        "\n",
        "    def forward(self, en_sent, zh_sent, *, log=suppress_log, get_qualvecs=False):\n",
        "        \"\"\"Perform forward pass and returns the prediction scores.\n",
        "\n",
        "        Parameters:\n",
        "            en_sent: (batch_size, en_max_sent_len)\n",
        "            zh_sent: (batch_size, zh_max_sent_len)\n",
        "        \n",
        "        Debug parameters:\n",
        "            log: custom `print` function, defaults to suppressing messages\n",
        "            get_qualvecs: if True, returns the quality vectors instead.\n",
        "        \"\"\"\n",
        "\n",
        "        en_batch_size, en_sent_len = en_sent.shape\n",
        "        en_emb = self.source_embedding(en_sent)\n",
        "        log('en_emb:', en_emb.shape)\n",
        "\n",
        "        en_emb = en_emb.view(en_sent_len, en_batch_size, -1)\n",
        "        log('en_emb:', en_emb.shape)\n",
        "        en_all_hids, en_last_hid = self.source_rnn(en_emb)\n",
        "\n",
        "        log('en_all_hids:', en_all_hids.shape)\n",
        "        log('en_last_hid:', en_last_hid.shape)\n",
        "\n",
        "        ############################################\n",
        "        def get_context(prev_state):\n",
        "            log('prev_state:', prev_state.shape)\n",
        "            s_s = []\n",
        "            for hid in en_all_hids:\n",
        "                s_s_batches = torch.Tensor([\n",
        "                    one_hid_batch.dot(one_prev_state_batch)\n",
        "                    for one_hid_batch, one_prev_state_batch in zip(prev_state, hid)\n",
        "                ])\n",
        "                s_s.append(s_s_batches)\n",
        "            \n",
        "            s_s = torch.stack(s_s, dim=0)\n",
        "            log('s_s', s_s.shape)\n",
        "\n",
        "            a_s = F.softmax(s_s, dim=0)\n",
        "            log('a_s', a_s.shape)\n",
        "\n",
        "            ctx_vecs = []\n",
        "            for j, (a_i, hid) in enumerate(zip(a_s, en_all_hids)):\n",
        "                vecs = []\n",
        "                for i, (one_a_batch, one_hid_batch) in enumerate(zip(a_i, hid)):\n",
        "                    vec = one_a_batch * one_hid_batch\n",
        "                    vecs.append(vec)\n",
        "                \n",
        "                vecs = torch.stack(vecs)\n",
        "                # print(f'hid state {j}', vecs.shape)\n",
        "                ctx_vecs.append(vecs)\n",
        "\n",
        "            ctx_vecs = torch.stack(ctx_vecs).sum(dim=0)\n",
        "            log(f'ctx_vecs', ctx_vecs.shape)\n",
        "            return ctx_vecs\n",
        "\n",
        "        ############################################\n",
        "\n",
        "        zh_batch_size, zh_sent_len = zh_sent.shape\n",
        "        log('zh_sent_len', zh_sent_len)\n",
        "        zh_emb = self.target_embedding(zh_sent)\n",
        "\n",
        "        log('zh_emb:', zh_emb.shape)\n",
        "        zh_emb = zh_emb.view(zh_sent_len, zh_batch_size, -1)\n",
        "        log('zh_emb:', zh_emb.shape)\n",
        "\n",
        "        qualvecs = []\n",
        "        zh_hid = None\n",
        "        for zh in zh_emb:\n",
        "            log('zh:', zh.shape)\n",
        "            zh = zh.view(1, zh_batch_size, -1)\n",
        "            log('zh:', zh.shape)\n",
        "            if zh_hid is None:\n",
        "                _, zh_hid = self.target_rnn(zh)\n",
        "            else:\n",
        "                _, zh_hid = self.target_rnn(zh, zh_hid)\n",
        "            log('zh_hid:', zh_hid.shape)\n",
        "\n",
        "            zh_hid_reshaped = zh_hid.view(zh_batch_size, -1)\n",
        "            log('zh_hid_reshaped:', zh_hid_reshaped.shape)\n",
        "\n",
        "            ctx = get_context(zh_hid_reshaped)\n",
        "            log('ctx:', ctx.shape)\n",
        "            \n",
        "            #TODO: fix the linear combination\n",
        "            qualvecs.append(ctx + zh_hid_reshaped)\n",
        "\n",
        "        qualvecs = torch.stack(qualvecs)\n",
        "        log('qualvecs:', qualvecs.shape)\n",
        "\n",
        "        if get_qualvecs:\n",
        "            return qualvecs\n",
        "\n",
        "        _, qualvec_hid = self.qualvec_rnn(qualvecs)\n",
        "        log('qualvec_hid:', qualvec_hid.shape)\n",
        "\n",
        "        qualvec_hid = qualvec_hid.view(zh_batch_size, -1)\n",
        "        log('qualvec_hid:', qualvec_hid.shape)\n",
        "\n",
        "        qualvec_hid_act = torch.tanh(qualvec_hid)\n",
        "\n",
        "\n",
        "        score = self.regressor_output(qualvec_hid_act)\n",
        "        log('score', score.shape)\n",
        "        \n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_kaWmXADs3H",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TRmOuCaDzGC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "427b4bd0-cf17-4e41-817a-a9f2bc6d486e"
      },
      "source": [
        "model = AutoencoderQEV(en_vocab_size=len(EN), zh_vocab_size=len(ZH), emb_dim=100)\n",
        "\n",
        "kfold_cross_validate(model, train_en_idxs, train_zh_idxs, train_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rMini-Batch:   0%|          | 0/1750 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "=======\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Mini-Batch:  35%|███▌      | 615/1750 [03:23<06:00,  3.15it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSlmNaYv4KUP",
        "colab_type": "text"
      },
      "source": [
        "### 4) BERT with Sentence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-z2M1O14PHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSHQkb7V4Pj2",
        "colab_type": "text"
      },
      "source": [
        "### 5) BERT with Sentence-Pair Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzp6ldw04XnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}